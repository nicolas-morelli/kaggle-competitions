{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c5d7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-22 19:24:53.678358: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-22 19:24:53.827068: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-22 19:24:53.879620: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-22 19:24:53.894269: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-22 19:24:53.999135: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-22 19:24:54.773813: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import class_weight\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c9843d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv').sample(frac=1)\n",
    "df_sub = pd.read_csv('test.csv').sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bd492fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Time_spent_Alone</th>\n",
       "      <th>Stage_fear</th>\n",
       "      <th>Social_event_attendance</th>\n",
       "      <th>Going_outside</th>\n",
       "      <th>Drained_after_socializing</th>\n",
       "      <th>Friends_circle_size</th>\n",
       "      <th>Post_frequency</th>\n",
       "      <th>Personality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7427</th>\n",
       "      <td>7427</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>No</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>1239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>No</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13783</th>\n",
       "      <td>13783</td>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>No</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15054</th>\n",
       "      <td>15054</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16405</th>\n",
       "      <td>16405</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  Time_spent_Alone Stage_fear  Social_event_attendance  \\\n",
       "7427    7427               2.0         No                      4.0   \n",
       "1239    1239               0.0         No                      4.0   \n",
       "13783  13783               3.0         No                      9.0   \n",
       "15054  15054               1.0         No                      5.0   \n",
       "16405  16405               1.0         No                      4.0   \n",
       "\n",
       "       Going_outside Drained_after_socializing  Friends_circle_size  \\\n",
       "7427             5.0                        No                 15.0   \n",
       "1239             5.0                        No                  9.0   \n",
       "13783            7.0                        No                 13.0   \n",
       "15054            4.0                        No                  NaN   \n",
       "16405            3.0                        No                 12.0   \n",
       "\n",
       "       Post_frequency Personality  \n",
       "7427              5.0   Extrovert  \n",
       "1239             10.0   Extrovert  \n",
       "13783             7.0   Extrovert  \n",
       "15054             8.0   Extrovert  \n",
       "16405            10.0   Extrovert  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c32d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Stage_fear'] = np.where(df['Stage_fear'] == 'Yes', 1, 0)\n",
    "df['Drained_after_socializing'] = np.where(df['Drained_after_socializing'] == 'Yes', 1, 0)\n",
    "df['Personality'] = np.where(df['Personality'] == 'Introvert', 1, 0)\n",
    "\n",
    "df_sub['Stage_fear'] = np.where(df_sub['Stage_fear'] == 'Yes', 1, 0)\n",
    "df_sub['Drained_after_socializing'] = np.where(df_sub['Drained_after_socializing'] == 'Yes', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ea6f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Reclusion'] = df['Time_spent_Alone'] - df['Going_outside']\n",
    "df['Social_Drainage'] = df['Drained_after_socializing'] / (df['Friends_circle_size'] + 1)\n",
    "df['Actual_Fear_Exposure'] = df['Stage_fear'] / (df['Post_frequency'] + 1)\n",
    "df['Media_Dependence'] = df['Post_frequency'] / (df['Time_spent_Alone'] + 1)\n",
    "df['Social_Volume'] = df['Social_event_attendance'] * df['Friends_circle_size'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e9c7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['Reclusion'] = df_sub['Time_spent_Alone'] - df_sub['Going_outside']\n",
    "df_sub['Social_Drainage'] = df_sub['Drained_after_socializing'] / (df_sub['Friends_circle_size'] + 1)\n",
    "df_sub['Actual_Fear_Exposure'] = df_sub['Stage_fear'] / (df_sub['Post_frequency'] + 1)\n",
    "df_sub['Media_Dependence'] = df_sub['Post_frequency'] / (df_sub['Time_spent_Alone'] + 1)\n",
    "df_sub['Social_Volume'] = df_sub['Social_event_attendance'] * df_sub['Friends_circle_size'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de90fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = IterativeImputer(estimator=RandomForestRegressor(n_estimators=100, random_state=47, n_jobs=-1), max_iter=50, random_state=47)\n",
    "personality = df['Personality']\n",
    "\n",
    "imputed_array = imputer.fit_transform(df.drop('Personality', axis=1))\n",
    "imputed_array_sub = imputer.transform(df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22d65afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(imputed_array, columns=df.drop('Personality', axis=1).columns, index=df.drop('Personality', axis=1).index).drop('id', axis=1)\n",
    "df['Personality'] = personality\n",
    "\n",
    "df_sub = pd.DataFrame(imputed_array_sub, columns=df_sub.columns, index=df_sub.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1bb1708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Log1p_' + col] = np.log1p(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Sqrt_' + col] = np.sqrt(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Exp_' + col] = np.exp(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Square_' + col] = np.power(df[col], 2)\n",
      "/tmp/ipykernel_108473/877111518.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Abs_' + col] = np.abs(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Mean_Diff_' + col] = df[col] - df[col].mean()\n",
      "/tmp/ipykernel_108473/877111518.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Zscore_' + col] = (df[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
      "/tmp/ipykernel_108473/877111518.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Minmax_' + col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
      "/tmp/ipykernel_108473/877111518.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Rank_' + col] = df[col].rank(method='average')\n",
      "/tmp/ipykernel_108473/877111518.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Quantile_' + col] = pd.qcut(df[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_108473/877111518.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['GT_Mean_' + col] = (df[col] > df[col].mean()).astype(int)\n",
      "/tmp/ipykernel_108473/877111518.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['GT_Median_' + col] = (df[col] > df[col].median()).astype(int)\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Log1p_' + col] = np.log1p(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Sqrt_' + col] = np.sqrt(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Exp_' + col] = np.exp(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Square_' + col] = np.power(df[col], 2)\n",
      "/tmp/ipykernel_108473/877111518.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Abs_' + col] = np.abs(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Mean_Diff_' + col] = df[col] - df[col].mean()\n",
      "/tmp/ipykernel_108473/877111518.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Zscore_' + col] = (df[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
      "/tmp/ipykernel_108473/877111518.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Minmax_' + col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
      "/tmp/ipykernel_108473/877111518.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Rank_' + col] = df[col].rank(method='average')\n",
      "/tmp/ipykernel_108473/877111518.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Quantile_' + col] = pd.qcut(df[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_108473/877111518.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['GT_Mean_' + col] = (df[col] > df[col].mean()).astype(int)\n",
      "/tmp/ipykernel_108473/877111518.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['GT_Median_' + col] = (df[col] > df[col].median()).astype(int)\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Log1p_' + col] = np.log1p(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Sqrt_' + col] = np.sqrt(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Exp_' + col] = np.exp(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Square_' + col] = np.power(df[col], 2)\n",
      "/tmp/ipykernel_108473/877111518.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Abs_' + col] = np.abs(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Mean_Diff_' + col] = df[col] - df[col].mean()\n",
      "/tmp/ipykernel_108473/877111518.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Zscore_' + col] = (df[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
      "/tmp/ipykernel_108473/877111518.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Minmax_' + col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
      "/tmp/ipykernel_108473/877111518.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Rank_' + col] = df[col].rank(method='average')\n",
      "/tmp/ipykernel_108473/877111518.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Quantile_' + col] = pd.qcut(df[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_108473/877111518.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['GT_Mean_' + col] = (df[col] > df[col].mean()).astype(int)\n",
      "/tmp/ipykernel_108473/877111518.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['GT_Median_' + col] = (df[col] > df[col].median()).astype(int)\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Log1p_' + col] = np.log1p(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Sqrt_' + col] = np.sqrt(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Exp_' + col] = np.exp(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Square_' + col] = np.power(df[col], 2)\n",
      "/tmp/ipykernel_108473/877111518.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Abs_' + col] = np.abs(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Mean_Diff_' + col] = df[col] - df[col].mean()\n",
      "/tmp/ipykernel_108473/877111518.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Zscore_' + col] = (df[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
      "/tmp/ipykernel_108473/877111518.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Minmax_' + col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
      "/tmp/ipykernel_108473/877111518.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Rank_' + col] = df[col].rank(method='average')\n",
      "/tmp/ipykernel_108473/877111518.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Quantile_' + col] = pd.qcut(df[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_108473/877111518.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['GT_Mean_' + col] = (df[col] > df[col].mean()).astype(int)\n",
      "/tmp/ipykernel_108473/877111518.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['GT_Median_' + col] = (df[col] > df[col].median()).astype(int)\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Log1p_' + col] = np.log1p(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Sqrt_' + col] = np.sqrt(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Exp_' + col] = np.exp(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Square_' + col] = np.power(df[col], 2)\n",
      "/tmp/ipykernel_108473/877111518.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Abs_' + col] = np.abs(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Mean_Diff_' + col] = df[col] - df[col].mean()\n",
      "/tmp/ipykernel_108473/877111518.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Zscore_' + col] = (df[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
      "/tmp/ipykernel_108473/877111518.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Minmax_' + col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
      "/tmp/ipykernel_108473/877111518.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Rank_' + col] = df[col].rank(method='average')\n",
      "/tmp/ipykernel_108473/877111518.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Quantile_' + col] = pd.qcut(df[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_108473/877111518.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['GT_Mean_' + col] = (df[col] > df[col].mean()).astype(int)\n",
      "/tmp/ipykernel_108473/877111518.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['GT_Median_' + col] = (df[col] > df[col].median()).astype(int)\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_108473/877111518.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Log1p_' + col] = np.log1p(df[col])\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_108473/877111518.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Sqrt_' + col] = np.sqrt(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Exp_' + col] = np.exp(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Square_' + col] = np.power(df[col], 2)\n",
      "/tmp/ipykernel_108473/877111518.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Abs_' + col] = np.abs(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Mean_Diff_' + col] = df[col] - df[col].mean()\n",
      "/tmp/ipykernel_108473/877111518.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Zscore_' + col] = (df[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
      "/tmp/ipykernel_108473/877111518.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Minmax_' + col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
      "/tmp/ipykernel_108473/877111518.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Rank_' + col] = df[col].rank(method='average')\n",
      "/tmp/ipykernel_108473/877111518.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Quantile_' + col] = pd.qcut(df[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_108473/877111518.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['GT_Mean_' + col] = (df[col] > df[col].mean()).astype(int)\n",
      "/tmp/ipykernel_108473/877111518.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['GT_Median_' + col] = (df[col] > df[col].median()).astype(int)\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Log1p_' + col] = np.log1p(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Sqrt_' + col] = np.sqrt(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Exp_' + col] = np.exp(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Square_' + col] = np.power(df[col], 2)\n",
      "/tmp/ipykernel_108473/877111518.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Abs_' + col] = np.abs(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Mean_Diff_' + col] = df[col] - df[col].mean()\n",
      "/tmp/ipykernel_108473/877111518.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Zscore_' + col] = (df[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
      "/tmp/ipykernel_108473/877111518.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Minmax_' + col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
      "/tmp/ipykernel_108473/877111518.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Rank_' + col] = df[col].rank(method='average')\n",
      "/tmp/ipykernel_108473/877111518.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Quantile_' + col] = pd.qcut(df[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_108473/877111518.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['GT_Mean_' + col] = (df[col] > df[col].mean()).astype(int)\n",
      "/tmp/ipykernel_108473/877111518.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['GT_Median_' + col] = (df[col] > df[col].median()).astype(int)\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Log1p_' + col] = np.log1p(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Sqrt_' + col] = np.sqrt(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Exp_' + col] = np.exp(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Square_' + col] = np.power(df[col], 2)\n",
      "/tmp/ipykernel_108473/877111518.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Abs_' + col] = np.abs(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Mean_Diff_' + col] = df[col] - df[col].mean()\n",
      "/tmp/ipykernel_108473/877111518.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Zscore_' + col] = (df[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
      "/tmp/ipykernel_108473/877111518.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Minmax_' + col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
      "/tmp/ipykernel_108473/877111518.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Rank_' + col] = df[col].rank(method='average')\n",
      "/tmp/ipykernel_108473/877111518.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Quantile_' + col] = pd.qcut(df[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_108473/877111518.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['GT_Mean_' + col] = (df[col] > df[col].mean()).astype(int)\n",
      "/tmp/ipykernel_108473/877111518.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['GT_Median_' + col] = (df[col] > df[col].median()).astype(int)\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_*_' + col2] = df[col] * df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_+_' + col2] = df[col] + df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Log1p_' + col] = np.log1p(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Sqrt_' + col] = np.sqrt(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Exp_' + col] = np.exp(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Square_' + col] = np.power(df[col], 2)\n",
      "/tmp/ipykernel_108473/877111518.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Abs_' + col] = np.abs(df[col])\n",
      "/tmp/ipykernel_108473/877111518.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Mean_Diff_' + col] = df[col] - df[col].mean()\n",
      "/tmp/ipykernel_108473/877111518.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Zscore_' + col] = (df[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
      "/tmp/ipykernel_108473/877111518.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Minmax_' + col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
      "/tmp/ipykernel_108473/877111518.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Rank_' + col] = df[col].rank(method='average')\n",
      "/tmp/ipykernel_108473/877111518.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Quantile_' + col] = pd.qcut(df[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_108473/877111518.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['GT_Mean_' + col] = (df[col] > df[col].mean()).astype(int)\n",
      "/tmp/ipykernel_108473/877111518.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['GT_Median_' + col] = (df[col] > df[col].median()).astype(int)\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
      "/tmp/ipykernel_108473/877111518.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_-_' + col2] = df[col] - df[col2]\n",
      "/tmp/ipykernel_108473/877111518.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + '_**_' + col2] = df[col] ** df[col2]\n"
     ]
    }
   ],
   "source": [
    "usedcols = []\n",
    "cols = df.drop('Personality', axis=1).columns\n",
    "\n",
    "for col in cols:\n",
    "    usedcols.append(col)\n",
    "    for col2 in cols:\n",
    "        if col != col2:\n",
    "            df[col + '_/_' + col2] = df[col] / (df[col2] + 1)\n",
    "            df[col + '_-_' + col2] = df[col] - df[col2]\n",
    "            df[col + '_**_' + col2] = df[col] ** df[col2]\n",
    "\n",
    "        if col2 in usedcols:\n",
    "            continue\n",
    "        \n",
    "        df[col + '_*_' + col2] = df[col] * df[col2]\n",
    "        df[col + '_+_' + col2] = df[col] + df[col2]\n",
    "\n",
    "        df['Log1p_' + col] = np.log1p(df[col])\n",
    "        df['Sqrt_' + col] = np.sqrt(df[col])\n",
    "        df['Exp_' + col] = np.exp(df[col])\n",
    "        df['Square_' + col] = np.power(df[col], 2)\n",
    "        df['Abs_' + col] = np.abs(df[col])\n",
    "\n",
    "        df['Mean_Diff_' + col] = df[col] - df[col].mean()\n",
    "        df['Zscore_' + col] = (df[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
    "        df['Minmax_' + col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
    "        df['Rank_' + col] = df[col].rank(method='average')\n",
    "        df['Quantile_' + col] = pd.qcut(df[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
    "\n",
    "        df['GT_Mean_' + col] = (df[col] > df[col].mean()).astype(int)\n",
    "        df['GT_Median_' + col] = (df[col] > df[col].median()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e38fe934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Log1p_' + col] = np.log1p(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Sqrt_' + col] = np.sqrt(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Exp_' + col] = np.exp(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Square_' + col] = np.power(df_sub[col], 2)\n",
      "/tmp/ipykernel_108473/1965970290.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Abs_' + col] = np.abs(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Mean_Diff_' + col] = df_sub[col] - df_sub[col].mean()\n",
      "/tmp/ipykernel_108473/1965970290.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Zscore_' + col] = (df_sub[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
      "/tmp/ipykernel_108473/1965970290.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Minmax_' + col] = (df_sub[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
      "/tmp/ipykernel_108473/1965970290.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Rank_' + col] = df_sub[col].rank(method='average')\n",
      "/tmp/ipykernel_108473/1965970290.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Quantile_' + col] = pd.qcut(df_sub[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_108473/1965970290.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['GT_Mean_' + col] = (df_sub[col] > df[col].mean()).astype(int)\n",
      "/tmp/ipykernel_108473/1965970290.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['GT_Median_' + col] = (df_sub[col] > df[col].median()).astype(int)\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Log1p_' + col] = np.log1p(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Sqrt_' + col] = np.sqrt(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Exp_' + col] = np.exp(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Square_' + col] = np.power(df_sub[col], 2)\n",
      "/tmp/ipykernel_108473/1965970290.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Abs_' + col] = np.abs(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Mean_Diff_' + col] = df_sub[col] - df_sub[col].mean()\n",
      "/tmp/ipykernel_108473/1965970290.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Zscore_' + col] = (df_sub[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
      "/tmp/ipykernel_108473/1965970290.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Minmax_' + col] = (df_sub[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
      "/tmp/ipykernel_108473/1965970290.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Rank_' + col] = df_sub[col].rank(method='average')\n",
      "/tmp/ipykernel_108473/1965970290.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Quantile_' + col] = pd.qcut(df_sub[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_108473/1965970290.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['GT_Mean_' + col] = (df_sub[col] > df[col].mean()).astype(int)\n",
      "/tmp/ipykernel_108473/1965970290.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['GT_Median_' + col] = (df_sub[col] > df[col].median()).astype(int)\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Log1p_' + col] = np.log1p(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Sqrt_' + col] = np.sqrt(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Exp_' + col] = np.exp(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Square_' + col] = np.power(df_sub[col], 2)\n",
      "/tmp/ipykernel_108473/1965970290.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Abs_' + col] = np.abs(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Mean_Diff_' + col] = df_sub[col] - df_sub[col].mean()\n",
      "/tmp/ipykernel_108473/1965970290.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Zscore_' + col] = (df_sub[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
      "/tmp/ipykernel_108473/1965970290.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Minmax_' + col] = (df_sub[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
      "/tmp/ipykernel_108473/1965970290.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Rank_' + col] = df_sub[col].rank(method='average')\n",
      "/tmp/ipykernel_108473/1965970290.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Quantile_' + col] = pd.qcut(df_sub[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_108473/1965970290.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['GT_Mean_' + col] = (df_sub[col] > df[col].mean()).astype(int)\n",
      "/tmp/ipykernel_108473/1965970290.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['GT_Median_' + col] = (df_sub[col] > df[col].median()).astype(int)\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Log1p_' + col] = np.log1p(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Sqrt_' + col] = np.sqrt(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Exp_' + col] = np.exp(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Square_' + col] = np.power(df_sub[col], 2)\n",
      "/tmp/ipykernel_108473/1965970290.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Abs_' + col] = np.abs(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Mean_Diff_' + col] = df_sub[col] - df_sub[col].mean()\n",
      "/tmp/ipykernel_108473/1965970290.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Zscore_' + col] = (df_sub[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
      "/tmp/ipykernel_108473/1965970290.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Minmax_' + col] = (df_sub[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
      "/tmp/ipykernel_108473/1965970290.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Rank_' + col] = df_sub[col].rank(method='average')\n",
      "/tmp/ipykernel_108473/1965970290.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Quantile_' + col] = pd.qcut(df_sub[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_108473/1965970290.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['GT_Mean_' + col] = (df_sub[col] > df[col].mean()).astype(int)\n",
      "/tmp/ipykernel_108473/1965970290.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['GT_Median_' + col] = (df_sub[col] > df[col].median()).astype(int)\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Log1p_' + col] = np.log1p(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Sqrt_' + col] = np.sqrt(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Exp_' + col] = np.exp(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Square_' + col] = np.power(df_sub[col], 2)\n",
      "/tmp/ipykernel_108473/1965970290.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Abs_' + col] = np.abs(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Mean_Diff_' + col] = df_sub[col] - df_sub[col].mean()\n",
      "/tmp/ipykernel_108473/1965970290.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Zscore_' + col] = (df_sub[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
      "/tmp/ipykernel_108473/1965970290.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Minmax_' + col] = (df_sub[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
      "/tmp/ipykernel_108473/1965970290.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Rank_' + col] = df_sub[col].rank(method='average')\n",
      "/tmp/ipykernel_108473/1965970290.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Quantile_' + col] = pd.qcut(df_sub[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_108473/1965970290.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['GT_Mean_' + col] = (df_sub[col] > df[col].mean()).astype(int)\n",
      "/tmp/ipykernel_108473/1965970290.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['GT_Median_' + col] = (df_sub[col] > df[col].median()).astype(int)\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_108473/1965970290.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Log1p_' + col] = np.log1p(df_sub[col])\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_108473/1965970290.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Sqrt_' + col] = np.sqrt(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Exp_' + col] = np.exp(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Square_' + col] = np.power(df_sub[col], 2)\n",
      "/tmp/ipykernel_108473/1965970290.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Abs_' + col] = np.abs(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Mean_Diff_' + col] = df_sub[col] - df_sub[col].mean()\n",
      "/tmp/ipykernel_108473/1965970290.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Zscore_' + col] = (df_sub[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
      "/tmp/ipykernel_108473/1965970290.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Minmax_' + col] = (df_sub[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
      "/tmp/ipykernel_108473/1965970290.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Rank_' + col] = df_sub[col].rank(method='average')\n",
      "/tmp/ipykernel_108473/1965970290.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Quantile_' + col] = pd.qcut(df_sub[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_108473/1965970290.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['GT_Mean_' + col] = (df_sub[col] > df[col].mean()).astype(int)\n",
      "/tmp/ipykernel_108473/1965970290.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['GT_Median_' + col] = (df_sub[col] > df[col].median()).astype(int)\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Log1p_' + col] = np.log1p(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Sqrt_' + col] = np.sqrt(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Exp_' + col] = np.exp(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Square_' + col] = np.power(df_sub[col], 2)\n",
      "/tmp/ipykernel_108473/1965970290.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Abs_' + col] = np.abs(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Mean_Diff_' + col] = df_sub[col] - df_sub[col].mean()\n",
      "/tmp/ipykernel_108473/1965970290.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Zscore_' + col] = (df_sub[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
      "/tmp/ipykernel_108473/1965970290.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Minmax_' + col] = (df_sub[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
      "/tmp/ipykernel_108473/1965970290.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Rank_' + col] = df_sub[col].rank(method='average')\n",
      "/tmp/ipykernel_108473/1965970290.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Quantile_' + col] = pd.qcut(df_sub[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_108473/1965970290.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['GT_Mean_' + col] = (df_sub[col] > df[col].mean()).astype(int)\n",
      "/tmp/ipykernel_108473/1965970290.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['GT_Median_' + col] = (df_sub[col] > df[col].median()).astype(int)\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Log1p_' + col] = np.log1p(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Sqrt_' + col] = np.sqrt(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Exp_' + col] = np.exp(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Square_' + col] = np.power(df_sub[col], 2)\n",
      "/tmp/ipykernel_108473/1965970290.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Abs_' + col] = np.abs(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Mean_Diff_' + col] = df_sub[col] - df_sub[col].mean()\n",
      "/tmp/ipykernel_108473/1965970290.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Zscore_' + col] = (df_sub[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
      "/tmp/ipykernel_108473/1965970290.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Minmax_' + col] = (df_sub[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
      "/tmp/ipykernel_108473/1965970290.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Rank_' + col] = df_sub[col].rank(method='average')\n",
      "/tmp/ipykernel_108473/1965970290.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Quantile_' + col] = pd.qcut(df_sub[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_108473/1965970290.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['GT_Mean_' + col] = (df_sub[col] > df[col].mean()).astype(int)\n",
      "/tmp/ipykernel_108473/1965970290.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['GT_Median_' + col] = (df_sub[col] > df[col].median()).astype(int)\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Log1p_' + col] = np.log1p(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Sqrt_' + col] = np.sqrt(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Exp_' + col] = np.exp(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Square_' + col] = np.power(df_sub[col], 2)\n",
      "/tmp/ipykernel_108473/1965970290.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Abs_' + col] = np.abs(df_sub[col])\n",
      "/tmp/ipykernel_108473/1965970290.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Mean_Diff_' + col] = df_sub[col] - df_sub[col].mean()\n",
      "/tmp/ipykernel_108473/1965970290.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Zscore_' + col] = (df_sub[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
      "/tmp/ipykernel_108473/1965970290.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Minmax_' + col] = (df_sub[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
      "/tmp/ipykernel_108473/1965970290.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Rank_' + col] = df_sub[col].rank(method='average')\n",
      "/tmp/ipykernel_108473/1965970290.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['Quantile_' + col] = pd.qcut(df_sub[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
      "/tmp/ipykernel_108473/1965970290.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['GT_Mean_' + col] = (df_sub[col] > df[col].mean()).astype(int)\n",
      "/tmp/ipykernel_108473/1965970290.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub['GT_Median_' + col] = (df_sub[col] > df[col].median()).astype(int)\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
      "/tmp/ipykernel_108473/1965970290.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
      "/tmp/ipykernel_108473/1965970290.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n"
     ]
    }
   ],
   "source": [
    "usedcols = []\n",
    "for col in cols:\n",
    "    usedcols.append(col)\n",
    "    for col2 in cols:\n",
    "        if col != col2:\n",
    "            df_sub[col + '_/_' + col2] = df_sub[col] / (df_sub[col2] + 1)\n",
    "            df_sub[col + '_-_' + col2] = df_sub[col] - df_sub[col2]\n",
    "            df_sub[col + '_**_' + col2] = df_sub[col] ** df_sub[col2]\n",
    "\n",
    "        if col2 in usedcols:\n",
    "            continue\n",
    "        \n",
    "        df_sub[col + '_*_' + col2] = df_sub[col] * df_sub[col2]\n",
    "        df_sub[col + '_+_' + col2] = df_sub[col] + df_sub[col2]\n",
    "\n",
    "        df_sub['Log1p_' + col] = np.log1p(df_sub[col])\n",
    "        df_sub['Sqrt_' + col] = np.sqrt(df_sub[col])\n",
    "        df_sub['Exp_' + col] = np.exp(df_sub[col])\n",
    "        df_sub['Square_' + col] = np.power(df_sub[col], 2)\n",
    "        df_sub['Abs_' + col] = np.abs(df_sub[col])\n",
    "\n",
    "        df_sub['Mean_Diff_' + col] = df_sub[col] - df_sub[col].mean()\n",
    "        df_sub['Zscore_' + col] = (df_sub[col] - df[col].mean()) / (df[col].std() + 1e-6)\n",
    "        df_sub['Minmax_' + col] = (df_sub[col] - df[col].min()) / (df[col].max() - df[col].min() + 1e-6)\n",
    "        df_sub['Rank_' + col] = df_sub[col].rank(method='average')\n",
    "        df_sub['Quantile_' + col] = pd.qcut(df_sub[col].rank(method='average'), q=10, labels=False, duplicates='drop')\n",
    "\n",
    "        df_sub['GT_Mean_' + col] = (df_sub[col] > df[col].mean()).astype(int)\n",
    "        df_sub['GT_Median_' + col] = (df_sub[col] > df[col].median()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e018f1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time_spent_Alone</th>\n",
       "      <th>Stage_fear</th>\n",
       "      <th>Social_event_attendance</th>\n",
       "      <th>Going_outside</th>\n",
       "      <th>Drained_after_socializing</th>\n",
       "      <th>Friends_circle_size</th>\n",
       "      <th>Post_frequency</th>\n",
       "      <th>Reclusion</th>\n",
       "      <th>Social_Drainage</th>\n",
       "      <th>Actual_Fear_Exposure</th>\n",
       "      <th>...</th>\n",
       "      <th>Social_Volume_**_Reclusion</th>\n",
       "      <th>Social_Volume_/_Social_Drainage</th>\n",
       "      <th>Social_Volume_-_Social_Drainage</th>\n",
       "      <th>Social_Volume_**_Social_Drainage</th>\n",
       "      <th>Social_Volume_/_Actual_Fear_Exposure</th>\n",
       "      <th>Social_Volume_-_Actual_Fear_Exposure</th>\n",
       "      <th>Social_Volume_**_Actual_Fear_Exposure</th>\n",
       "      <th>Social_Volume_/_Media_Dependence</th>\n",
       "      <th>Social_Volume_-_Media_Dependence</th>\n",
       "      <th>Social_Volume_**_Media_Dependence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7427</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.629630e-06</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>58.333333</td>\n",
       "      <td>9.195713e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.653817e-08</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.272727</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>3.656158e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13783</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.336500e-09</td>\n",
       "      <td>117.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.545455</td>\n",
       "      <td>115.250000</td>\n",
       "      <td>4.162222e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15054</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000e-06</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>6.250000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16405</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.340278e-04</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>2.548040e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 673 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time_spent_Alone  Stage_fear  Social_event_attendance  Going_outside  \\\n",
       "7427                2.0         0.0                      4.0            5.0   \n",
       "1239                0.0         0.0                      4.0            5.0   \n",
       "13783               3.0         0.0                      9.0            7.0   \n",
       "15054               1.0         0.0                      5.0            4.0   \n",
       "16405               1.0         0.0                      4.0            3.0   \n",
       "\n",
       "       Drained_after_socializing  Friends_circle_size  Post_frequency  \\\n",
       "7427                         0.0                 15.0             5.0   \n",
       "1239                         0.0                  9.0            10.0   \n",
       "13783                        0.0                 13.0             7.0   \n",
       "15054                        0.0                 10.0             8.0   \n",
       "16405                        0.0                 12.0            10.0   \n",
       "\n",
       "       Reclusion  Social_Drainage  Actual_Fear_Exposure  ...  \\\n",
       "7427        -3.0              0.0                   0.0  ...   \n",
       "1239        -5.0              0.0                   0.0  ...   \n",
       "13783       -4.0              0.0                   0.0  ...   \n",
       "15054       -3.0              0.0                   0.0  ...   \n",
       "16405       -2.0              0.0                   0.0  ...   \n",
       "\n",
       "       Social_Volume_**_Reclusion  Social_Volume_/_Social_Drainage  \\\n",
       "7427                 4.629630e-06                             60.0   \n",
       "1239                 1.653817e-08                             36.0   \n",
       "13783                5.336500e-09                            117.0   \n",
       "15054                8.000000e-06                             50.0   \n",
       "16405                4.340278e-04                             48.0   \n",
       "\n",
       "       Social_Volume_-_Social_Drainage  Social_Volume_**_Social_Drainage  \\\n",
       "7427                              60.0                               1.0   \n",
       "1239                              36.0                               1.0   \n",
       "13783                            117.0                               1.0   \n",
       "15054                             50.0                               1.0   \n",
       "16405                             48.0                               1.0   \n",
       "\n",
       "       Social_Volume_/_Actual_Fear_Exposure  \\\n",
       "7427                                   60.0   \n",
       "1239                                   36.0   \n",
       "13783                                 117.0   \n",
       "15054                                  50.0   \n",
       "16405                                  48.0   \n",
       "\n",
       "       Social_Volume_-_Actual_Fear_Exposure  \\\n",
       "7427                                   60.0   \n",
       "1239                                   36.0   \n",
       "13783                                 117.0   \n",
       "15054                                  50.0   \n",
       "16405                                  48.0   \n",
       "\n",
       "       Social_Volume_**_Actual_Fear_Exposure  \\\n",
       "7427                                     1.0   \n",
       "1239                                     1.0   \n",
       "13783                                    1.0   \n",
       "15054                                    1.0   \n",
       "16405                                    1.0   \n",
       "\n",
       "       Social_Volume_/_Media_Dependence  Social_Volume_-_Media_Dependence  \\\n",
       "7427                          22.500000                         58.333333   \n",
       "1239                           3.272727                         26.000000   \n",
       "13783                         42.545455                        115.250000   \n",
       "15054                         10.000000                         46.000000   \n",
       "16405                          8.000000                         43.000000   \n",
       "\n",
       "       Social_Volume_**_Media_Dependence  \n",
       "7427                        9.195713e+02  \n",
       "1239                        3.656158e+15  \n",
       "13783                       4.162222e+03  \n",
       "15054                       6.250000e+06  \n",
       "16405                       2.548040e+08  \n",
       "\n",
       "[5 rows x 673 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b051f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "642eb832",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.drop('Personality', axis=1).corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "df = df.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "227c308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df_sub.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c2fea11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time_spent_Alone</th>\n",
       "      <th>Stage_fear</th>\n",
       "      <th>Social_event_attendance</th>\n",
       "      <th>Going_outside</th>\n",
       "      <th>Drained_after_socializing</th>\n",
       "      <th>Friends_circle_size</th>\n",
       "      <th>Post_frequency</th>\n",
       "      <th>Reclusion</th>\n",
       "      <th>Social_Drainage</th>\n",
       "      <th>Actual_Fear_Exposure</th>\n",
       "      <th>...</th>\n",
       "      <th>Social_Volume_**_Time_spent_Alone</th>\n",
       "      <th>Social_Volume_/_Going_outside</th>\n",
       "      <th>Social_Volume_**_Going_outside</th>\n",
       "      <th>Social_Volume_/_Post_frequency</th>\n",
       "      <th>Social_Volume_**_Post_frequency</th>\n",
       "      <th>Social_Volume_**_Reclusion</th>\n",
       "      <th>Social_Volume_**_Social_Drainage</th>\n",
       "      <th>Social_Volume_**_Actual_Fear_Exposure</th>\n",
       "      <th>Social_Volume_/_Media_Dependence</th>\n",
       "      <th>Social_Volume_**_Media_Dependence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7427</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>10.000</td>\n",
       "      <td>7.776000e+08</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.776000e+08</td>\n",
       "      <td>4.629630e-06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>9.195713e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000</td>\n",
       "      <td>6.046618e+07</td>\n",
       "      <td>3.272727</td>\n",
       "      <td>3.656158e+15</td>\n",
       "      <td>1.653817e-08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.272727</td>\n",
       "      <td>3.656158e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13783</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1601613.0</td>\n",
       "      <td>14.625</td>\n",
       "      <td>3.001242e+14</td>\n",
       "      <td>14.625000</td>\n",
       "      <td>3.001242e+14</td>\n",
       "      <td>5.336500e-09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.545455</td>\n",
       "      <td>4.162222e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15054</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>10.000</td>\n",
       "      <td>6.250000e+06</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>3.906250e+13</td>\n",
       "      <td>8.000000e-06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.250000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16405</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.0</td>\n",
       "      <td>12.000</td>\n",
       "      <td>1.105920e+05</td>\n",
       "      <td>4.363636</td>\n",
       "      <td>6.492506e+16</td>\n",
       "      <td>4.340278e-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.548040e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 292 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time_spent_Alone  Stage_fear  Social_event_attendance  Going_outside  \\\n",
       "7427                2.0         0.0                      4.0            5.0   \n",
       "1239                0.0         0.0                      4.0            5.0   \n",
       "13783               3.0         0.0                      9.0            7.0   \n",
       "15054               1.0         0.0                      5.0            4.0   \n",
       "16405               1.0         0.0                      4.0            3.0   \n",
       "\n",
       "       Drained_after_socializing  Friends_circle_size  Post_frequency  \\\n",
       "7427                         0.0                 15.0             5.0   \n",
       "1239                         0.0                  9.0            10.0   \n",
       "13783                        0.0                 13.0             7.0   \n",
       "15054                        0.0                 10.0             8.0   \n",
       "16405                        0.0                 12.0            10.0   \n",
       "\n",
       "       Reclusion  Social_Drainage  Actual_Fear_Exposure  ...  \\\n",
       "7427        -3.0              0.0                   0.0  ...   \n",
       "1239        -5.0              0.0                   0.0  ...   \n",
       "13783       -4.0              0.0                   0.0  ...   \n",
       "15054       -3.0              0.0                   0.0  ...   \n",
       "16405       -2.0              0.0                   0.0  ...   \n",
       "\n",
       "       Social_Volume_**_Time_spent_Alone  Social_Volume_/_Going_outside  \\\n",
       "7427                              3600.0                         10.000   \n",
       "1239                                 1.0                          6.000   \n",
       "13783                          1601613.0                         14.625   \n",
       "15054                               50.0                         10.000   \n",
       "16405                               48.0                         12.000   \n",
       "\n",
       "       Social_Volume_**_Going_outside  Social_Volume_/_Post_frequency  \\\n",
       "7427                     7.776000e+08                       10.000000   \n",
       "1239                     6.046618e+07                        3.272727   \n",
       "13783                    3.001242e+14                       14.625000   \n",
       "15054                    6.250000e+06                        5.555556   \n",
       "16405                    1.105920e+05                        4.363636   \n",
       "\n",
       "       Social_Volume_**_Post_frequency  Social_Volume_**_Reclusion  \\\n",
       "7427                      7.776000e+08                4.629630e-06   \n",
       "1239                      3.656158e+15                1.653817e-08   \n",
       "13783                     3.001242e+14                5.336500e-09   \n",
       "15054                     3.906250e+13                8.000000e-06   \n",
       "16405                     6.492506e+16                4.340278e-04   \n",
       "\n",
       "       Social_Volume_**_Social_Drainage  \\\n",
       "7427                                1.0   \n",
       "1239                                1.0   \n",
       "13783                               1.0   \n",
       "15054                               1.0   \n",
       "16405                               1.0   \n",
       "\n",
       "       Social_Volume_**_Actual_Fear_Exposure  \\\n",
       "7427                                     1.0   \n",
       "1239                                     1.0   \n",
       "13783                                    1.0   \n",
       "15054                                    1.0   \n",
       "16405                                    1.0   \n",
       "\n",
       "       Social_Volume_/_Media_Dependence  Social_Volume_**_Media_Dependence  \n",
       "7427                          22.500000                       9.195713e+02  \n",
       "1239                           3.272727                       3.656158e+15  \n",
       "13783                         42.545455                       4.162222e+03  \n",
       "15054                         10.000000                       6.250000e+06  \n",
       "16405                          8.000000                       2.548040e+08  \n",
       "\n",
       "[5 rows x 292 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1f81329",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['Personality'], axis=1), df['Personality'], test_size=0.2, stratify=df['Personality'], random_state=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52702721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14819\n",
      "3705\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[0])\n",
    "print(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b1b87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'xgb': XGBClassifier,\n",
    "    'lgbm': LGBMClassifier,\n",
    "    'catboost': CatBoostClassifier,\n",
    "    'rf': RandomForestClassifier,\n",
    "    'et': ExtraTreesClassifier,\n",
    "    'lr': LogisticRegression,  \n",
    "    'mlp': MLPClassifier,     \n",
    "    'gb': GradientBoostingClassifier,\n",
    "    'hgb': HistGradientBoostingClassifier \n",
    "}\n",
    "models_trained = {name: [] for name in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b2646db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/sklearn/utils/extmath.py:1156: RuntimeWarning: overflow encountered in square\n",
      "  temp **= 2\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: overflow encountered in square\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in subtract\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:84: RuntimeWarning: overflow encountered in square\n",
      "  upper_bound = n_samples * eps * var + (n_samples * mean * eps) ** 2\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_train_rf = pd.DataFrame(X_train_sc, columns=X_train.columns, index=X_train.index)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_rf, y_train)\n",
    "importances = rf.feature_importances_\n",
    "probs = importances / np.sum(importances)\n",
    "feats = rf.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f7f121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, model_name, X_train, y_train, X_valid, y_valid):\n",
    "    early_stopping_rounds = 10\n",
    "\n",
    "    if model_name == 'xgb':\n",
    "        params = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 100, 300),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.1, log=True),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 80, 120),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'eval_metric': 'logloss'\n",
    "        }\n",
    "        model = XGBClassifier(**params, early_stopping_rounds=early_stopping_rounds)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "\n",
    "    elif model_name == 'lgbm':\n",
    "        params = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 100, 400),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 120),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),\n",
    "            'min_child_weight': trial.suggest_float('min_child_weight', 0.0001, 0.01, log=True),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 50)\n",
    "        }\n",
    "        model = LGBMClassifier(**params, early_stopping_rounds=early_stopping_rounds)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n",
    "\n",
    "    elif model_name == 'catboost':\n",
    "        params = {\n",
    "            'depth': trial.suggest_int('depth', 7, 16),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "            'iterations': trial.suggest_int('iterations', 100, 120),\n",
    "            'verbose': 0\n",
    "        }\n",
    "        model = CatBoostClassifier(**params, early_stopping_rounds=early_stopping_rounds)\n",
    "        model.fit(X_train, y_train, eval_set=(X_valid, y_valid))\n",
    "\n",
    "    elif model_name == 'rf':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 80, 120),\n",
    "            'max_depth': trial.suggest_int('max_depth', 100, 250)\n",
    "        }\n",
    "        model = RandomForestClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    elif model_name == 'et':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 60, 100),\n",
    "            'max_depth': trial.suggest_int('max_depth', 1, 10)\n",
    "        }\n",
    "        model = ExtraTreesClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    elif model_name == 'lr':\n",
    "        params = {\n",
    "            'C': trial.suggest_float('C', 1e-4, 10.0, log=True),\n",
    "            'max_iter': trial.suggest_int('max_iter', 100, 200)\n",
    "        }\n",
    "        model = LogisticRegression(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    elif model_name == 'mlp':\n",
    "        width = trial.suggest_int('hidden_layer_width', 20, 120)\n",
    "        depth = trial.suggest_int('hidden_layer_depth', 1, 15)\n",
    "        params = {\n",
    "            'hidden_layer_sizes': tuple([width] * depth),\n",
    "            'activation': trial.suggest_categorical('activation', ['relu', 'tanh']),\n",
    "            'alpha': trial.suggest_float('alpha', 1e-5, 1e-2, log=True),\n",
    "            'learning_rate_init': trial.suggest_float('learning_rate_init', 1e-4, 1e-2, log=True),\n",
    "            'max_iter': 1000,\n",
    "            'early_stopping': True\n",
    "        }\n",
    "        model = MLPClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    elif model_name == 'gb':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 80, 120),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10)\n",
    "        }\n",
    "        model = GradientBoostingClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    elif model_name == 'hgb':\n",
    "        params = {\n",
    "            'max_iter': trial.suggest_int('max_iter', 100, 200),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "            'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "            'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 20, 60),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 40)\n",
    "        }\n",
    "        model = HistGradientBoostingClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    try:\n",
    "        calibrated_model = CalibratedClassifierCV(model, method='sigmoid', cv='prefit')\n",
    "        calibrated_model.fit(X_valid, y_valid)\n",
    "        preds = calibrated_model.predict(X_valid)\n",
    "    except Exception as e:\n",
    "        preds = model.predict(X_valid)\n",
    "\n",
    "    return accuracy_score(y_valid, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c69278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:25:17,029] A new study created in memory with name: no-name-6ba67f73-e991-41ce-b0db-8ff019cf44a8\n",
      "[I 2025-07-22 19:25:26,917] Trial 0 finished with value: 0.9676113360323887 and parameters: {'max_depth': 300, 'learning_rate': 0.004782896375780932, 'n_estimators': 98, 'subsample': 0.9524948454614519, 'colsample_bytree': 0.7825337669681691}. Best is trial 0 with value: 0.9676113360323887.\n",
      "[I 2025-07-22 19:25:56,911] Trial 1 finished with value: 0.9676113360323887 and parameters: {'max_depth': 165, 'learning_rate': 0.00012396254981298104, 'n_estimators': 111, 'subsample': 0.7630575856804814, 'colsample_bytree': 0.743650923628053}. Best is trial 0 with value: 0.9676113360323887.\n",
      "[I 2025-07-22 19:26:14,106] Trial 2 finished with value: 0.9652496626180836 and parameters: {'max_depth': 120, 'learning_rate': 0.029358014482471332, 'n_estimators': 117, 'subsample': 0.9046698327384363, 'colsample_bytree': 0.9217981922874057}. Best is trial 0 with value: 0.9676113360323887.\n",
      "[I 2025-07-22 19:26:20,487] Trial 3 finished with value: 0.9676113360323887 and parameters: {'max_depth': 240, 'learning_rate': 0.0016513749024210768, 'n_estimators': 83, 'subsample': 0.710848033275834, 'colsample_bytree': 0.7009317750362731}. Best is trial 0 with value: 0.9676113360323887.\n",
      "[I 2025-07-22 19:26:28,390] Trial 4 finished with value: 0.9676113360323887 and parameters: {'max_depth': 172, 'learning_rate': 0.0008981627213734445, 'n_estimators': 92, 'subsample': 0.9539760361800916, 'colsample_bytree': 0.9402845801650745}. Best is trial 0 with value: 0.9676113360323887.\n",
      "[I 2025-07-22 19:26:37,082] Trial 5 finished with value: 0.967948717948718 and parameters: {'max_depth': 282, 'learning_rate': 0.0010735175196684844, 'n_estimators': 99, 'subsample': 0.7515013094302271, 'colsample_bytree': 0.9341577458673215}. Best is trial 5 with value: 0.967948717948718.\n",
      "[I 2025-07-22 19:26:45,910] Trial 6 finished with value: 0.967948717948718 and parameters: {'max_depth': 169, 'learning_rate': 0.00025977796197680853, 'n_estimators': 95, 'subsample': 0.7964105774295654, 'colsample_bytree': 0.7153658584771418}. Best is trial 5 with value: 0.967948717948718.\n",
      "[I 2025-07-22 19:26:56,296] Trial 7 finished with value: 0.9676113360323887 and parameters: {'max_depth': 149, 'learning_rate': 0.01081873471097243, 'n_estimators': 88, 'subsample': 0.625862052816259, 'colsample_bytree': 0.9666462159590801}. Best is trial 5 with value: 0.967948717948718.\n",
      "[I 2025-07-22 19:27:07,691] Trial 8 finished with value: 0.9672739541160594 and parameters: {'max_depth': 267, 'learning_rate': 0.03084934202998081, 'n_estimators': 81, 'subsample': 0.9597258832218285, 'colsample_bytree': 0.7242526573928678}. Best is trial 5 with value: 0.967948717948718.\n",
      "[I 2025-07-22 19:27:17,691] Trial 9 finished with value: 0.9669365721997301 and parameters: {'max_depth': 291, 'learning_rate': 0.0256760761562363, 'n_estimators': 114, 'subsample': 0.6201161318777738, 'colsample_bytree': 0.6196699121818939}. Best is trial 5 with value: 0.967948717948718.\n",
      "[I 2025-07-22 19:27:25,112] A new study created in memory with name: no-name-f8aa7bf4-8a5d-4195-b6b3-b180e75b6f83\n",
      "[I 2025-07-22 19:27:37,829] Trial 0 finished with value: 0.9736842105263158 and parameters: {'max_depth': 255, 'learning_rate': 0.0010020878285087102, 'n_estimators': 109, 'subsample': 0.750759909259929, 'colsample_bytree': 0.9346204403633012}. Best is trial 0 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:27:47,655] Trial 1 finished with value: 0.9713225371120108 and parameters: {'max_depth': 194, 'learning_rate': 0.07215374338329177, 'n_estimators': 92, 'subsample': 0.9746589184265879, 'colsample_bytree': 0.878028195273878}. Best is trial 0 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:28:03,792] Trial 2 finished with value: 0.9719973009446694 and parameters: {'max_depth': 279, 'learning_rate': 0.040732875013866646, 'n_estimators': 91, 'subsample': 0.8596555482418559, 'colsample_bytree': 0.9637669929896744}. Best is trial 0 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:28:23,068] Trial 3 finished with value: 0.9736842105263158 and parameters: {'max_depth': 240, 'learning_rate': 0.000678255448814946, 'n_estimators': 115, 'subsample': 0.6697236871036628, 'colsample_bytree': 0.7363581926702449}. Best is trial 0 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:28:49,875] Trial 4 finished with value: 0.9733468286099866 and parameters: {'max_depth': 244, 'learning_rate': 0.01116258274589136, 'n_estimators': 100, 'subsample': 0.6810195975355318, 'colsample_bytree': 0.8846691330169432}. Best is trial 0 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:29:02,428] Trial 5 finished with value: 0.9733468286099866 and parameters: {'max_depth': 279, 'learning_rate': 0.008239166707258768, 'n_estimators': 101, 'subsample': 0.7238974834056993, 'colsample_bytree': 0.7469793486672651}. Best is trial 0 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:29:13,954] Trial 6 finished with value: 0.9733468286099866 and parameters: {'max_depth': 205, 'learning_rate': 0.001007289160915745, 'n_estimators': 108, 'subsample': 0.681166977633981, 'colsample_bytree': 0.7776765663226419}. Best is trial 0 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:29:21,254] Trial 7 finished with value: 0.9723346828609987 and parameters: {'max_depth': 118, 'learning_rate': 0.00031432816673259713, 'n_estimators': 81, 'subsample': 0.9843328224682338, 'colsample_bytree': 0.7451049026647266}. Best is trial 0 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:29:30,288] Trial 8 finished with value: 0.9733468286099866 and parameters: {'max_depth': 167, 'learning_rate': 0.002168168068914457, 'n_estimators': 104, 'subsample': 0.7118471322872247, 'colsample_bytree': 0.6233038295588952}. Best is trial 0 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:29:42,944] Trial 9 finished with value: 0.9726720647773279 and parameters: {'max_depth': 171, 'learning_rate': 0.0010729428034811689, 'n_estimators': 103, 'subsample': 0.8836412308605639, 'colsample_bytree': 0.9672121311857634}. Best is trial 0 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:29:50,487] A new study created in memory with name: no-name-00e7d0ca-451e-444a-af81-50418a9014f2\n",
      "[I 2025-07-22 19:30:00,360] Trial 0 finished with value: 0.9689608636977058 and parameters: {'max_depth': 257, 'learning_rate': 0.00773667302424802, 'n_estimators': 118, 'subsample': 0.8475918523758579, 'colsample_bytree': 0.8408631716530572}. Best is trial 0 with value: 0.9689608636977058.\n",
      "[I 2025-07-22 19:30:08,691] Trial 1 finished with value: 0.9689608636977058 and parameters: {'max_depth': 108, 'learning_rate': 0.0014124112573111415, 'n_estimators': 102, 'subsample': 0.7026220426639574, 'colsample_bytree': 0.7278451920371483}. Best is trial 0 with value: 0.9689608636977058.\n",
      "[I 2025-07-22 19:30:34,234] Trial 2 finished with value: 0.9689608636977058 and parameters: {'max_depth': 181, 'learning_rate': 0.002509233032333468, 'n_estimators': 118, 'subsample': 0.8551186100220012, 'colsample_bytree': 0.9310526910117192}. Best is trial 0 with value: 0.9689608636977058.\n",
      "[I 2025-07-22 19:31:09,146] Trial 3 finished with value: 0.9689608636977058 and parameters: {'max_depth': 242, 'learning_rate': 0.0017787645490537423, 'n_estimators': 105, 'subsample': 0.7467351818725162, 'colsample_bytree': 0.6337600529818411}. Best is trial 0 with value: 0.9689608636977058.\n",
      "[I 2025-07-22 19:32:03,848] Trial 4 finished with value: 0.9689608636977058 and parameters: {'max_depth': 295, 'learning_rate': 0.0002781253563935873, 'n_estimators': 120, 'subsample': 0.7667518425197491, 'colsample_bytree': 0.6013042206791449}. Best is trial 0 with value: 0.9689608636977058.\n",
      "[I 2025-07-22 19:32:39,516] Trial 5 finished with value: 0.9686234817813765 and parameters: {'max_depth': 244, 'learning_rate': 0.00037341377695088157, 'n_estimators': 87, 'subsample': 0.613600882983079, 'colsample_bytree': 0.6595015225955527}. Best is trial 0 with value: 0.9689608636977058.\n",
      "[I 2025-07-22 19:33:50,300] Trial 6 finished with value: 0.9689608636977058 and parameters: {'max_depth': 252, 'learning_rate': 0.0015238885100073778, 'n_estimators': 108, 'subsample': 0.7774262015873524, 'colsample_bytree': 0.760345899191045}. Best is trial 0 with value: 0.9689608636977058.\n",
      "[I 2025-07-22 19:33:55,023] Trial 7 finished with value: 0.9686234817813765 and parameters: {'max_depth': 154, 'learning_rate': 0.00024824748754976016, 'n_estimators': 87, 'subsample': 0.9030909276248387, 'colsample_bytree': 0.8804488884724422}. Best is trial 0 with value: 0.9689608636977058.\n",
      "[I 2025-07-22 19:33:58,853] Trial 8 finished with value: 0.9682860998650472 and parameters: {'max_depth': 153, 'learning_rate': 0.0501538216241317, 'n_estimators': 97, 'subsample': 0.8794654145660048, 'colsample_bytree': 0.9304820049357787}. Best is trial 0 with value: 0.9689608636977058.\n",
      "[I 2025-07-22 19:34:01,134] Trial 9 finished with value: 0.9689608636977058 and parameters: {'max_depth': 224, 'learning_rate': 0.00041464601275920365, 'n_estimators': 106, 'subsample': 0.6669188166680006, 'colsample_bytree': 0.7497881820974642}. Best is trial 0 with value: 0.9689608636977058.\n",
      "[I 2025-07-22 19:34:04,082] A new study created in memory with name: no-name-cec9a1a8-2f3f-4492-897d-cdd22f3b0300\n",
      "[I 2025-07-22 19:34:06,854] Trial 0 finished with value: 0.9625506072874493 and parameters: {'max_depth': 257, 'learning_rate': 0.0001551443405094394, 'n_estimators': 97, 'subsample': 0.9902733901993088, 'colsample_bytree': 0.980879453661803}. Best is trial 0 with value: 0.9625506072874493.\n",
      "[I 2025-07-22 19:34:08,787] Trial 1 finished with value: 0.9642375168690959 and parameters: {'max_depth': 151, 'learning_rate': 0.0034243152255605085, 'n_estimators': 93, 'subsample': 0.9895679030722327, 'colsample_bytree': 0.8315869027230764}. Best is trial 1 with value: 0.9642375168690959.\n",
      "[I 2025-07-22 19:34:10,687] Trial 2 finished with value: 0.9652496626180836 and parameters: {'max_depth': 174, 'learning_rate': 0.0027859348333999323, 'n_estimators': 83, 'subsample': 0.843187743509103, 'colsample_bytree': 0.8525091907340142}. Best is trial 2 with value: 0.9652496626180836.\n",
      "[I 2025-07-22 19:34:13,532] Trial 3 finished with value: 0.9652496626180836 and parameters: {'max_depth': 290, 'learning_rate': 0.001076414686679218, 'n_estimators': 112, 'subsample': 0.8751133179641838, 'colsample_bytree': 0.9303376907470293}. Best is trial 2 with value: 0.9652496626180836.\n",
      "[I 2025-07-22 19:34:15,791] Trial 4 finished with value: 0.9652496626180836 and parameters: {'max_depth': 191, 'learning_rate': 0.0001717503611454269, 'n_estimators': 111, 'subsample': 0.825357390562848, 'colsample_bytree': 0.8892218531590304}. Best is trial 2 with value: 0.9652496626180836.\n",
      "[I 2025-07-22 19:34:17,711] Trial 5 finished with value: 0.9659244264507423 and parameters: {'max_depth': 171, 'learning_rate': 0.002985127925908433, 'n_estimators': 92, 'subsample': 0.6053703847856208, 'colsample_bytree': 0.6853520396072775}. Best is trial 5 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 19:34:22,498] Trial 6 finished with value: 0.9652496626180836 and parameters: {'max_depth': 227, 'learning_rate': 0.0003457464916605431, 'n_estimators': 116, 'subsample': 0.810944151682532, 'colsample_bytree': 0.9568402127588879}. Best is trial 5 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 19:34:23,881] Trial 7 finished with value: 0.9659244264507423 and parameters: {'max_depth': 112, 'learning_rate': 0.000386482495006664, 'n_estimators': 102, 'subsample': 0.6524731002413577, 'colsample_bytree': 0.8723670915406213}. Best is trial 5 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 19:34:26,635] Trial 8 finished with value: 0.9652496626180836 and parameters: {'max_depth': 109, 'learning_rate': 0.004856613821007981, 'n_estimators': 102, 'subsample': 0.7634933139012262, 'colsample_bytree': 0.6638870852652131}. Best is trial 5 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 19:34:28,497] Trial 9 finished with value: 0.9655870445344129 and parameters: {'max_depth': 240, 'learning_rate': 0.00041939870636193045, 'n_estimators': 83, 'subsample': 0.6446939577466367, 'colsample_bytree': 0.9985849763188809}. Best is trial 5 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 19:34:30,439] A new study created in memory with name: no-name-87c044b3-a24f-4cbc-b27e-e0245104979d\n",
      "[I 2025-07-22 19:34:35,113] Trial 0 finished with value: 0.9665879176510294 and parameters: {'max_depth': 103, 'learning_rate': 0.023565269444638104, 'n_estimators': 109, 'subsample': 0.7099205528342469, 'colsample_bytree': 0.7368086526348183}. Best is trial 0 with value: 0.9665879176510294.\n",
      "[I 2025-07-22 19:34:38,494] Trial 1 finished with value: 0.9665879176510294 and parameters: {'max_depth': 231, 'learning_rate': 0.00291225773864156, 'n_estimators': 84, 'subsample': 0.8624676376860396, 'colsample_bytree': 0.6266634566817569}. Best is trial 0 with value: 0.9665879176510294.\n",
      "[I 2025-07-22 19:34:41,768] Trial 2 finished with value: 0.9665879176510294 and parameters: {'max_depth': 182, 'learning_rate': 0.0026752704140298395, 'n_estimators': 99, 'subsample': 0.8498874262516508, 'colsample_bytree': 0.7551102132442873}. Best is trial 0 with value: 0.9665879176510294.\n",
      "[I 2025-07-22 19:34:44,551] Trial 3 finished with value: 0.9665879176510294 and parameters: {'max_depth': 152, 'learning_rate': 0.00012432957092949014, 'n_estimators': 93, 'subsample': 0.729645790425383, 'colsample_bytree': 0.6918915333724108}. Best is trial 0 with value: 0.9665879176510294.\n",
      "[I 2025-07-22 19:34:47,380] Trial 4 finished with value: 0.9659129260884239 and parameters: {'max_depth': 104, 'learning_rate': 0.0005795369001572605, 'n_estimators': 97, 'subsample': 0.7839822175334689, 'colsample_bytree': 0.9866010166345163}. Best is trial 0 with value: 0.9665879176510294.\n",
      "[I 2025-07-22 19:34:48,862] Trial 5 finished with value: 0.9669254134323321 and parameters: {'max_depth': 247, 'learning_rate': 0.00013282628389604974, 'n_estimators': 117, 'subsample': 0.6297228087207484, 'colsample_bytree': 0.6584698729589795}. Best is trial 5 with value: 0.9669254134323321.\n",
      "[I 2025-07-22 19:34:52,583] Trial 6 finished with value: 0.9669254134323321 and parameters: {'max_depth': 232, 'learning_rate': 0.006194562519713742, 'n_estimators': 99, 'subsample': 0.750359637117443, 'colsample_bytree': 0.6376461262002538}. Best is trial 5 with value: 0.9669254134323321.\n",
      "[I 2025-07-22 19:34:54,329] Trial 7 finished with value: 0.9669254134323321 and parameters: {'max_depth': 276, 'learning_rate': 0.0007553782566773216, 'n_estimators': 105, 'subsample': 0.6662515130591613, 'colsample_bytree': 0.6975218095816382}. Best is trial 5 with value: 0.9669254134323321.\n",
      "[I 2025-07-22 19:34:56,408] Trial 8 finished with value: 0.9665879176510294 and parameters: {'max_depth': 226, 'learning_rate': 0.0005191994092857626, 'n_estimators': 96, 'subsample': 0.9516260777255225, 'colsample_bytree': 0.9238687172208088}. Best is trial 5 with value: 0.9669254134323321.\n",
      "[I 2025-07-22 19:34:59,170] Trial 9 finished with value: 0.9669254134323321 and parameters: {'max_depth': 208, 'learning_rate': 0.00044113597835109144, 'n_estimators': 102, 'subsample': 0.6414754613780183, 'colsample_bytree': 0.7431399564818922}. Best is trial 5 with value: 0.9669254134323321.\n",
      "[I 2025-07-22 19:35:02,060] A new study created in memory with name: no-name-382258fe-fecb-4440-89a7-82c674bdf425\n",
      "[I 2025-07-22 19:35:02,205] Trial 0 finished with value: 0.9682860998650472 and parameters: {'max_depth': 156, 'learning_rate': 0.03208410390278614, 'n_estimators': 112, 'subsample': 0.6040388403319764, 'colsample_bytree': 0.8799393634728571, 'min_child_samples': 36, 'min_child_weight': 0.0006992323751746481, 'num_leaves': 31}. Best is trial 0 with value: 0.9682860998650472.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001286 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14729\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[112]\tvalid_0's binary_logloss: 0.143062\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14729\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:02,354] Trial 1 finished with value: 0.9672739541160594 and parameters: {'max_depth': 326, 'learning_rate': 0.03142283240817659, 'n_estimators': 120, 'subsample': 0.887275187727705, 'colsample_bytree': 0.7406560541736671, 'min_child_samples': 12, 'min_child_weight': 0.00012260521937005117, 'num_leaves': 39}. Best is trial 0 with value: 0.9682860998650472.\n",
      "[I 2025-07-22 19:35:02,442] Trial 2 finished with value: 0.9672739541160594 and parameters: {'max_depth': 338, 'learning_rate': 0.06450999284061436, 'n_estimators': 115, 'subsample': 0.8784806271919186, 'colsample_bytree': 0.7795698580120688, 'min_child_samples': 22, 'min_child_weight': 0.0007203352588945199, 'num_leaves': 21}. Best is trial 0 with value: 0.9682860998650472.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[116]\tvalid_0's binary_logloss: 0.141939\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001181 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14729\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid_0's binary_logloss: 0.14208\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001179 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14729\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[110]\tvalid_0's binary_logloss: 0.396123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:02,559] Trial 3 finished with value: 0.9669365721997301 and parameters: {'max_depth': 318, 'learning_rate': 0.004866652785015947, 'n_estimators': 110, 'subsample': 0.6955114495708407, 'colsample_bytree': 0.7626321863087742, 'min_child_samples': 37, 'min_child_weight': 0.00024185660298851568, 'num_leaves': 28}. Best is trial 0 with value: 0.9682860998650472.\n",
      "[I 2025-07-22 19:35:02,656] Trial 4 finished with value: 0.9676113360323887 and parameters: {'max_depth': 217, 'learning_rate': 0.0760860532961336, 'n_estimators': 106, 'subsample': 0.6403140034980229, 'colsample_bytree': 0.9831221573735629, 'min_child_samples': 41, 'min_child_weight': 0.00013425591537243704, 'num_leaves': 20}. Best is trial 0 with value: 0.9682860998650472.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14729\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid_0's binary_logloss: 0.141884\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000932 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14729\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:02,788] Trial 5 finished with value: 0.967948717948718 and parameters: {'max_depth': 232, 'learning_rate': 0.0556578685330194, 'n_estimators': 108, 'subsample': 0.8998579790610077, 'colsample_bytree': 0.8801515142744472, 'min_child_samples': 15, 'min_child_weight': 0.0013770804906629364, 'num_leaves': 37}. Best is trial 0 with value: 0.9682860998650472.\n",
      "[I 2025-07-22 19:35:02,900] Trial 6 finished with value: 0.967948717948718 and parameters: {'max_depth': 175, 'learning_rate': 0.013072538817401702, 'n_estimators': 102, 'subsample': 0.9884061416359701, 'colsample_bytree': 0.7346946496311059, 'min_child_samples': 30, 'min_child_weight': 0.001957551994140519, 'num_leaves': 32}. Best is trial 0 with value: 0.9682860998650472.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[88]\tvalid_0's binary_logloss: 0.142335\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001207 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14729\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[102]\tvalid_0's binary_logloss: 0.226082\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001146 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14729\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:03,021] Trial 7 finished with value: 0.9676113360323887 and parameters: {'max_depth': 159, 'learning_rate': 0.004176866564181079, 'n_estimators': 119, 'subsample': 0.719131810779275, 'colsample_bytree': 0.9669757195469456, 'min_child_samples': 26, 'min_child_weight': 0.00022372132436832877, 'num_leaves': 25}. Best is trial 0 with value: 0.9682860998650472.\n",
      "[I 2025-07-22 19:35:03,160] Trial 8 finished with value: 0.967948717948718 and parameters: {'max_depth': 260, 'learning_rate': 0.010710259005648705, 'n_estimators': 114, 'subsample': 0.9998722408027025, 'colsample_bytree': 0.6670376292439939, 'min_child_samples': 35, 'min_child_weight': 0.0014387804102898125, 'num_leaves': 43}. Best is trial 0 with value: 0.9682860998650472.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[119]\tvalid_0's binary_logloss: 0.409254\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14729\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[114]\tvalid_0's binary_logloss: 0.240858\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000935 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14729\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:03,288] Trial 9 finished with value: 0.9676113360323887 and parameters: {'max_depth': 215, 'learning_rate': 0.002297446119551009, 'n_estimators': 103, 'subsample': 0.9015763615150867, 'colsample_bytree': 0.7557825042467899, 'min_child_samples': 30, 'min_child_weight': 0.008974623338835478, 'num_leaves': 42}. Best is trial 0 with value: 0.9682860998650472.\n",
      "[I 2025-07-22 19:35:03,452] A new study created in memory with name: no-name-f0342e4f-d832-494e-9bd3-3bf80a1583ea\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[103]\tvalid_0's binary_logloss: 0.528562\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14729\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14793\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:03,560] Trial 0 finished with value: 0.9730094466936572 and parameters: {'max_depth': 310, 'learning_rate': 0.003753834419655047, 'n_estimators': 110, 'subsample': 0.6028126418103629, 'colsample_bytree': 0.6400236401761292, 'min_child_samples': 44, 'min_child_weight': 0.004120374143837996, 'num_leaves': 26}. Best is trial 0 with value: 0.9730094466936572.\n",
      "[I 2025-07-22 19:35:03,694] Trial 1 finished with value: 0.9736842105263158 and parameters: {'max_depth': 177, 'learning_rate': 0.011560022037328529, 'n_estimators': 113, 'subsample': 0.7710542685105222, 'colsample_bytree': 0.9807369283729289, 'min_child_samples': 24, 'min_child_weight': 0.0003610449083786867, 'num_leaves': 38}. Best is trial 1 with value: 0.9736842105263158.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[110]\tvalid_0's binary_logloss: 0.442388\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14793\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[113]\tvalid_0's binary_logloss: 0.224223\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14793\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:03,813] Trial 2 finished with value: 0.9733468286099866 and parameters: {'max_depth': 335, 'learning_rate': 0.001349730433964543, 'n_estimators': 101, 'subsample': 0.6954719428429049, 'colsample_bytree': 0.9467928801120271, 'min_child_samples': 42, 'min_child_weight': 0.003612899560181985, 'num_leaves': 36}. Best is trial 1 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:35:03,908] Trial 3 finished with value: 0.9733468286099866 and parameters: {'max_depth': 301, 'learning_rate': 0.001079558594739429, 'n_estimators': 109, 'subsample': 0.9304936236597051, 'colsample_bytree': 0.7200109312373492, 'min_child_samples': 44, 'min_child_weight': 0.0013535942899129342, 'num_leaves': 21}. Best is trial 1 with value: 0.9736842105263158.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[101]\tvalid_0's binary_logloss: 0.589842\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000927 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14793\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[109]\tvalid_0's binary_logloss: 0.602779\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001055 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14793\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:04,047] Trial 4 finished with value: 0.9726720647773279 and parameters: {'max_depth': 391, 'learning_rate': 0.0034996790889682923, 'n_estimators': 117, 'subsample': 0.6494330762010126, 'colsample_bytree': 0.9361735737607135, 'min_child_samples': 50, 'min_child_weight': 0.00108051797053947, 'num_leaves': 38}. Best is trial 1 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:35:04,185] Trial 5 finished with value: 0.9723346828609987 and parameters: {'max_depth': 129, 'learning_rate': 0.0017464706074557749, 'n_estimators': 107, 'subsample': 0.8117532813866446, 'colsample_bytree': 0.6753033414747021, 'min_child_samples': 47, 'min_child_weight': 0.0005587352543213992, 'num_leaves': 45}. Best is trial 1 with value: 0.9736842105263158.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[117]\tvalid_0's binary_logloss: 0.44365\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14793\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[107]\tvalid_0's binary_logloss: 0.557407\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001215 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14793\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:04,326] Trial 6 finished with value: 0.9733468286099866 and parameters: {'max_depth': 331, 'learning_rate': 0.019278195530905883, 'n_estimators': 108, 'subsample': 0.6936613139383702, 'colsample_bytree': 0.6732528530161689, 'min_child_samples': 15, 'min_child_weight': 0.0003521567452536751, 'num_leaves': 45}. Best is trial 1 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:35:04,440] Trial 7 finished with value: 0.9730094466936572 and parameters: {'max_depth': 314, 'learning_rate': 0.04576763091523816, 'n_estimators': 100, 'subsample': 0.6548810554581243, 'colsample_bytree': 0.7238891660310597, 'min_child_samples': 49, 'min_child_weight': 0.0001234724664529209, 'num_leaves': 31}. Best is trial 1 with value: 0.9736842105263158.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[108]\tvalid_0's binary_logloss: 0.160818\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000841 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14793\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.131438\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14793\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:04,574] Trial 8 finished with value: 0.9713225371120108 and parameters: {'max_depth': 133, 'learning_rate': 0.0012513080204486595, 'n_estimators': 114, 'subsample': 0.6579694037504181, 'colsample_bytree': 0.9257568298177901, 'min_child_samples': 29, 'min_child_weight': 0.004016284533507845, 'num_leaves': 38}. Best is trial 1 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:35:04,707] Trial 9 finished with value: 0.9730094466936572 and parameters: {'max_depth': 126, 'learning_rate': 0.019630893276086858, 'n_estimators': 119, 'subsample': 0.7775463558494486, 'colsample_bytree': 0.6327035806297289, 'min_child_samples': 40, 'min_child_weight': 0.00858881791092426, 'num_leaves': 39}. Best is trial 1 with value: 0.9736842105263158.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[114]\tvalid_0's binary_logloss: 0.585259\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14793\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[119]\tvalid_0's binary_logloss: 0.153362\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000959 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14793\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:04,870] A new study created in memory with name: no-name-29f019f9-5f79-41cf-ade5-7cfc9024b455\n",
      "[I 2025-07-22 19:35:05,022] Trial 0 finished with value: 0.9689608636977058 and parameters: {'max_depth': 355, 'learning_rate': 0.003206975027304583, 'n_estimators': 115, 'subsample': 0.8345777880760237, 'colsample_bytree': 0.7288378736776988, 'min_child_samples': 45, 'min_child_weight': 0.00021744499799830933, 'num_leaves': 44}. Best is trial 0 with value: 0.9689608636977058.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15018\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[115]\tvalid_0's binary_logloss: 0.462083\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15018\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:05,104] Trial 1 finished with value: 0.9689608636977058 and parameters: {'max_depth': 167, 'learning_rate': 0.07368200990140733, 'n_estimators': 104, 'subsample': 0.656902075442693, 'colsample_bytree': 0.6140026730709534, 'min_child_samples': 46, 'min_child_weight': 0.009796936054101986, 'num_leaves': 22}. Best is trial 0 with value: 0.9689608636977058.\n",
      "[I 2025-07-22 19:35:05,253] Trial 2 finished with value: 0.9686234817813765 and parameters: {'max_depth': 208, 'learning_rate': 0.011691629229250216, 'n_estimators': 106, 'subsample': 0.967955371676577, 'colsample_bytree': 0.8097803167627469, 'min_child_samples': 28, 'min_child_weight': 0.0027350985825106526, 'num_leaves': 50}. Best is trial 0 with value: 0.9689608636977058.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[70]\tvalid_0's binary_logloss: 0.138784\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000921 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15018\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[106]\tvalid_0's binary_logloss: 0.235359\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000937 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15018\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:05,353] Trial 3 finished with value: 0.9689608636977058 and parameters: {'max_depth': 199, 'learning_rate': 0.09341793641069454, 'n_estimators': 106, 'subsample': 0.6278649534425119, 'colsample_bytree': 0.8247976504250769, 'min_child_samples': 44, 'min_child_weight': 0.00010015898257623455, 'num_leaves': 47}. Best is trial 0 with value: 0.9689608636977058.\n",
      "[I 2025-07-22 19:35:05,447] Trial 4 finished with value: 0.9686234817813765 and parameters: {'max_depth': 202, 'learning_rate': 0.014152240673010335, 'n_estimators': 108, 'subsample': 0.9128265929125585, 'colsample_bytree': 0.6727375836173685, 'min_child_samples': 23, 'min_child_weight': 0.0009775020853691233, 'num_leaves': 20}. Best is trial 0 with value: 0.9689608636977058.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[55]\tvalid_0's binary_logloss: 0.139382\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000906 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15018\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[108]\tvalid_0's binary_logloss: 0.205653\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001112 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15018\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:05,585] Trial 5 finished with value: 0.9686234817813765 and parameters: {'max_depth': 204, 'learning_rate': 0.022077834350291946, 'n_estimators': 103, 'subsample': 0.7070069994821143, 'colsample_bytree': 0.8646728286064509, 'min_child_samples': 40, 'min_child_weight': 0.0004854440267163784, 'num_leaves': 42}. Best is trial 0 with value: 0.9689608636977058.\n",
      "[I 2025-07-22 19:35:05,715] Trial 6 finished with value: 0.9682860998650472 and parameters: {'max_depth': 249, 'learning_rate': 0.0061565231179956294, 'n_estimators': 117, 'subsample': 0.9951152578388283, 'colsample_bytree': 0.8211746862988123, 'min_child_samples': 48, 'min_child_weight': 0.0005296969041998911, 'num_leaves': 37}. Best is trial 0 with value: 0.9689608636977058.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[103]\tvalid_0's binary_logloss: 0.160898\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15018\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[117]\tvalid_0's binary_logloss: 0.338931\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15018\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:05,847] Trial 7 finished with value: 0.9686234817813765 and parameters: {'max_depth': 385, 'learning_rate': 0.00856060813919548, 'n_estimators': 111, 'subsample': 0.9163224289389333, 'colsample_bytree': 0.6947179297793632, 'min_child_samples': 44, 'min_child_weight': 0.00028062927354728445, 'num_leaves': 44}. Best is trial 0 with value: 0.9689608636977058.\n",
      "[I 2025-07-22 19:35:05,983] Trial 8 finished with value: 0.9689608636977058 and parameters: {'max_depth': 324, 'learning_rate': 0.0013676409832836158, 'n_estimators': 103, 'subsample': 0.6032119636610652, 'colsample_bytree': 0.6976304582126504, 'min_child_samples': 10, 'min_child_weight': 0.0024382724669631674, 'num_leaves': 43}. Best is trial 0 with value: 0.9689608636977058.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[111]\tvalid_0's binary_logloss: 0.285219\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000910 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15018\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[103]\tvalid_0's binary_logloss: 0.585769\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15018\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:06,112] Trial 9 finished with value: 0.9696356275303644 and parameters: {'max_depth': 297, 'learning_rate': 0.0072880619339961645, 'n_estimators': 110, 'subsample': 0.7455559812274426, 'colsample_bytree': 0.901853950575775, 'min_child_samples': 16, 'min_child_weight': 0.0008013699124322602, 'num_leaves': 31}. Best is trial 9 with value: 0.9696356275303644.\n",
      "[I 2025-07-22 19:35:06,273] A new study created in memory with name: no-name-68306719-a439-45fd-b104-8c613c29a18e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[110]\tvalid_0's binary_logloss: 0.315011\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000940 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15018\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000895 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14875\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:06,422] Trial 0 finished with value: 0.9665991902834008 and parameters: {'max_depth': 191, 'learning_rate': 0.006300155256890502, 'n_estimators': 118, 'subsample': 0.8015130499925589, 'colsample_bytree': 0.6401379705523217, 'min_child_samples': 31, 'min_child_weight': 0.009605447737589767, 'num_leaves': 50}. Best is trial 0 with value: 0.9665991902834008.\n",
      "[I 2025-07-22 19:35:06,534] Trial 1 finished with value: 0.9659244264507423 and parameters: {'max_depth': 170, 'learning_rate': 0.02157638701906151, 'n_estimators': 110, 'subsample': 0.6016633184426438, 'colsample_bytree': 0.7629436044217923, 'min_child_samples': 46, 'min_child_weight': 0.005206987377459705, 'num_leaves': 28}. Best is trial 0 with value: 0.9665991902834008.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[118]\tvalid_0's binary_logloss: 0.335094\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14875\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[110]\tvalid_0's binary_logloss: 0.169049\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000907 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14875\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:06,671] Trial 2 finished with value: 0.9662618083670715 and parameters: {'max_depth': 123, 'learning_rate': 0.007366742965401991, 'n_estimators': 111, 'subsample': 0.6023804001480096, 'colsample_bytree': 0.7844275880324252, 'min_child_samples': 41, 'min_child_weight': 0.00010135058443370294, 'num_leaves': 42}. Best is trial 0 with value: 0.9665991902834008.\n",
      "[I 2025-07-22 19:35:06,776] Trial 3 finished with value: 0.9662618083670715 and parameters: {'max_depth': 214, 'learning_rate': 0.03184748680851365, 'n_estimators': 108, 'subsample': 0.8310190763495084, 'colsample_bytree': 0.774850201011003, 'min_child_samples': 32, 'min_child_weight': 0.0002285136505415236, 'num_leaves': 23}. Best is trial 0 with value: 0.9665991902834008.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[111]\tvalid_0's binary_logloss: 0.318017\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14875\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[108]\tvalid_0's binary_logloss: 0.155404\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000936 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14875\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:06,882] Trial 4 finished with value: 0.9659244264507423 and parameters: {'max_depth': 267, 'learning_rate': 0.0024543060180858716, 'n_estimators': 107, 'subsample': 0.693259307715355, 'colsample_bytree': 0.7935340442275453, 'min_child_samples': 49, 'min_child_weight': 0.0033267695833805095, 'num_leaves': 23}. Best is trial 0 with value: 0.9665991902834008.\n",
      "[I 2025-07-22 19:35:07,003] Trial 5 finished with value: 0.9659244264507423 and parameters: {'max_depth': 101, 'learning_rate': 0.011019395353251926, 'n_estimators': 112, 'subsample': 0.7629680883544212, 'colsample_bytree': 0.6246425606383561, 'min_child_samples': 17, 'min_child_weight': 0.005000639464238826, 'num_leaves': 35}. Best is trial 0 with value: 0.9665991902834008.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[107]\tvalid_0's binary_logloss: 0.516853\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001347 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14875\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[112]\tvalid_0's binary_logloss: 0.2433\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001116 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14875\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:07,130] Trial 6 finished with value: 0.9655870445344129 and parameters: {'max_depth': 317, 'learning_rate': 0.019747903853800125, 'n_estimators': 102, 'subsample': 0.7681010760321156, 'colsample_bytree': 0.8869836874363379, 'min_child_samples': 13, 'min_child_weight': 0.003387154052596072, 'num_leaves': 30}. Best is trial 0 with value: 0.9665991902834008.\n",
      "[I 2025-07-22 19:35:07,231] Trial 7 finished with value: 0.9662618083670715 and parameters: {'max_depth': 234, 'learning_rate': 0.0678781450845165, 'n_estimators': 104, 'subsample': 0.9315766264666112, 'colsample_bytree': 0.7441364830015402, 'min_child_samples': 36, 'min_child_weight': 0.0015001062139595178, 'num_leaves': 41}. Best is trial 0 with value: 0.9665991902834008.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[102]\tvalid_0's binary_logloss: 0.17989\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14875\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[55]\tvalid_0's binary_logloss: 0.153399\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000989 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14875\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:07,342] Trial 8 finished with value: 0.9662618083670715 and parameters: {'max_depth': 161, 'learning_rate': 0.001522177239301978, 'n_estimators': 109, 'subsample': 0.6335615882686872, 'colsample_bytree': 0.6476106163336685, 'min_child_samples': 36, 'min_child_weight': 0.00022826102288530333, 'num_leaves': 28}. Best is trial 0 with value: 0.9665991902834008.\n",
      "[I 2025-07-22 19:35:07,460] Trial 9 finished with value: 0.9655870445344129 and parameters: {'max_depth': 300, 'learning_rate': 0.03468519568328302, 'n_estimators': 105, 'subsample': 0.7156203524097539, 'colsample_bytree': 0.7311066303376479, 'min_child_samples': 27, 'min_child_weight': 0.00023301033585744919, 'num_leaves': 30}. Best is trial 0 with value: 0.9665991902834008.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[109]\tvalid_0's binary_logloss: 0.572108\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14875\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[105]\tvalid_0's binary_logloss: 0.153329\n",
      "[LightGBM] [Info] Number of positive: 8767, number of negative: 8767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000919 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14875\n",
      "[LightGBM] [Info] Number of data points in the train set: 17534, number of used features: 60\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:07,637] A new study created in memory with name: no-name-5a895fc6-c163-43ad-ac03-2df99e43be20\n",
      "[I 2025-07-22 19:35:07,751] Trial 0 finished with value: 0.9669254134323321 and parameters: {'max_depth': 379, 'learning_rate': 0.05186475600805609, 'n_estimators': 116, 'subsample': 0.9207440152520419, 'colsample_bytree': 0.6829008092857584, 'min_child_samples': 14, 'min_child_weight': 0.0011009895670673466, 'num_leaves': 31}. Best is trial 0 with value: 0.9669254134323321.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8768, number of negative: 8768\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14634\n",
      "[LightGBM] [Info] Number of data points in the train set: 17536, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid_0's binary_logloss: 0.150414\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8768, number of negative: 8768\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000953 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14634\n",
      "[LightGBM] [Info] Number of data points in the train set: 17536, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:07,863] Trial 1 finished with value: 0.9662504218697267 and parameters: {'max_depth': 236, 'learning_rate': 0.08267713663231718, 'n_estimators': 116, 'subsample': 0.943859211540439, 'colsample_bytree': 0.9579437633340612, 'min_child_samples': 27, 'min_child_weight': 0.009219685804773812, 'num_leaves': 39}. Best is trial 0 with value: 0.9669254134323321.\n",
      "[I 2025-07-22 19:35:07,955] Trial 2 finished with value: 0.9659129260884239 and parameters: {'max_depth': 370, 'learning_rate': 0.06516737821690761, 'n_estimators': 111, 'subsample': 0.980813050179286, 'colsample_bytree': 0.8691498540379685, 'min_child_samples': 37, 'min_child_weight': 0.0005687866420849957, 'num_leaves': 24}. Best is trial 0 with value: 0.9669254134323321.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[63]\tvalid_0's binary_logloss: 0.151381\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8768, number of negative: 8768\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000952 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14634\n",
      "[LightGBM] [Info] Number of data points in the train set: 17536, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid_0's binary_logloss: 0.151133\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8768, number of negative: 8768\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14634\n",
      "[LightGBM] [Info] Number of data points in the train set: 17536, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:08,104] Trial 3 finished with value: 0.9652379345258184 and parameters: {'max_depth': 330, 'learning_rate': 0.00776415169774896, 'n_estimators': 110, 'subsample': 0.8705092280259896, 'colsample_bytree': 0.8756139537310029, 'min_child_samples': 23, 'min_child_weight': 0.0003820551731663503, 'num_leaves': 49}. Best is trial 0 with value: 0.9669254134323321.\n",
      "[I 2025-07-22 19:35:08,224] Trial 4 finished with value: 0.9659129260884239 and parameters: {'max_depth': 202, 'learning_rate': 0.0020930418030151742, 'n_estimators': 101, 'subsample': 0.681901993420746, 'colsample_bytree': 0.6000195748318162, 'min_child_samples': 47, 'min_child_weight': 0.007854445914551977, 'num_leaves': 40}. Best is trial 0 with value: 0.9669254134323321.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[110]\tvalid_0's binary_logloss: 0.309013\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8768, number of negative: 8768\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001165 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14634\n",
      "[LightGBM] [Info] Number of data points in the train set: 17536, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[101]\tvalid_0's binary_logloss: 0.545045\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8768, number of negative: 8768\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14634\n",
      "[LightGBM] [Info] Number of data points in the train set: 17536, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:08,369] Trial 5 finished with value: 0.9662504218697267 and parameters: {'max_depth': 151, 'learning_rate': 0.023416487971157654, 'n_estimators': 119, 'subsample': 0.6998629495971478, 'colsample_bytree': 0.9655789678331815, 'min_child_samples': 38, 'min_child_weight': 0.00014823973417382456, 'num_leaves': 36}. Best is trial 0 with value: 0.9669254134323321.\n",
      "[I 2025-07-22 19:35:08,473] Trial 6 finished with value: 0.9659129260884239 and parameters: {'max_depth': 142, 'learning_rate': 0.04951097593904178, 'n_estimators': 113, 'subsample': 0.875710539352367, 'colsample_bytree': 0.7351755529763262, 'min_child_samples': 21, 'min_child_weight': 0.001143022622441546, 'num_leaves': 26}. Best is trial 0 with value: 0.9669254134323321.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[119]\tvalid_0's binary_logloss: 0.158991\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8768, number of negative: 8768\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14634\n",
      "[LightGBM] [Info] Number of data points in the train set: 17536, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid_0's binary_logloss: 0.150181\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8768, number of negative: 8768\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14634\n",
      "[LightGBM] [Info] Number of data points in the train set: 17536, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:08,579] Trial 7 finished with value: 0.9662504218697267 and parameters: {'max_depth': 289, 'learning_rate': 0.026956403912020123, 'n_estimators': 116, 'subsample': 0.778003556604883, 'colsample_bytree': 0.6760752687501879, 'min_child_samples': 12, 'min_child_weight': 0.009821976657390702, 'num_leaves': 20}. Best is trial 0 with value: 0.9669254134323321.\n",
      "[I 2025-07-22 19:35:08,721] Trial 8 finished with value: 0.9662504218697267 and parameters: {'max_depth': 118, 'learning_rate': 0.015240011670876704, 'n_estimators': 116, 'subsample': 0.6798219131631198, 'colsample_bytree': 0.8019260878073905, 'min_child_samples': 12, 'min_child_weight': 0.0009530612260705189, 'num_leaves': 40}. Best is trial 0 with value: 0.9669254134323321.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[116]\tvalid_0's binary_logloss: 0.154264\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8768, number of negative: 8768\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000940 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14634\n",
      "[LightGBM] [Info] Number of data points in the train set: 17536, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[116]\tvalid_0's binary_logloss: 0.190528\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] Number of positive: 8768, number of negative: 8768\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14634\n",
      "[LightGBM] [Info] Number of data points in the train set: 17536, number of used features: 60\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:08,841] Trial 9 finished with value: 0.9652379345258184 and parameters: {'max_depth': 296, 'learning_rate': 0.002612832064182356, 'n_estimators': 102, 'subsample': 0.9165851873910862, 'colsample_bytree': 0.8162818146924289, 'min_child_samples': 32, 'min_child_weight': 0.007964163459537456, 'num_leaves': 40}. Best is trial 0 with value: 0.9669254134323321.\n",
      "[I 2025-07-22 19:35:09,008] A new study created in memory with name: no-name-15d9e19a-cb17-4732-b249-e8246461a77c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[102]\tvalid_0's binary_logloss: 0.514256\n",
      "[LightGBM] [Info] Number of positive: 8768, number of negative: 8768\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001226 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14634\n",
      "[LightGBM] [Info] Number of data points in the train set: 17536, number of used features: 60\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:35:10,675] Trial 0 finished with value: 0.9672739541160594 and parameters: {'depth': 9, 'learning_rate': 0.0898625772875156, 'iterations': 107}. Best is trial 0 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 19:37:45,414] Trial 1 finished with value: 0.9672739541160594 and parameters: {'depth': 16, 'learning_rate': 0.00554352092165663, 'iterations': 109}. Best is trial 0 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 19:38:25,660] Trial 2 finished with value: 0.9672739541160594 and parameters: {'depth': 14, 'learning_rate': 0.004013752915712212, 'iterations': 117}. Best is trial 0 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 19:38:35,883] Trial 3 finished with value: 0.9676113360323887 and parameters: {'depth': 12, 'learning_rate': 0.024534429207373913, 'iterations': 118}. Best is trial 3 with value: 0.9676113360323887.\n",
      "[I 2025-07-22 19:41:04,687] Trial 4 finished with value: 0.9672739541160594 and parameters: {'depth': 16, 'learning_rate': 0.024286032837221264, 'iterations': 105}. Best is trial 3 with value: 0.9676113360323887.\n",
      "[I 2025-07-22 19:41:10,046] Trial 5 finished with value: 0.9672739541160594 and parameters: {'depth': 11, 'learning_rate': 0.02687228440735098, 'iterations': 115}. Best is trial 3 with value: 0.9676113360323887.\n",
      "[I 2025-07-22 19:42:29,778] Trial 6 finished with value: 0.9669365721997301 and parameters: {'depth': 16, 'learning_rate': 0.08314674413494944, 'iterations': 117}. Best is trial 3 with value: 0.9676113360323887.\n",
      "[I 2025-07-22 19:42:30,794] Trial 7 finished with value: 0.9676113360323887 and parameters: {'depth': 8, 'learning_rate': 0.004426786713953809, 'iterations': 116}. Best is trial 3 with value: 0.9676113360323887.\n",
      "[I 2025-07-22 19:42:32,632] Trial 8 finished with value: 0.9669365721997301 and parameters: {'depth': 9, 'learning_rate': 0.001442281687033465, 'iterations': 103}. Best is trial 3 with value: 0.9676113360323887.\n",
      "[I 2025-07-22 19:42:37,702] Trial 9 finished with value: 0.9672739541160594 and parameters: {'depth': 11, 'learning_rate': 0.008943383136093042, 'iterations': 109}. Best is trial 3 with value: 0.9676113360323887.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6555748\ttotal: 85.4ms\tremaining: 9.99s\n",
      "1:\tlearn: 0.6207656\ttotal: 173ms\tremaining: 10s\n",
      "2:\tlearn: 0.5888348\ttotal: 253ms\tremaining: 9.72s\n",
      "3:\tlearn: 0.5583391\ttotal: 337ms\tremaining: 9.59s\n",
      "4:\tlearn: 0.5308493\ttotal: 418ms\tremaining: 9.46s\n",
      "5:\tlearn: 0.5047355\ttotal: 511ms\tremaining: 9.53s\n",
      "6:\tlearn: 0.4803312\ttotal: 604ms\tremaining: 9.58s\n",
      "7:\tlearn: 0.4580069\ttotal: 691ms\tremaining: 9.51s\n",
      "8:\tlearn: 0.4376102\ttotal: 773ms\tremaining: 9.36s\n",
      "9:\tlearn: 0.4182930\ttotal: 853ms\tremaining: 9.21s\n",
      "10:\tlearn: 0.4005626\ttotal: 934ms\tremaining: 9.09s\n",
      "11:\tlearn: 0.3841341\ttotal: 1.01s\tremaining: 8.96s\n",
      "12:\tlearn: 0.3688082\ttotal: 1.1s\tremaining: 8.91s\n",
      "13:\tlearn: 0.3545503\ttotal: 1.19s\tremaining: 8.82s\n",
      "14:\tlearn: 0.3413026\ttotal: 1.27s\tremaining: 8.7s\n",
      "15:\tlearn: 0.3290427\ttotal: 1.35s\tremaining: 8.6s\n",
      "16:\tlearn: 0.3175595\ttotal: 1.43s\tremaining: 8.5s\n",
      "17:\tlearn: 0.3070285\ttotal: 1.52s\tremaining: 8.42s\n",
      "18:\tlearn: 0.2973749\ttotal: 1.59s\tremaining: 8.31s\n",
      "19:\tlearn: 0.2882558\ttotal: 1.67s\tremaining: 8.2s\n",
      "20:\tlearn: 0.2795190\ttotal: 1.75s\tremaining: 8.11s\n",
      "21:\tlearn: 0.2715770\ttotal: 1.85s\tremaining: 8.06s\n",
      "22:\tlearn: 0.2641191\ttotal: 1.94s\tremaining: 8.01s\n",
      "23:\tlearn: 0.2570181\ttotal: 2.03s\tremaining: 7.97s\n",
      "24:\tlearn: 0.2506459\ttotal: 2.12s\tremaining: 7.9s\n",
      "25:\tlearn: 0.2444776\ttotal: 2.21s\tremaining: 7.8s\n",
      "26:\tlearn: 0.2386763\ttotal: 2.29s\tremaining: 7.7s\n",
      "27:\tlearn: 0.2334313\ttotal: 2.37s\tremaining: 7.61s\n",
      "28:\tlearn: 0.2280139\ttotal: 2.44s\tremaining: 7.5s\n",
      "29:\tlearn: 0.2233352\ttotal: 2.52s\tremaining: 7.4s\n",
      "30:\tlearn: 0.2185804\ttotal: 2.6s\tremaining: 7.31s\n",
      "31:\tlearn: 0.2141614\ttotal: 2.68s\tremaining: 7.21s\n",
      "32:\tlearn: 0.2099892\ttotal: 2.76s\tremaining: 7.12s\n",
      "33:\tlearn: 0.2065984\ttotal: 2.85s\tremaining: 7.04s\n",
      "34:\tlearn: 0.2031272\ttotal: 2.93s\tremaining: 6.96s\n",
      "35:\tlearn: 0.2000339\ttotal: 3.02s\tremaining: 6.88s\n",
      "36:\tlearn: 0.1970203\ttotal: 3.1s\tremaining: 6.79s\n",
      "37:\tlearn: 0.1938005\ttotal: 3.18s\tremaining: 6.7s\n",
      "38:\tlearn: 0.1907044\ttotal: 3.26s\tremaining: 6.61s\n",
      "39:\tlearn: 0.1883178\ttotal: 3.35s\tremaining: 6.53s\n",
      "40:\tlearn: 0.1859738\ttotal: 3.43s\tremaining: 6.44s\n",
      "41:\tlearn: 0.1832979\ttotal: 3.51s\tremaining: 6.34s\n",
      "42:\tlearn: 0.1812126\ttotal: 3.59s\tremaining: 6.26s\n",
      "43:\tlearn: 0.1788121\ttotal: 3.68s\tremaining: 6.19s\n",
      "44:\tlearn: 0.1766626\ttotal: 3.77s\tremaining: 6.11s\n",
      "45:\tlearn: 0.1749178\ttotal: 3.86s\tremaining: 6.04s\n",
      "46:\tlearn: 0.1729567\ttotal: 3.94s\tremaining: 5.95s\n",
      "47:\tlearn: 0.1709106\ttotal: 4.02s\tremaining: 5.87s\n",
      "48:\tlearn: 0.1693655\ttotal: 4.11s\tremaining: 5.79s\n",
      "49:\tlearn: 0.1678537\ttotal: 4.2s\tremaining: 5.71s\n",
      "50:\tlearn: 0.1660186\ttotal: 4.29s\tremaining: 5.64s\n",
      "51:\tlearn: 0.1647150\ttotal: 4.38s\tremaining: 5.56s\n",
      "52:\tlearn: 0.1631850\ttotal: 4.47s\tremaining: 5.49s\n",
      "53:\tlearn: 0.1618807\ttotal: 4.56s\tremaining: 5.41s\n",
      "54:\tlearn: 0.1603576\ttotal: 4.64s\tremaining: 5.32s\n",
      "55:\tlearn: 0.1594052\ttotal: 4.72s\tremaining: 5.23s\n",
      "56:\tlearn: 0.1582465\ttotal: 4.8s\tremaining: 5.13s\n",
      "57:\tlearn: 0.1573531\ttotal: 4.88s\tremaining: 5.05s\n",
      "58:\tlearn: 0.1560454\ttotal: 4.96s\tremaining: 4.96s\n",
      "59:\tlearn: 0.1552411\ttotal: 5.04s\tremaining: 4.87s\n",
      "60:\tlearn: 0.1538939\ttotal: 5.13s\tremaining: 4.79s\n",
      "61:\tlearn: 0.1529890\ttotal: 5.21s\tremaining: 4.7s\n",
      "62:\tlearn: 0.1520106\ttotal: 5.29s\tremaining: 4.61s\n",
      "63:\tlearn: 0.1510171\ttotal: 5.37s\tremaining: 4.53s\n",
      "64:\tlearn: 0.1501942\ttotal: 5.45s\tremaining: 4.45s\n",
      "65:\tlearn: 0.1494176\ttotal: 5.53s\tremaining: 4.36s\n",
      "66:\tlearn: 0.1484927\ttotal: 5.61s\tremaining: 4.27s\n",
      "67:\tlearn: 0.1476699\ttotal: 5.7s\tremaining: 4.19s\n",
      "68:\tlearn: 0.1470438\ttotal: 5.78s\tremaining: 4.1s\n",
      "69:\tlearn: 0.1462351\ttotal: 5.87s\tremaining: 4.02s\n",
      "70:\tlearn: 0.1455591\ttotal: 5.95s\tremaining: 3.94s\n",
      "71:\tlearn: 0.1447358\ttotal: 6.04s\tremaining: 3.86s\n",
      "72:\tlearn: 0.1441926\ttotal: 6.12s\tremaining: 3.77s\n",
      "73:\tlearn: 0.1434573\ttotal: 6.21s\tremaining: 3.69s\n",
      "74:\tlearn: 0.1426506\ttotal: 6.29s\tremaining: 3.61s\n",
      "75:\tlearn: 0.1420898\ttotal: 6.37s\tremaining: 3.52s\n",
      "76:\tlearn: 0.1415570\ttotal: 6.46s\tremaining: 3.44s\n",
      "77:\tlearn: 0.1410089\ttotal: 6.54s\tremaining: 3.35s\n",
      "78:\tlearn: 0.1402237\ttotal: 6.62s\tremaining: 3.27s\n",
      "79:\tlearn: 0.1395827\ttotal: 6.71s\tremaining: 3.19s\n",
      "80:\tlearn: 0.1387491\ttotal: 6.79s\tremaining: 3.1s\n",
      "81:\tlearn: 0.1380243\ttotal: 6.88s\tremaining: 3.02s\n",
      "82:\tlearn: 0.1374273\ttotal: 6.96s\tremaining: 2.94s\n",
      "83:\tlearn: 0.1369489\ttotal: 7.05s\tremaining: 2.85s\n",
      "84:\tlearn: 0.1364064\ttotal: 7.13s\tremaining: 2.77s\n",
      "85:\tlearn: 0.1359326\ttotal: 7.21s\tremaining: 2.68s\n",
      "86:\tlearn: 0.1353587\ttotal: 7.29s\tremaining: 2.6s\n",
      "87:\tlearn: 0.1350141\ttotal: 7.37s\tremaining: 2.51s\n",
      "88:\tlearn: 0.1345065\ttotal: 7.45s\tremaining: 2.43s\n",
      "89:\tlearn: 0.1341006\ttotal: 7.54s\tremaining: 2.35s\n",
      "90:\tlearn: 0.1335853\ttotal: 7.62s\tremaining: 2.26s\n",
      "91:\tlearn: 0.1330193\ttotal: 7.7s\tremaining: 2.17s\n",
      "92:\tlearn: 0.1323999\ttotal: 7.78s\tremaining: 2.09s\n",
      "93:\tlearn: 0.1318786\ttotal: 7.86s\tremaining: 2s\n",
      "94:\tlearn: 0.1312566\ttotal: 7.93s\tremaining: 1.92s\n",
      "95:\tlearn: 0.1308239\ttotal: 8.02s\tremaining: 1.84s\n",
      "96:\tlearn: 0.1303694\ttotal: 8.1s\tremaining: 1.75s\n",
      "97:\tlearn: 0.1298776\ttotal: 8.18s\tremaining: 1.67s\n",
      "98:\tlearn: 0.1292442\ttotal: 8.27s\tremaining: 1.59s\n",
      "99:\tlearn: 0.1286642\ttotal: 8.35s\tremaining: 1.5s\n",
      "100:\tlearn: 0.1280165\ttotal: 8.43s\tremaining: 1.42s\n",
      "101:\tlearn: 0.1273206\ttotal: 8.51s\tremaining: 1.33s\n",
      "102:\tlearn: 0.1266884\ttotal: 8.59s\tremaining: 1.25s\n",
      "103:\tlearn: 0.1263155\ttotal: 8.68s\tremaining: 1.17s\n",
      "104:\tlearn: 0.1260764\ttotal: 8.77s\tremaining: 1.08s\n",
      "105:\tlearn: 0.1257039\ttotal: 8.86s\tremaining: 1s\n",
      "106:\tlearn: 0.1251370\ttotal: 8.95s\tremaining: 920ms\n",
      "107:\tlearn: 0.1245724\ttotal: 9.04s\tremaining: 837ms\n",
      "108:\tlearn: 0.1242305\ttotal: 9.13s\tremaining: 754ms\n",
      "109:\tlearn: 0.1238114\ttotal: 9.22s\tremaining: 670ms\n",
      "110:\tlearn: 0.1234999\ttotal: 9.3s\tremaining: 586ms\n",
      "111:\tlearn: 0.1232137\ttotal: 9.38s\tremaining: 502ms\n",
      "112:\tlearn: 0.1228628\ttotal: 9.46s\tremaining: 418ms\n",
      "113:\tlearn: 0.1223876\ttotal: 9.54s\tremaining: 335ms\n",
      "114:\tlearn: 0.1219871\ttotal: 9.63s\tremaining: 251ms\n",
      "115:\tlearn: 0.1217728\ttotal: 9.71s\tremaining: 167ms\n",
      "116:\tlearn: 0.1213916\ttotal: 9.8s\tremaining: 83.7ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:42:47,857] A new study created in memory with name: no-name-73703fc7-2ad4-4b8c-9aab-d9284faef5a5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117:\tlearn: 0.1209527\ttotal: 9.89s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:43:24,615] Trial 0 finished with value: 0.9733468286099866 and parameters: {'depth': 14, 'learning_rate': 0.00811308250838698, 'iterations': 106}. Best is trial 0 with value: 0.9733468286099866.\n",
      "[I 2025-07-22 19:43:29,729] Trial 1 finished with value: 0.9733468286099866 and parameters: {'depth': 11, 'learning_rate': 0.015363634026110849, 'iterations': 115}. Best is trial 0 with value: 0.9733468286099866.\n",
      "[I 2025-07-22 19:43:32,833] Trial 2 finished with value: 0.9733468286099866 and parameters: {'depth': 10, 'learning_rate': 0.008907839332730755, 'iterations': 119}. Best is trial 0 with value: 0.9733468286099866.\n",
      "[I 2025-07-22 19:44:09,586] Trial 3 finished with value: 0.9736842105263158 and parameters: {'depth': 14, 'learning_rate': 0.0011129720100631789, 'iterations': 106}. Best is trial 3 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:44:10,225] Trial 4 finished with value: 0.9733468286099866 and parameters: {'depth': 7, 'learning_rate': 0.0014198654831261966, 'iterations': 102}. Best is trial 3 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:46:17,668] Trial 5 finished with value: 0.9736842105263158 and parameters: {'depth': 16, 'learning_rate': 0.05226069492177232, 'iterations': 103}. Best is trial 3 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:48:50,426] Trial 6 finished with value: 0.9736842105263158 and parameters: {'depth': 16, 'learning_rate': 0.02157353232764569, 'iterations': 113}. Best is trial 3 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:48:55,239] Trial 7 finished with value: 0.9733468286099866 and parameters: {'depth': 11, 'learning_rate': 0.009574018770557005, 'iterations': 102}. Best is trial 3 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:49:05,621] Trial 8 finished with value: 0.9733468286099866 and parameters: {'depth': 12, 'learning_rate': 0.005838274871105928, 'iterations': 106}. Best is trial 3 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 19:49:08,302] Trial 9 finished with value: 0.9733468286099866 and parameters: {'depth': 10, 'learning_rate': 0.08411633172487408, 'iterations': 119}. Best is trial 3 with value: 0.9736842105263158.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6913909\ttotal: 325ms\tremaining: 34.1s\n",
      "1:\tlearn: 0.6896506\ttotal: 690ms\tremaining: 35.9s\n",
      "2:\tlearn: 0.6879429\ttotal: 1.06s\tremaining: 36.3s\n",
      "3:\tlearn: 0.6862614\ttotal: 1.41s\tremaining: 35.9s\n",
      "4:\tlearn: 0.6845315\ttotal: 1.76s\tremaining: 35.6s\n",
      "5:\tlearn: 0.6827726\ttotal: 2.1s\tremaining: 35s\n",
      "6:\tlearn: 0.6811056\ttotal: 2.44s\tremaining: 34.5s\n",
      "7:\tlearn: 0.6793977\ttotal: 2.81s\tremaining: 34.4s\n",
      "8:\tlearn: 0.6777242\ttotal: 3.16s\tremaining: 34.1s\n",
      "9:\tlearn: 0.6761128\ttotal: 3.5s\tremaining: 33.6s\n",
      "10:\tlearn: 0.6744780\ttotal: 3.87s\tremaining: 33.4s\n",
      "11:\tlearn: 0.6728064\ttotal: 4.21s\tremaining: 33s\n",
      "12:\tlearn: 0.6711775\ttotal: 4.56s\tremaining: 32.6s\n",
      "13:\tlearn: 0.6695351\ttotal: 4.91s\tremaining: 32.3s\n",
      "14:\tlearn: 0.6678755\ttotal: 5.26s\tremaining: 31.9s\n",
      "15:\tlearn: 0.6662429\ttotal: 5.61s\tremaining: 31.6s\n",
      "16:\tlearn: 0.6646181\ttotal: 5.97s\tremaining: 31.3s\n",
      "17:\tlearn: 0.6629382\ttotal: 6.33s\tremaining: 30.9s\n",
      "18:\tlearn: 0.6612873\ttotal: 6.67s\tremaining: 30.6s\n",
      "19:\tlearn: 0.6596472\ttotal: 7.01s\tremaining: 30.1s\n",
      "20:\tlearn: 0.6580209\ttotal: 7.35s\tremaining: 29.8s\n",
      "21:\tlearn: 0.6564216\ttotal: 7.68s\tremaining: 29.3s\n",
      "22:\tlearn: 0.6547683\ttotal: 8.05s\tremaining: 29.1s\n",
      "23:\tlearn: 0.6531607\ttotal: 8.41s\tremaining: 28.7s\n",
      "24:\tlearn: 0.6515448\ttotal: 8.76s\tremaining: 28.4s\n",
      "25:\tlearn: 0.6499857\ttotal: 9.13s\tremaining: 28.1s\n",
      "26:\tlearn: 0.6483846\ttotal: 9.5s\tremaining: 27.8s\n",
      "27:\tlearn: 0.6467897\ttotal: 9.84s\tremaining: 27.4s\n",
      "28:\tlearn: 0.6451875\ttotal: 10.2s\tremaining: 27s\n",
      "29:\tlearn: 0.6436172\ttotal: 10.5s\tremaining: 26.7s\n",
      "30:\tlearn: 0.6420231\ttotal: 10.9s\tremaining: 26.3s\n",
      "31:\tlearn: 0.6404794\ttotal: 11.2s\tremaining: 26s\n",
      "32:\tlearn: 0.6388864\ttotal: 11.6s\tremaining: 25.6s\n",
      "33:\tlearn: 0.6373510\ttotal: 11.9s\tremaining: 25.2s\n",
      "34:\tlearn: 0.6357783\ttotal: 12.3s\tremaining: 24.9s\n",
      "35:\tlearn: 0.6342212\ttotal: 12.6s\tremaining: 24.5s\n",
      "36:\tlearn: 0.6326887\ttotal: 12.9s\tremaining: 24.1s\n",
      "37:\tlearn: 0.6311167\ttotal: 13.3s\tremaining: 23.7s\n",
      "38:\tlearn: 0.6295924\ttotal: 13.6s\tremaining: 23.4s\n",
      "39:\tlearn: 0.6280491\ttotal: 14s\tremaining: 23s\n",
      "40:\tlearn: 0.6265103\ttotal: 14.3s\tremaining: 22.7s\n",
      "41:\tlearn: 0.6250031\ttotal: 14.7s\tremaining: 22.3s\n",
      "42:\tlearn: 0.6235146\ttotal: 15s\tremaining: 22s\n",
      "43:\tlearn: 0.6220498\ttotal: 15.3s\tremaining: 21.6s\n",
      "44:\tlearn: 0.6205373\ttotal: 15.7s\tremaining: 21.2s\n",
      "45:\tlearn: 0.6190296\ttotal: 16s\tremaining: 20.9s\n",
      "46:\tlearn: 0.6175219\ttotal: 16.4s\tremaining: 20.6s\n",
      "47:\tlearn: 0.6160361\ttotal: 16.7s\tremaining: 20.2s\n",
      "48:\tlearn: 0.6145644\ttotal: 17s\tremaining: 19.8s\n",
      "49:\tlearn: 0.6130511\ttotal: 17.4s\tremaining: 19.5s\n",
      "50:\tlearn: 0.6115800\ttotal: 17.7s\tremaining: 19.1s\n",
      "51:\tlearn: 0.6101351\ttotal: 18.1s\tremaining: 18.8s\n",
      "52:\tlearn: 0.6086911\ttotal: 18.4s\tremaining: 18.4s\n",
      "53:\tlearn: 0.6072277\ttotal: 18.8s\tremaining: 18.1s\n",
      "54:\tlearn: 0.6057326\ttotal: 19.1s\tremaining: 17.7s\n",
      "55:\tlearn: 0.6043056\ttotal: 19.5s\tremaining: 17.4s\n",
      "56:\tlearn: 0.6028406\ttotal: 19.8s\tremaining: 17s\n",
      "57:\tlearn: 0.6014077\ttotal: 20.1s\tremaining: 16.7s\n",
      "58:\tlearn: 0.5999597\ttotal: 20.5s\tremaining: 16.3s\n",
      "59:\tlearn: 0.5984966\ttotal: 20.8s\tremaining: 16s\n",
      "60:\tlearn: 0.5970378\ttotal: 21.2s\tremaining: 15.6s\n",
      "61:\tlearn: 0.5955871\ttotal: 21.5s\tremaining: 15.3s\n",
      "62:\tlearn: 0.5941861\ttotal: 21.8s\tremaining: 14.9s\n",
      "63:\tlearn: 0.5927741\ttotal: 22.2s\tremaining: 14.6s\n",
      "64:\tlearn: 0.5913870\ttotal: 22.5s\tremaining: 14.2s\n",
      "65:\tlearn: 0.5900097\ttotal: 22.9s\tremaining: 13.9s\n",
      "66:\tlearn: 0.5886094\ttotal: 23.2s\tremaining: 13.5s\n",
      "67:\tlearn: 0.5872421\ttotal: 23.6s\tremaining: 13.2s\n",
      "68:\tlearn: 0.5858091\ttotal: 23.9s\tremaining: 12.8s\n",
      "69:\tlearn: 0.5844296\ttotal: 24.3s\tremaining: 12.5s\n",
      "70:\tlearn: 0.5830382\ttotal: 24.6s\tremaining: 12.2s\n",
      "71:\tlearn: 0.5816801\ttotal: 25s\tremaining: 11.8s\n",
      "72:\tlearn: 0.5803561\ttotal: 25.3s\tremaining: 11.4s\n",
      "73:\tlearn: 0.5789589\ttotal: 25.7s\tremaining: 11.1s\n",
      "74:\tlearn: 0.5776107\ttotal: 26s\tremaining: 10.7s\n",
      "75:\tlearn: 0.5762471\ttotal: 26.3s\tremaining: 10.4s\n",
      "76:\tlearn: 0.5748503\ttotal: 26.7s\tremaining: 10s\n",
      "77:\tlearn: 0.5735445\ttotal: 27s\tremaining: 9.69s\n",
      "78:\tlearn: 0.5722036\ttotal: 27.4s\tremaining: 9.35s\n",
      "79:\tlearn: 0.5708622\ttotal: 27.7s\tremaining: 9s\n",
      "80:\tlearn: 0.5695380\ttotal: 28s\tremaining: 8.65s\n",
      "81:\tlearn: 0.5682061\ttotal: 28.4s\tremaining: 8.3s\n",
      "82:\tlearn: 0.5668752\ttotal: 28.7s\tremaining: 7.96s\n",
      "83:\tlearn: 0.5655468\ttotal: 29.1s\tremaining: 7.62s\n",
      "84:\tlearn: 0.5642552\ttotal: 29.4s\tremaining: 7.27s\n",
      "85:\tlearn: 0.5629425\ttotal: 29.8s\tremaining: 6.93s\n",
      "86:\tlearn: 0.5616251\ttotal: 30.1s\tremaining: 6.58s\n",
      "87:\tlearn: 0.5603461\ttotal: 30.5s\tremaining: 6.24s\n",
      "88:\tlearn: 0.5590385\ttotal: 30.8s\tremaining: 5.89s\n",
      "89:\tlearn: 0.5577534\ttotal: 31.2s\tremaining: 5.54s\n",
      "90:\tlearn: 0.5564471\ttotal: 31.5s\tremaining: 5.19s\n",
      "91:\tlearn: 0.5551562\ttotal: 31.8s\tremaining: 4.84s\n",
      "92:\tlearn: 0.5538705\ttotal: 32.2s\tremaining: 4.5s\n",
      "93:\tlearn: 0.5525680\ttotal: 32.5s\tremaining: 4.15s\n",
      "94:\tlearn: 0.5513163\ttotal: 32.9s\tremaining: 3.81s\n",
      "95:\tlearn: 0.5500635\ttotal: 33.2s\tremaining: 3.46s\n",
      "96:\tlearn: 0.5487718\ttotal: 33.6s\tremaining: 3.12s\n",
      "97:\tlearn: 0.5475352\ttotal: 33.9s\tremaining: 2.77s\n",
      "98:\tlearn: 0.5462185\ttotal: 34.3s\tremaining: 2.42s\n",
      "99:\tlearn: 0.5449879\ttotal: 34.6s\tremaining: 2.08s\n",
      "100:\tlearn: 0.5437542\ttotal: 34.9s\tremaining: 1.73s\n",
      "101:\tlearn: 0.5424799\ttotal: 35.3s\tremaining: 1.38s\n",
      "102:\tlearn: 0.5411959\ttotal: 35.6s\tremaining: 1.04s\n",
      "103:\tlearn: 0.5399980\ttotal: 36s\tremaining: 692ms\n",
      "104:\tlearn: 0.5387905\ttotal: 36.3s\tremaining: 346ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:49:45,243] A new study created in memory with name: no-name-781f37ef-eb5f-4cbf-a820-7b74d29c1b45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105:\tlearn: 0.5375669\ttotal: 36.6s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:49:52,895] Trial 0 finished with value: 0.9689608636977058 and parameters: {'depth': 12, 'learning_rate': 0.07103252364344466, 'iterations': 115}. Best is trial 0 with value: 0.9689608636977058.\n",
      "[I 2025-07-22 19:50:12,608] Trial 1 finished with value: 0.9692982456140351 and parameters: {'depth': 13, 'learning_rate': 0.019202098505353142, 'iterations': 115}. Best is trial 1 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 19:50:13,697] Trial 2 finished with value: 0.9689608636977058 and parameters: {'depth': 8, 'learning_rate': 0.07180411174348579, 'iterations': 119}. Best is trial 1 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 19:50:15,243] Trial 3 finished with value: 0.9689608636977058 and parameters: {'depth': 9, 'learning_rate': 0.03074441907213939, 'iterations': 108}. Best is trial 1 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 19:50:51,122] Trial 4 finished with value: 0.9692982456140351 and parameters: {'depth': 14, 'learning_rate': 0.0017119050307880372, 'iterations': 104}. Best is trial 1 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 19:50:53,232] Trial 5 finished with value: 0.9692982456140351 and parameters: {'depth': 9, 'learning_rate': 0.002709792342801735, 'iterations': 101}. Best is trial 1 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 19:51:02,359] Trial 6 finished with value: 0.9692982456140351 and parameters: {'depth': 12, 'learning_rate': 0.0019529778612713614, 'iterations': 108}. Best is trial 1 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 19:51:07,028] Trial 7 finished with value: 0.9692982456140351 and parameters: {'depth': 11, 'learning_rate': 0.014435139821361395, 'iterations': 102}. Best is trial 1 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 19:51:27,154] Trial 8 finished with value: 0.9689608636977058 and parameters: {'depth': 13, 'learning_rate': 0.012211458780397918, 'iterations': 118}. Best is trial 1 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 19:52:08,452] Trial 9 finished with value: 0.9689608636977058 and parameters: {'depth': 14, 'learning_rate': 0.01265712141948144, 'iterations': 120}. Best is trial 1 with value: 0.9692982456140351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6631445\ttotal: 188ms\tremaining: 21.5s\n",
      "1:\tlearn: 0.6346828\ttotal: 362ms\tremaining: 20.5s\n",
      "2:\tlearn: 0.6086366\ttotal: 541ms\tremaining: 20.2s\n",
      "3:\tlearn: 0.5839373\ttotal: 713ms\tremaining: 19.8s\n",
      "4:\tlearn: 0.5596415\ttotal: 877ms\tremaining: 19.3s\n",
      "5:\tlearn: 0.5375262\ttotal: 1.04s\tremaining: 18.9s\n",
      "6:\tlearn: 0.5165982\ttotal: 1.22s\tremaining: 18.8s\n",
      "7:\tlearn: 0.4970749\ttotal: 1.39s\tremaining: 18.5s\n",
      "8:\tlearn: 0.4787870\ttotal: 1.55s\tremaining: 18.3s\n",
      "9:\tlearn: 0.4613737\ttotal: 1.73s\tremaining: 18.2s\n",
      "10:\tlearn: 0.4448206\ttotal: 1.91s\tremaining: 18s\n",
      "11:\tlearn: 0.4291148\ttotal: 2.09s\tremaining: 17.9s\n",
      "12:\tlearn: 0.4146510\ttotal: 2.25s\tremaining: 17.7s\n",
      "13:\tlearn: 0.4007985\ttotal: 2.42s\tremaining: 17.5s\n",
      "14:\tlearn: 0.3878795\ttotal: 2.6s\tremaining: 17.3s\n",
      "15:\tlearn: 0.3751744\ttotal: 2.77s\tremaining: 17.1s\n",
      "16:\tlearn: 0.3635401\ttotal: 2.95s\tremaining: 17s\n",
      "17:\tlearn: 0.3526051\ttotal: 3.12s\tremaining: 16.8s\n",
      "18:\tlearn: 0.3420060\ttotal: 3.28s\tremaining: 16.6s\n",
      "19:\tlearn: 0.3323659\ttotal: 3.46s\tremaining: 16.4s\n",
      "20:\tlearn: 0.3230630\ttotal: 3.63s\tremaining: 16.2s\n",
      "21:\tlearn: 0.3144182\ttotal: 3.79s\tremaining: 16s\n",
      "22:\tlearn: 0.3060241\ttotal: 3.96s\tremaining: 15.8s\n",
      "23:\tlearn: 0.2976121\ttotal: 4.13s\tremaining: 15.7s\n",
      "24:\tlearn: 0.2899565\ttotal: 4.32s\tremaining: 15.6s\n",
      "25:\tlearn: 0.2827956\ttotal: 4.49s\tremaining: 15.4s\n",
      "26:\tlearn: 0.2760981\ttotal: 4.65s\tremaining: 15.2s\n",
      "27:\tlearn: 0.2699741\ttotal: 4.81s\tremaining: 15s\n",
      "28:\tlearn: 0.2642417\ttotal: 4.98s\tremaining: 14.8s\n",
      "29:\tlearn: 0.2587746\ttotal: 5.14s\tremaining: 14.6s\n",
      "30:\tlearn: 0.2529844\ttotal: 5.32s\tremaining: 14.4s\n",
      "31:\tlearn: 0.2474303\ttotal: 5.5s\tremaining: 14.3s\n",
      "32:\tlearn: 0.2425508\ttotal: 5.66s\tremaining: 14.1s\n",
      "33:\tlearn: 0.2374275\ttotal: 5.83s\tremaining: 13.9s\n",
      "34:\tlearn: 0.2333481\ttotal: 6s\tremaining: 13.7s\n",
      "35:\tlearn: 0.2289860\ttotal: 6.17s\tremaining: 13.5s\n",
      "36:\tlearn: 0.2250723\ttotal: 6.34s\tremaining: 13.4s\n",
      "37:\tlearn: 0.2214914\ttotal: 6.51s\tremaining: 13.2s\n",
      "38:\tlearn: 0.2174626\ttotal: 6.69s\tremaining: 13s\n",
      "39:\tlearn: 0.2140904\ttotal: 6.85s\tremaining: 12.8s\n",
      "40:\tlearn: 0.2102298\ttotal: 7.03s\tremaining: 12.7s\n",
      "41:\tlearn: 0.2067668\ttotal: 7.2s\tremaining: 12.5s\n",
      "42:\tlearn: 0.2038356\ttotal: 7.37s\tremaining: 12.3s\n",
      "43:\tlearn: 0.2012635\ttotal: 7.53s\tremaining: 12.2s\n",
      "44:\tlearn: 0.1980174\ttotal: 7.7s\tremaining: 12s\n",
      "45:\tlearn: 0.1955067\ttotal: 7.86s\tremaining: 11.8s\n",
      "46:\tlearn: 0.1927259\ttotal: 8.03s\tremaining: 11.6s\n",
      "47:\tlearn: 0.1901694\ttotal: 8.21s\tremaining: 11.5s\n",
      "48:\tlearn: 0.1878281\ttotal: 8.38s\tremaining: 11.3s\n",
      "49:\tlearn: 0.1857468\ttotal: 8.55s\tremaining: 11.1s\n",
      "50:\tlearn: 0.1837998\ttotal: 8.71s\tremaining: 10.9s\n",
      "51:\tlearn: 0.1817262\ttotal: 8.8s\tremaining: 10.7s\n",
      "52:\tlearn: 0.1799519\ttotal: 8.98s\tremaining: 10.5s\n",
      "53:\tlearn: 0.1779612\ttotal: 9.15s\tremaining: 10.3s\n",
      "54:\tlearn: 0.1762125\ttotal: 9.32s\tremaining: 10.2s\n",
      "55:\tlearn: 0.1747190\ttotal: 9.48s\tremaining: 9.99s\n",
      "56:\tlearn: 0.1728962\ttotal: 9.64s\tremaining: 9.81s\n",
      "57:\tlearn: 0.1713101\ttotal: 9.81s\tremaining: 9.65s\n",
      "58:\tlearn: 0.1697046\ttotal: 9.98s\tremaining: 9.48s\n",
      "59:\tlearn: 0.1682020\ttotal: 10.2s\tremaining: 9.31s\n",
      "60:\tlearn: 0.1668331\ttotal: 10.3s\tremaining: 9.14s\n",
      "61:\tlearn: 0.1655441\ttotal: 10.5s\tremaining: 8.97s\n",
      "62:\tlearn: 0.1643696\ttotal: 10.7s\tremaining: 8.8s\n",
      "63:\tlearn: 0.1630688\ttotal: 10.8s\tremaining: 8.64s\n",
      "64:\tlearn: 0.1618428\ttotal: 11s\tremaining: 8.48s\n",
      "65:\tlearn: 0.1607486\ttotal: 11.2s\tremaining: 8.3s\n",
      "66:\tlearn: 0.1594401\ttotal: 11.3s\tremaining: 8.13s\n",
      "67:\tlearn: 0.1584946\ttotal: 11.5s\tremaining: 7.96s\n",
      "68:\tlearn: 0.1576252\ttotal: 11.7s\tremaining: 7.78s\n",
      "69:\tlearn: 0.1561575\ttotal: 11.8s\tremaining: 7.61s\n",
      "70:\tlearn: 0.1551568\ttotal: 12s\tremaining: 7.45s\n",
      "71:\tlearn: 0.1540815\ttotal: 12.2s\tremaining: 7.28s\n",
      "72:\tlearn: 0.1531343\ttotal: 12.4s\tremaining: 7.12s\n",
      "73:\tlearn: 0.1521432\ttotal: 12.5s\tremaining: 6.95s\n",
      "74:\tlearn: 0.1514774\ttotal: 12.7s\tremaining: 6.78s\n",
      "75:\tlearn: 0.1504368\ttotal: 12.9s\tremaining: 6.61s\n",
      "76:\tlearn: 0.1497213\ttotal: 13s\tremaining: 6.43s\n",
      "77:\tlearn: 0.1486761\ttotal: 13.2s\tremaining: 6.26s\n",
      "78:\tlearn: 0.1476435\ttotal: 13.4s\tremaining: 6.09s\n",
      "79:\tlearn: 0.1470020\ttotal: 13.5s\tremaining: 5.93s\n",
      "80:\tlearn: 0.1461042\ttotal: 13.7s\tremaining: 5.76s\n",
      "81:\tlearn: 0.1451489\ttotal: 13.9s\tremaining: 5.59s\n",
      "82:\tlearn: 0.1445246\ttotal: 14.1s\tremaining: 5.43s\n",
      "83:\tlearn: 0.1440372\ttotal: 14.2s\tremaining: 5.25s\n",
      "84:\tlearn: 0.1433849\ttotal: 14.4s\tremaining: 5.08s\n",
      "85:\tlearn: 0.1426366\ttotal: 14.6s\tremaining: 4.92s\n",
      "86:\tlearn: 0.1421189\ttotal: 14.7s\tremaining: 4.74s\n",
      "87:\tlearn: 0.1415806\ttotal: 14.9s\tremaining: 4.57s\n",
      "88:\tlearn: 0.1410402\ttotal: 15.1s\tremaining: 4.4s\n",
      "89:\tlearn: 0.1402760\ttotal: 15.2s\tremaining: 4.23s\n",
      "90:\tlearn: 0.1396888\ttotal: 15.4s\tremaining: 4.07s\n",
      "91:\tlearn: 0.1391765\ttotal: 15.6s\tremaining: 3.9s\n",
      "92:\tlearn: 0.1384715\ttotal: 15.8s\tremaining: 3.73s\n",
      "93:\tlearn: 0.1377929\ttotal: 15.9s\tremaining: 3.56s\n",
      "94:\tlearn: 0.1369007\ttotal: 16.1s\tremaining: 3.39s\n",
      "95:\tlearn: 0.1363613\ttotal: 16.3s\tremaining: 3.22s\n",
      "96:\tlearn: 0.1358184\ttotal: 16.4s\tremaining: 3.05s\n",
      "97:\tlearn: 0.1353115\ttotal: 16.6s\tremaining: 2.88s\n",
      "98:\tlearn: 0.1347692\ttotal: 16.8s\tremaining: 2.71s\n",
      "99:\tlearn: 0.1343182\ttotal: 16.9s\tremaining: 2.54s\n",
      "100:\tlearn: 0.1339447\ttotal: 17.1s\tremaining: 2.37s\n",
      "101:\tlearn: 0.1334241\ttotal: 17.3s\tremaining: 2.2s\n",
      "102:\tlearn: 0.1330587\ttotal: 17.4s\tremaining: 2.03s\n",
      "103:\tlearn: 0.1325687\ttotal: 17.6s\tremaining: 1.86s\n",
      "104:\tlearn: 0.1321216\ttotal: 17.8s\tremaining: 1.69s\n",
      "105:\tlearn: 0.1314733\ttotal: 17.9s\tremaining: 1.52s\n",
      "106:\tlearn: 0.1311070\ttotal: 18.1s\tremaining: 1.35s\n",
      "107:\tlearn: 0.1306574\ttotal: 18.3s\tremaining: 1.19s\n",
      "108:\tlearn: 0.1303432\ttotal: 18.5s\tremaining: 1.01s\n",
      "109:\tlearn: 0.1298633\ttotal: 18.6s\tremaining: 846ms\n",
      "110:\tlearn: 0.1295555\ttotal: 18.8s\tremaining: 677ms\n",
      "111:\tlearn: 0.1291945\ttotal: 19s\tremaining: 508ms\n",
      "112:\tlearn: 0.1286342\ttotal: 19.1s\tremaining: 338ms\n",
      "113:\tlearn: 0.1281157\ttotal: 19.3s\tremaining: 169ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:52:28,154] A new study created in memory with name: no-name-5a3779a5-3806-4be5-a0ed-ee0ea5afc02f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114:\tlearn: 0.1277107\ttotal: 19.5s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:52:28,957] Trial 0 finished with value: 0.9659244264507423 and parameters: {'depth': 7, 'learning_rate': 0.008547274461481935, 'iterations': 109}. Best is trial 0 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 19:53:45,370] Trial 1 finished with value: 0.9659244264507423 and parameters: {'depth': 15, 'learning_rate': 0.007228978739334703, 'iterations': 105}. Best is trial 0 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 19:56:11,125] Trial 2 finished with value: 0.9652496626180836 and parameters: {'depth': 16, 'learning_rate': 0.030623996110919344, 'iterations': 106}. Best is trial 0 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 19:56:49,585] Trial 3 finished with value: 0.9659244264507423 and parameters: {'depth': 14, 'learning_rate': 0.0010138680824081175, 'iterations': 112}. Best is trial 0 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 19:56:52,700] Trial 4 finished with value: 0.9659244264507423 and parameters: {'depth': 10, 'learning_rate': 0.012733473650800263, 'iterations': 105}. Best is trial 0 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 19:59:12,407] Trial 5 finished with value: 0.9659244264507423 and parameters: {'depth': 16, 'learning_rate': 0.0077278573540188, 'iterations': 102}. Best is trial 0 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 19:59:13,378] Trial 6 finished with value: 0.9662618083670715 and parameters: {'depth': 8, 'learning_rate': 0.023682887307563724, 'iterations': 100}. Best is trial 6 with value: 0.9662618083670715.\n",
      "[I 2025-07-22 19:59:14,118] Trial 7 finished with value: 0.9659244264507423 and parameters: {'depth': 7, 'learning_rate': 0.07385127665475046, 'iterations': 109}. Best is trial 6 with value: 0.9662618083670715.\n",
      "[I 2025-07-22 19:59:14,723] Trial 8 finished with value: 0.9662618083670715 and parameters: {'depth': 7, 'learning_rate': 0.0015082876303153946, 'iterations': 105}. Best is trial 6 with value: 0.9662618083670715.\n",
      "[I 2025-07-22 19:59:32,631] Trial 9 finished with value: 0.9652496626180836 and parameters: {'depth': 13, 'learning_rate': 0.04631071478007499, 'iterations': 110}. Best is trial 6 with value: 0.9662618083670715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6569082\ttotal: 7.85ms\tremaining: 777ms\n",
      "1:\tlearn: 0.6230348\ttotal: 13.5ms\tremaining: 662ms\n",
      "2:\tlearn: 0.5916212\ttotal: 18.4ms\tremaining: 594ms\n",
      "3:\tlearn: 0.5620593\ttotal: 25.6ms\tremaining: 613ms\n",
      "4:\tlearn: 0.5344040\ttotal: 33.3ms\tremaining: 632ms\n",
      "5:\tlearn: 0.5092083\ttotal: 39.1ms\tremaining: 612ms\n",
      "6:\tlearn: 0.4853113\ttotal: 45.3ms\tremaining: 602ms\n",
      "7:\tlearn: 0.4636378\ttotal: 52.7ms\tremaining: 606ms\n",
      "8:\tlearn: 0.4432415\ttotal: 57.9ms\tremaining: 585ms\n",
      "9:\tlearn: 0.4246450\ttotal: 62.5ms\tremaining: 562ms\n",
      "10:\tlearn: 0.4067666\ttotal: 67.1ms\tremaining: 543ms\n",
      "11:\tlearn: 0.3904107\ttotal: 72.3ms\tremaining: 530ms\n",
      "12:\tlearn: 0.3750871\ttotal: 77.4ms\tremaining: 518ms\n",
      "13:\tlearn: 0.3608964\ttotal: 83.8ms\tremaining: 515ms\n",
      "14:\tlearn: 0.3477881\ttotal: 90ms\tremaining: 510ms\n",
      "15:\tlearn: 0.3355019\ttotal: 114ms\tremaining: 599ms\n",
      "16:\tlearn: 0.3240482\ttotal: 119ms\tremaining: 582ms\n",
      "17:\tlearn: 0.3132453\ttotal: 127ms\tremaining: 578ms\n",
      "18:\tlearn: 0.3033434\ttotal: 136ms\tremaining: 581ms\n",
      "19:\tlearn: 0.2941601\ttotal: 142ms\tremaining: 569ms\n",
      "20:\tlearn: 0.2854282\ttotal: 156ms\tremaining: 588ms\n",
      "21:\tlearn: 0.2773576\ttotal: 163ms\tremaining: 579ms\n",
      "22:\tlearn: 0.2698838\ttotal: 170ms\tremaining: 570ms\n",
      "23:\tlearn: 0.2629121\ttotal: 178ms\tremaining: 565ms\n",
      "24:\tlearn: 0.2560323\ttotal: 187ms\tremaining: 560ms\n",
      "25:\tlearn: 0.2497788\ttotal: 195ms\tremaining: 556ms\n",
      "26:\tlearn: 0.2438954\ttotal: 206ms\tremaining: 557ms\n",
      "27:\tlearn: 0.2386891\ttotal: 219ms\tremaining: 563ms\n",
      "28:\tlearn: 0.2334951\ttotal: 226ms\tremaining: 554ms\n",
      "29:\tlearn: 0.2287396\ttotal: 232ms\tremaining: 542ms\n",
      "30:\tlearn: 0.2242638\ttotal: 240ms\tremaining: 534ms\n",
      "31:\tlearn: 0.2201865\ttotal: 245ms\tremaining: 521ms\n",
      "32:\tlearn: 0.2163334\ttotal: 254ms\tremaining: 515ms\n",
      "33:\tlearn: 0.2125800\ttotal: 263ms\tremaining: 511ms\n",
      "34:\tlearn: 0.2092495\ttotal: 270ms\tremaining: 502ms\n",
      "35:\tlearn: 0.2061428\ttotal: 277ms\tremaining: 492ms\n",
      "36:\tlearn: 0.2031556\ttotal: 284ms\tremaining: 483ms\n",
      "37:\tlearn: 0.2003326\ttotal: 292ms\tremaining: 477ms\n",
      "38:\tlearn: 0.1976057\ttotal: 301ms\tremaining: 471ms\n",
      "39:\tlearn: 0.1947625\ttotal: 309ms\tremaining: 463ms\n",
      "40:\tlearn: 0.1923752\ttotal: 315ms\tremaining: 453ms\n",
      "41:\tlearn: 0.1901181\ttotal: 326ms\tremaining: 450ms\n",
      "42:\tlearn: 0.1880651\ttotal: 334ms\tremaining: 442ms\n",
      "43:\tlearn: 0.1862948\ttotal: 339ms\tremaining: 432ms\n",
      "44:\tlearn: 0.1844254\ttotal: 350ms\tremaining: 428ms\n",
      "45:\tlearn: 0.1826990\ttotal: 357ms\tremaining: 419ms\n",
      "46:\tlearn: 0.1808477\ttotal: 368ms\tremaining: 415ms\n",
      "47:\tlearn: 0.1793237\ttotal: 379ms\tremaining: 410ms\n",
      "48:\tlearn: 0.1777303\ttotal: 391ms\tremaining: 407ms\n",
      "49:\tlearn: 0.1761655\ttotal: 402ms\tremaining: 402ms\n",
      "50:\tlearn: 0.1745722\ttotal: 410ms\tremaining: 394ms\n",
      "51:\tlearn: 0.1733171\ttotal: 417ms\tremaining: 385ms\n",
      "52:\tlearn: 0.1719728\ttotal: 426ms\tremaining: 378ms\n",
      "53:\tlearn: 0.1707608\ttotal: 438ms\tremaining: 373ms\n",
      "54:\tlearn: 0.1693806\ttotal: 446ms\tremaining: 365ms\n",
      "55:\tlearn: 0.1684556\ttotal: 456ms\tremaining: 358ms\n",
      "56:\tlearn: 0.1674820\ttotal: 468ms\tremaining: 353ms\n",
      "57:\tlearn: 0.1665246\ttotal: 474ms\tremaining: 343ms\n",
      "58:\tlearn: 0.1654529\ttotal: 482ms\tremaining: 335ms\n",
      "59:\tlearn: 0.1645592\ttotal: 489ms\tremaining: 326ms\n",
      "60:\tlearn: 0.1636949\ttotal: 497ms\tremaining: 318ms\n",
      "61:\tlearn: 0.1628847\ttotal: 504ms\tremaining: 309ms\n",
      "62:\tlearn: 0.1620006\ttotal: 513ms\tremaining: 301ms\n",
      "63:\tlearn: 0.1613089\ttotal: 518ms\tremaining: 291ms\n",
      "64:\tlearn: 0.1605049\ttotal: 526ms\tremaining: 283ms\n",
      "65:\tlearn: 0.1595301\ttotal: 532ms\tremaining: 274ms\n",
      "66:\tlearn: 0.1589189\ttotal: 537ms\tremaining: 264ms\n",
      "67:\tlearn: 0.1580256\ttotal: 543ms\tremaining: 255ms\n",
      "68:\tlearn: 0.1574089\ttotal: 550ms\tremaining: 247ms\n",
      "69:\tlearn: 0.1567339\ttotal: 556ms\tremaining: 238ms\n",
      "70:\tlearn: 0.1562372\ttotal: 563ms\tremaining: 230ms\n",
      "71:\tlearn: 0.1555305\ttotal: 570ms\tremaining: 222ms\n",
      "72:\tlearn: 0.1550780\ttotal: 575ms\tremaining: 213ms\n",
      "73:\tlearn: 0.1545452\ttotal: 580ms\tremaining: 204ms\n",
      "74:\tlearn: 0.1540704\ttotal: 585ms\tremaining: 195ms\n",
      "75:\tlearn: 0.1534244\ttotal: 589ms\tremaining: 186ms\n",
      "76:\tlearn: 0.1528314\ttotal: 595ms\tremaining: 178ms\n",
      "77:\tlearn: 0.1523068\ttotal: 601ms\tremaining: 169ms\n",
      "78:\tlearn: 0.1515442\ttotal: 607ms\tremaining: 161ms\n",
      "79:\tlearn: 0.1510307\ttotal: 613ms\tremaining: 153ms\n",
      "80:\tlearn: 0.1504037\ttotal: 619ms\tremaining: 145ms\n",
      "81:\tlearn: 0.1500109\ttotal: 626ms\tremaining: 138ms\n",
      "82:\tlearn: 0.1493492\ttotal: 632ms\tremaining: 130ms\n",
      "83:\tlearn: 0.1489889\ttotal: 638ms\tremaining: 122ms\n",
      "84:\tlearn: 0.1485080\ttotal: 644ms\tremaining: 114ms\n",
      "85:\tlearn: 0.1478682\ttotal: 651ms\tremaining: 106ms\n",
      "86:\tlearn: 0.1474181\ttotal: 656ms\tremaining: 98ms\n",
      "87:\tlearn: 0.1468962\ttotal: 661ms\tremaining: 90.2ms\n",
      "88:\tlearn: 0.1466007\ttotal: 667ms\tremaining: 82.4ms\n",
      "89:\tlearn: 0.1461191\ttotal: 675ms\tremaining: 75ms\n",
      "90:\tlearn: 0.1457455\ttotal: 683ms\tremaining: 67.6ms\n",
      "91:\tlearn: 0.1454037\ttotal: 690ms\tremaining: 60ms\n",
      "92:\tlearn: 0.1449303\ttotal: 698ms\tremaining: 52.5ms\n",
      "93:\tlearn: 0.1444790\ttotal: 708ms\tremaining: 45.2ms\n",
      "94:\tlearn: 0.1443026\ttotal: 716ms\tremaining: 37.7ms\n",
      "95:\tlearn: 0.1437839\ttotal: 723ms\tremaining: 30.1ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 19:59:33,606] A new study created in memory with name: no-name-2373581d-9bee-452a-a5c9-3989bce1ab5a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96:\tlearn: 0.1435491\ttotal: 731ms\tremaining: 22.6ms\n",
      "97:\tlearn: 0.1433171\ttotal: 742ms\tremaining: 15.1ms\n",
      "98:\tlearn: 0.1431017\ttotal: 753ms\tremaining: 7.6ms\n",
      "99:\tlearn: 0.1427694\ttotal: 760ms\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 20:00:59,967] Trial 0 finished with value: 0.9669254134323321 and parameters: {'depth': 15, 'learning_rate': 0.019845328470046244, 'iterations': 117}. Best is trial 0 with value: 0.9669254134323321.\n",
      "[I 2025-07-22 20:01:00,688] Trial 1 finished with value: 0.9662504218697267 and parameters: {'depth': 7, 'learning_rate': 0.007220923242892805, 'iterations': 117}. Best is trial 0 with value: 0.9669254134323321.\n",
      "[I 2025-07-22 20:01:01,692] Trial 2 finished with value: 0.9669254134323321 and parameters: {'depth': 8, 'learning_rate': 0.016068596723603137, 'iterations': 118}. Best is trial 0 with value: 0.9669254134323321.\n",
      "[I 2025-07-22 20:02:23,107] Trial 3 finished with value: 0.9669254134323321 and parameters: {'depth': 15, 'learning_rate': 0.002847192400414913, 'iterations': 110}. Best is trial 0 with value: 0.9669254134323321.\n",
      "[I 2025-07-22 20:02:40,649] Trial 4 finished with value: 0.9665879176510294 and parameters: {'depth': 13, 'learning_rate': 0.008224583697320998, 'iterations': 101}. Best is trial 0 with value: 0.9669254134323321.\n",
      "[I 2025-07-22 20:02:43,473] Trial 5 finished with value: 0.9665879176510294 and parameters: {'depth': 10, 'learning_rate': 0.005032085365713429, 'iterations': 118}. Best is trial 0 with value: 0.9669254134323321.\n",
      "[I 2025-07-22 20:02:46,259] Trial 6 finished with value: 0.9662504218697267 and parameters: {'depth': 10, 'learning_rate': 0.00708555103664017, 'iterations': 120}. Best is trial 0 with value: 0.9669254134323321.\n",
      "[I 2025-07-22 20:02:48,770] Trial 7 finished with value: 0.9665879176510294 and parameters: {'depth': 10, 'learning_rate': 0.0036135145773299087, 'iterations': 109}. Best is trial 0 with value: 0.9669254134323321.\n",
      "[I 2025-07-22 20:02:50,590] Trial 8 finished with value: 0.9672629092136348 and parameters: {'depth': 9, 'learning_rate': 0.010447892069001178, 'iterations': 120}. Best is trial 8 with value: 0.9672629092136348.\n",
      "[I 2025-07-22 20:02:53,989] Trial 9 finished with value: 0.9669254134323321 and parameters: {'depth': 11, 'learning_rate': 0.07943738467727486, 'iterations': 110}. Best is trial 8 with value: 0.9672629092136348.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6770984\ttotal: 16.6ms\tremaining: 1.97s\n",
      "1:\tlearn: 0.6613219\ttotal: 29.3ms\tremaining: 1.73s\n",
      "2:\tlearn: 0.6462456\ttotal: 45ms\tremaining: 1.75s\n",
      "3:\tlearn: 0.6318268\ttotal: 53.8ms\tremaining: 1.56s\n",
      "4:\tlearn: 0.6175358\ttotal: 71.7ms\tremaining: 1.65s\n",
      "5:\tlearn: 0.6037434\ttotal: 87.7ms\tremaining: 1.67s\n",
      "6:\tlearn: 0.5904706\ttotal: 101ms\tremaining: 1.63s\n",
      "7:\tlearn: 0.5772059\ttotal: 121ms\tremaining: 1.69s\n",
      "8:\tlearn: 0.5646737\ttotal: 137ms\tremaining: 1.69s\n",
      "9:\tlearn: 0.5524763\ttotal: 153ms\tremaining: 1.68s\n",
      "10:\tlearn: 0.5405231\ttotal: 168ms\tremaining: 1.67s\n",
      "11:\tlearn: 0.5290242\ttotal: 182ms\tremaining: 1.64s\n",
      "12:\tlearn: 0.5181946\ttotal: 196ms\tremaining: 1.61s\n",
      "13:\tlearn: 0.5075930\ttotal: 212ms\tremaining: 1.61s\n",
      "14:\tlearn: 0.4972697\ttotal: 227ms\tremaining: 1.59s\n",
      "15:\tlearn: 0.4870973\ttotal: 246ms\tremaining: 1.6s\n",
      "16:\tlearn: 0.4771575\ttotal: 260ms\tremaining: 1.58s\n",
      "17:\tlearn: 0.4676524\ttotal: 270ms\tremaining: 1.53s\n",
      "18:\tlearn: 0.4586033\ttotal: 282ms\tremaining: 1.5s\n",
      "19:\tlearn: 0.4498646\ttotal: 295ms\tremaining: 1.48s\n",
      "20:\tlearn: 0.4411660\ttotal: 309ms\tremaining: 1.46s\n",
      "21:\tlearn: 0.4326982\ttotal: 320ms\tremaining: 1.42s\n",
      "22:\tlearn: 0.4247080\ttotal: 333ms\tremaining: 1.4s\n",
      "23:\tlearn: 0.4166052\ttotal: 344ms\tremaining: 1.38s\n",
      "24:\tlearn: 0.4090649\ttotal: 356ms\tremaining: 1.35s\n",
      "25:\tlearn: 0.4017520\ttotal: 368ms\tremaining: 1.33s\n",
      "26:\tlearn: 0.3945799\ttotal: 380ms\tremaining: 1.31s\n",
      "27:\tlearn: 0.3877872\ttotal: 394ms\tremaining: 1.29s\n",
      "28:\tlearn: 0.3811089\ttotal: 409ms\tremaining: 1.28s\n",
      "29:\tlearn: 0.3746190\ttotal: 427ms\tremaining: 1.28s\n",
      "30:\tlearn: 0.3684322\ttotal: 441ms\tremaining: 1.27s\n",
      "31:\tlearn: 0.3623593\ttotal: 454ms\tremaining: 1.25s\n",
      "32:\tlearn: 0.3564899\ttotal: 466ms\tremaining: 1.23s\n",
      "33:\tlearn: 0.3508055\ttotal: 479ms\tremaining: 1.21s\n",
      "34:\tlearn: 0.3453127\ttotal: 491ms\tremaining: 1.19s\n",
      "35:\tlearn: 0.3399635\ttotal: 501ms\tremaining: 1.17s\n",
      "36:\tlearn: 0.3347053\ttotal: 514ms\tremaining: 1.15s\n",
      "37:\tlearn: 0.3296869\ttotal: 532ms\tremaining: 1.15s\n",
      "38:\tlearn: 0.3247777\ttotal: 544ms\tremaining: 1.13s\n",
      "39:\tlearn: 0.3200999\ttotal: 557ms\tremaining: 1.11s\n",
      "40:\tlearn: 0.3155466\ttotal: 572ms\tremaining: 1.1s\n",
      "41:\tlearn: 0.3110596\ttotal: 583ms\tremaining: 1.08s\n",
      "42:\tlearn: 0.3067267\ttotal: 602ms\tremaining: 1.08s\n",
      "43:\tlearn: 0.3024768\ttotal: 617ms\tremaining: 1.06s\n",
      "44:\tlearn: 0.2983939\ttotal: 634ms\tremaining: 1.06s\n",
      "45:\tlearn: 0.2943570\ttotal: 648ms\tremaining: 1.04s\n",
      "46:\tlearn: 0.2905420\ttotal: 665ms\tremaining: 1.03s\n",
      "47:\tlearn: 0.2868352\ttotal: 681ms\tremaining: 1.02s\n",
      "48:\tlearn: 0.2832558\ttotal: 697ms\tremaining: 1.01s\n",
      "49:\tlearn: 0.2797320\ttotal: 710ms\tremaining: 994ms\n",
      "50:\tlearn: 0.2763703\ttotal: 731ms\tremaining: 989ms\n",
      "51:\tlearn: 0.2730931\ttotal: 750ms\tremaining: 980ms\n",
      "52:\tlearn: 0.2699735\ttotal: 767ms\tremaining: 970ms\n",
      "53:\tlearn: 0.2668223\ttotal: 781ms\tremaining: 955ms\n",
      "54:\tlearn: 0.2639264\ttotal: 796ms\tremaining: 941ms\n",
      "55:\tlearn: 0.2607638\ttotal: 812ms\tremaining: 928ms\n",
      "56:\tlearn: 0.2578947\ttotal: 831ms\tremaining: 919ms\n",
      "57:\tlearn: 0.2550770\ttotal: 845ms\tremaining: 903ms\n",
      "58:\tlearn: 0.2524371\ttotal: 865ms\tremaining: 894ms\n",
      "59:\tlearn: 0.2498757\ttotal: 884ms\tremaining: 884ms\n",
      "60:\tlearn: 0.2473587\ttotal: 900ms\tremaining: 871ms\n",
      "61:\tlearn: 0.2449467\ttotal: 914ms\tremaining: 855ms\n",
      "62:\tlearn: 0.2426020\ttotal: 930ms\tremaining: 842ms\n",
      "63:\tlearn: 0.2402001\ttotal: 941ms\tremaining: 823ms\n",
      "64:\tlearn: 0.2380219\ttotal: 952ms\tremaining: 805ms\n",
      "65:\tlearn: 0.2356666\ttotal: 964ms\tremaining: 788ms\n",
      "66:\tlearn: 0.2334168\ttotal: 974ms\tremaining: 771ms\n",
      "67:\tlearn: 0.2313510\ttotal: 986ms\tremaining: 754ms\n",
      "68:\tlearn: 0.2294165\ttotal: 1000ms\tremaining: 739ms\n",
      "69:\tlearn: 0.2273703\ttotal: 1.01s\tremaining: 725ms\n",
      "70:\tlearn: 0.2253749\ttotal: 1.03s\tremaining: 711ms\n",
      "71:\tlearn: 0.2235472\ttotal: 1.04s\tremaining: 696ms\n",
      "72:\tlearn: 0.2216819\ttotal: 1.06s\tremaining: 682ms\n",
      "73:\tlearn: 0.2200029\ttotal: 1.07s\tremaining: 666ms\n",
      "74:\tlearn: 0.2182516\ttotal: 1.08s\tremaining: 651ms\n",
      "75:\tlearn: 0.2166691\ttotal: 1.1s\tremaining: 636ms\n",
      "76:\tlearn: 0.2151632\ttotal: 1.11s\tremaining: 621ms\n",
      "77:\tlearn: 0.2136398\ttotal: 1.12s\tremaining: 605ms\n",
      "78:\tlearn: 0.2121962\ttotal: 1.13s\tremaining: 588ms\n",
      "79:\tlearn: 0.2106776\ttotal: 1.15s\tremaining: 575ms\n",
      "80:\tlearn: 0.2091570\ttotal: 1.16s\tremaining: 560ms\n",
      "81:\tlearn: 0.2078049\ttotal: 1.18s\tremaining: 545ms\n",
      "82:\tlearn: 0.2063880\ttotal: 1.19s\tremaining: 531ms\n",
      "83:\tlearn: 0.2051010\ttotal: 1.2s\tremaining: 516ms\n",
      "84:\tlearn: 0.2036986\ttotal: 1.22s\tremaining: 501ms\n",
      "85:\tlearn: 0.2023621\ttotal: 1.23s\tremaining: 485ms\n",
      "86:\tlearn: 0.2011556\ttotal: 1.24s\tremaining: 470ms\n",
      "87:\tlearn: 0.1999496\ttotal: 1.25s\tremaining: 455ms\n",
      "88:\tlearn: 0.1988039\ttotal: 1.26s\tremaining: 441ms\n",
      "89:\tlearn: 0.1975713\ttotal: 1.28s\tremaining: 426ms\n",
      "90:\tlearn: 0.1964127\ttotal: 1.29s\tremaining: 411ms\n",
      "91:\tlearn: 0.1953008\ttotal: 1.3s\tremaining: 397ms\n",
      "92:\tlearn: 0.1940464\ttotal: 1.32s\tremaining: 382ms\n",
      "93:\tlearn: 0.1929827\ttotal: 1.33s\tremaining: 368ms\n",
      "94:\tlearn: 0.1920184\ttotal: 1.34s\tremaining: 354ms\n",
      "95:\tlearn: 0.1910099\ttotal: 1.36s\tremaining: 339ms\n",
      "96:\tlearn: 0.1900594\ttotal: 1.37s\tremaining: 325ms\n",
      "97:\tlearn: 0.1891290\ttotal: 1.38s\tremaining: 311ms\n",
      "98:\tlearn: 0.1881481\ttotal: 1.4s\tremaining: 296ms\n",
      "99:\tlearn: 0.1873024\ttotal: 1.41s\tremaining: 282ms\n",
      "100:\tlearn: 0.1862891\ttotal: 1.42s\tremaining: 267ms\n",
      "101:\tlearn: 0.1853700\ttotal: 1.43s\tremaining: 252ms\n",
      "102:\tlearn: 0.1845074\ttotal: 1.44s\tremaining: 238ms\n",
      "103:\tlearn: 0.1836867\ttotal: 1.45s\tremaining: 223ms\n",
      "104:\tlearn: 0.1828320\ttotal: 1.46s\tremaining: 209ms\n",
      "105:\tlearn: 0.1818258\ttotal: 1.47s\tremaining: 194ms\n",
      "106:\tlearn: 0.1810745\ttotal: 1.48s\tremaining: 180ms\n",
      "107:\tlearn: 0.1802347\ttotal: 1.49s\tremaining: 166ms\n",
      "108:\tlearn: 0.1794650\ttotal: 1.5s\tremaining: 151ms\n",
      "109:\tlearn: 0.1788141\ttotal: 1.51s\tremaining: 137ms\n",
      "110:\tlearn: 0.1781132\ttotal: 1.52s\tremaining: 123ms\n",
      "111:\tlearn: 0.1774733\ttotal: 1.53s\tremaining: 109ms\n",
      "112:\tlearn: 0.1766534\ttotal: 1.54s\tremaining: 95.7ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 20:02:55,842] A new study created in memory with name: no-name-5794071d-ba3b-4944-a1ae-bd04812bc3f3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113:\tlearn: 0.1760067\ttotal: 1.56s\tremaining: 82ms\n",
      "114:\tlearn: 0.1754035\ttotal: 1.57s\tremaining: 68.2ms\n",
      "115:\tlearn: 0.1747534\ttotal: 1.58s\tremaining: 54.7ms\n",
      "116:\tlearn: 0.1741679\ttotal: 1.6s\tremaining: 40.9ms\n",
      "117:\tlearn: 0.1735516\ttotal: 1.61s\tremaining: 27.3ms\n",
      "118:\tlearn: 0.1729568\ttotal: 1.62s\tremaining: 13.6ms\n",
      "119:\tlearn: 0.1723826\ttotal: 1.63s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-22 20:02:59,110] Trial 0 finished with value: 0.9612010796221323 and parameters: {'n_estimators': 101, 'max_depth': 130}. Best is trial 0 with value: 0.9612010796221323.\n",
      "[I 2025-07-22 20:03:02,298] Trial 1 finished with value: 0.9618758434547908 and parameters: {'n_estimators': 98, 'max_depth': 229}. Best is trial 1 with value: 0.9618758434547908.\n",
      "[I 2025-07-22 20:03:05,558] Trial 2 finished with value: 0.9601889338731444 and parameters: {'n_estimators': 100, 'max_depth': 220}. Best is trial 1 with value: 0.9618758434547908.\n",
      "[I 2025-07-22 20:03:08,218] Trial 3 finished with value: 0.9612010796221323 and parameters: {'n_estimators': 81, 'max_depth': 133}. Best is trial 1 with value: 0.9618758434547908.\n",
      "[I 2025-07-22 20:03:12,046] Trial 4 finished with value: 0.9608636977058029 and parameters: {'n_estimators': 119, 'max_depth': 184}. Best is trial 1 with value: 0.9618758434547908.\n",
      "[I 2025-07-22 20:03:14,958] Trial 5 finished with value: 0.9612010796221323 and parameters: {'n_estimators': 90, 'max_depth': 108}. Best is trial 1 with value: 0.9618758434547908.\n",
      "[I 2025-07-22 20:03:17,909] Trial 6 finished with value: 0.9612010796221323 and parameters: {'n_estimators': 90, 'max_depth': 167}. Best is trial 1 with value: 0.9618758434547908.\n",
      "[I 2025-07-22 20:03:21,056] Trial 7 finished with value: 0.9615384615384616 and parameters: {'n_estimators': 97, 'max_depth': 216}. Best is trial 1 with value: 0.9618758434547908.\n",
      "[I 2025-07-22 20:03:24,776] Trial 8 finished with value: 0.9615384615384616 and parameters: {'n_estimators': 114, 'max_depth': 222}. Best is trial 1 with value: 0.9618758434547908.\n",
      "[I 2025-07-22 20:03:28,477] Trial 9 finished with value: 0.9618758434547908 and parameters: {'n_estimators': 114, 'max_depth': 145}. Best is trial 1 with value: 0.9618758434547908.\n",
      "[I 2025-07-22 20:03:31,702] A new study created in memory with name: no-name-9bb7d51a-8c89-469b-a9bf-17cb11f6b300\n",
      "[I 2025-07-22 20:03:34,505] Trial 0 finished with value: 0.9696356275303644 and parameters: {'n_estimators': 89, 'max_depth': 104}. Best is trial 0 with value: 0.9696356275303644.\n",
      "[I 2025-07-22 20:03:37,697] Trial 1 finished with value: 0.9699730094466936 and parameters: {'n_estimators': 102, 'max_depth': 219}. Best is trial 1 with value: 0.9699730094466936.\n",
      "[I 2025-07-22 20:03:41,172] Trial 2 finished with value: 0.9692982456140351 and parameters: {'n_estimators': 111, 'max_depth': 247}. Best is trial 1 with value: 0.9699730094466936.\n",
      "[I 2025-07-22 20:03:44,651] Trial 3 finished with value: 0.9696356275303644 and parameters: {'n_estimators': 111, 'max_depth': 128}. Best is trial 1 with value: 0.9699730094466936.\n",
      "[I 2025-07-22 20:03:48,259] Trial 4 finished with value: 0.970310391363023 and parameters: {'n_estimators': 114, 'max_depth': 159}. Best is trial 4 with value: 0.970310391363023.\n",
      "[I 2025-07-22 20:03:51,313] Trial 5 finished with value: 0.970310391363023 and parameters: {'n_estimators': 98, 'max_depth': 232}. Best is trial 4 with value: 0.970310391363023.\n",
      "[I 2025-07-22 20:03:53,952] Trial 6 finished with value: 0.9689608636977058 and parameters: {'n_estimators': 84, 'max_depth': 225}. Best is trial 4 with value: 0.970310391363023.\n",
      "[I 2025-07-22 20:03:57,575] Trial 7 finished with value: 0.9692982456140351 and parameters: {'n_estimators': 115, 'max_depth': 129}. Best is trial 4 with value: 0.970310391363023.\n",
      "[I 2025-07-22 20:04:01,210] Trial 8 finished with value: 0.9699730094466936 and parameters: {'n_estimators': 116, 'max_depth': 227}. Best is trial 4 with value: 0.970310391363023.\n",
      "[I 2025-07-22 20:04:03,916] Trial 9 finished with value: 0.9699730094466936 and parameters: {'n_estimators': 86, 'max_depth': 128}. Best is trial 4 with value: 0.970310391363023.\n",
      "[I 2025-07-22 20:04:07,494] A new study created in memory with name: no-name-b111a1d1-6a7e-444c-b247-1c5bd8543184\n",
      "[I 2025-07-22 20:04:10,173] Trial 0 finished with value: 0.9669365721997301 and parameters: {'n_estimators': 93, 'max_depth': 128}. Best is trial 0 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 20:04:13,618] Trial 1 finished with value: 0.9665991902834008 and parameters: {'n_estimators': 116, 'max_depth': 140}. Best is trial 0 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 20:04:16,008] Trial 2 finished with value: 0.9669365721997301 and parameters: {'n_estimators': 82, 'max_depth': 246}. Best is trial 0 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 20:04:19,476] Trial 3 finished with value: 0.9662618083670715 and parameters: {'n_estimators': 119, 'max_depth': 128}. Best is trial 0 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 20:04:22,152] Trial 4 finished with value: 0.9662618083670715 and parameters: {'n_estimators': 92, 'max_depth': 216}. Best is trial 0 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 20:04:25,593] Trial 5 finished with value: 0.9672739541160594 and parameters: {'n_estimators': 118, 'max_depth': 143}. Best is trial 5 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:04:29,041] Trial 6 finished with value: 0.9665991902834008 and parameters: {'n_estimators': 118, 'max_depth': 173}. Best is trial 5 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:04:32,088] Trial 7 finished with value: 0.9662618083670715 and parameters: {'n_estimators': 103, 'max_depth': 133}. Best is trial 5 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:04:35,427] Trial 8 finished with value: 0.9662618083670715 and parameters: {'n_estimators': 115, 'max_depth': 197}. Best is trial 5 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:04:38,365] Trial 9 finished with value: 0.9669365721997301 and parameters: {'n_estimators': 100, 'max_depth': 211}. Best is trial 5 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:04:41,836] A new study created in memory with name: no-name-0ef1aa19-f99f-4ed1-8ce1-507b6dfa2855\n",
      "[I 2025-07-22 20:04:44,907] Trial 0 finished with value: 0.9618758434547908 and parameters: {'n_estimators': 94, 'max_depth': 135}. Best is trial 0 with value: 0.9618758434547908.\n",
      "[I 2025-07-22 20:04:47,561] Trial 1 finished with value: 0.9625506072874493 and parameters: {'n_estimators': 82, 'max_depth': 146}. Best is trial 1 with value: 0.9625506072874493.\n",
      "[I 2025-07-22 20:04:51,364] Trial 2 finished with value: 0.9618758434547908 and parameters: {'n_estimators': 118, 'max_depth': 105}. Best is trial 1 with value: 0.9625506072874493.\n",
      "[I 2025-07-22 20:04:54,622] Trial 3 finished with value: 0.9625506072874493 and parameters: {'n_estimators': 101, 'max_depth': 161}. Best is trial 1 with value: 0.9625506072874493.\n",
      "[I 2025-07-22 20:04:57,908] Trial 4 finished with value: 0.9618758434547908 and parameters: {'n_estimators': 101, 'max_depth': 209}. Best is trial 1 with value: 0.9625506072874493.\n",
      "[I 2025-07-22 20:05:01,694] Trial 5 finished with value: 0.9615384615384616 and parameters: {'n_estimators': 119, 'max_depth': 215}. Best is trial 1 with value: 0.9625506072874493.\n",
      "[I 2025-07-22 20:05:04,868] Trial 6 finished with value: 0.9615384615384616 and parameters: {'n_estimators': 98, 'max_depth': 236}. Best is trial 1 with value: 0.9625506072874493.\n",
      "[I 2025-07-22 20:05:07,551] Trial 7 finished with value: 0.9618758434547908 and parameters: {'n_estimators': 83, 'max_depth': 144}. Best is trial 1 with value: 0.9625506072874493.\n",
      "[I 2025-07-22 20:05:11,307] Trial 8 finished with value: 0.9618758434547908 and parameters: {'n_estimators': 117, 'max_depth': 236}. Best is trial 1 with value: 0.9625506072874493.\n",
      "[I 2025-07-22 20:05:14,785] Trial 9 finished with value: 0.9615384615384616 and parameters: {'n_estimators': 110, 'max_depth': 196}. Best is trial 1 with value: 0.9625506072874493.\n",
      "[I 2025-07-22 20:05:17,425] A new study created in memory with name: no-name-e3bfb708-ea9b-44d1-8fd8-f7e0f8313793\n",
      "[I 2025-07-22 20:05:20,788] Trial 0 finished with value: 0.9642254471819103 and parameters: {'n_estimators': 111, 'max_depth': 236}. Best is trial 0 with value: 0.9642254471819103.\n",
      "[I 2025-07-22 20:05:23,736] Trial 1 finished with value: 0.9638879514006075 and parameters: {'n_estimators': 96, 'max_depth': 235}. Best is trial 0 with value: 0.9642254471819103.\n",
      "[I 2025-07-22 20:05:26,325] Trial 2 finished with value: 0.9642254471819103 and parameters: {'n_estimators': 87, 'max_depth': 101}. Best is trial 0 with value: 0.9642254471819103.\n",
      "[I 2025-07-22 20:05:28,783] Trial 3 finished with value: 0.9638879514006075 and parameters: {'n_estimators': 81, 'max_depth': 112}. Best is trial 0 with value: 0.9642254471819103.\n",
      "[I 2025-07-22 20:05:31,269] Trial 4 finished with value: 0.9642254471819103 and parameters: {'n_estimators': 81, 'max_depth': 194}. Best is trial 0 with value: 0.9642254471819103.\n",
      "[I 2025-07-22 20:05:34,036] Trial 5 finished with value: 0.9638879514006075 and parameters: {'n_estimators': 89, 'max_depth': 133}. Best is trial 0 with value: 0.9642254471819103.\n",
      "[I 2025-07-22 20:05:37,199] Trial 6 finished with value: 0.9635504556193047 and parameters: {'n_estimators': 103, 'max_depth': 211}. Best is trial 0 with value: 0.9642254471819103.\n",
      "[I 2025-07-22 20:05:40,217] Trial 7 finished with value: 0.9642254471819103 and parameters: {'n_estimators': 100, 'max_depth': 150}. Best is trial 0 with value: 0.9642254471819103.\n",
      "[I 2025-07-22 20:05:43,033] Trial 8 finished with value: 0.9642254471819103 and parameters: {'n_estimators': 91, 'max_depth': 101}. Best is trial 0 with value: 0.9642254471819103.\n",
      "[I 2025-07-22 20:05:46,380] Trial 9 finished with value: 0.9638879514006075 and parameters: {'n_estimators': 109, 'max_depth': 133}. Best is trial 0 with value: 0.9642254471819103.\n",
      "[I 2025-07-22 20:05:49,794] A new study created in memory with name: no-name-af1010f3-6e7c-4ad2-beb7-c0031ac912d5\n",
      "[I 2025-07-22 20:05:50,008] Trial 0 finished with value: 0.9669365721997301 and parameters: {'n_estimators': 87, 'max_depth': 4}. Best is trial 0 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 20:05:50,250] Trial 1 finished with value: 0.9672739541160594 and parameters: {'n_estimators': 84, 'max_depth': 5}. Best is trial 1 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:05:50,682] Trial 2 finished with value: 0.9662618083670715 and parameters: {'n_estimators': 90, 'max_depth': 10}. Best is trial 1 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:05:50,880] Trial 3 finished with value: 0.9672739541160594 and parameters: {'n_estimators': 60, 'max_depth': 6}. Best is trial 1 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:05:51,170] Trial 4 finished with value: 0.9665991902834008 and parameters: {'n_estimators': 66, 'max_depth': 9}. Best is trial 1 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:05:51,448] Trial 5 finished with value: 0.9665991902834008 and parameters: {'n_estimators': 64, 'max_depth': 9}. Best is trial 1 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:05:51,881] Trial 6 finished with value: 0.9669365721997301 and parameters: {'n_estimators': 100, 'max_depth': 9}. Best is trial 1 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:05:52,186] Trial 7 finished with value: 0.9662618083670715 and parameters: {'n_estimators': 70, 'max_depth': 9}. Best is trial 1 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:05:52,278] Trial 8 finished with value: 0.9655870445344129 and parameters: {'n_estimators': 65, 'max_depth': 1}. Best is trial 1 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:05:52,566] Trial 9 finished with value: 0.9669365721997301 and parameters: {'n_estimators': 72, 'max_depth': 8}. Best is trial 1 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:05:52,870] A new study created in memory with name: no-name-299e469b-e093-4b13-9d89-702bf298b101\n",
      "[I 2025-07-22 20:05:53,252] Trial 0 finished with value: 0.9733468286099866 and parameters: {'n_estimators': 96, 'max_depth': 8}. Best is trial 0 with value: 0.9733468286099866.\n",
      "[I 2025-07-22 20:05:53,517] Trial 1 finished with value: 0.9733468286099866 and parameters: {'n_estimators': 82, 'max_depth': 6}. Best is trial 0 with value: 0.9733468286099866.\n",
      "[I 2025-07-22 20:05:53,691] Trial 2 finished with value: 0.9733468286099866 and parameters: {'n_estimators': 83, 'max_depth': 3}. Best is trial 0 with value: 0.9733468286099866.\n",
      "[I 2025-07-22 20:05:53,812] Trial 3 finished with value: 0.9713225371120108 and parameters: {'n_estimators': 88, 'max_depth': 1}. Best is trial 0 with value: 0.9733468286099866.\n",
      "[I 2025-07-22 20:05:53,962] Trial 4 finished with value: 0.9736842105263158 and parameters: {'n_estimators': 85, 'max_depth': 2}. Best is trial 4 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 20:05:54,392] Trial 5 finished with value: 0.9733468286099866 and parameters: {'n_estimators': 89, 'max_depth': 10}. Best is trial 4 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 20:05:54,565] Trial 6 finished with value: 0.9730094466936572 and parameters: {'n_estimators': 81, 'max_depth': 3}. Best is trial 4 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 20:05:54,798] Trial 7 finished with value: 0.9733468286099866 and parameters: {'n_estimators': 64, 'max_depth': 7}. Best is trial 4 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 20:05:54,972] Trial 8 finished with value: 0.9733468286099866 and parameters: {'n_estimators': 70, 'max_depth': 4}. Best is trial 4 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 20:05:55,204] Trial 9 finished with value: 0.9733468286099866 and parameters: {'n_estimators': 79, 'max_depth': 5}. Best is trial 4 with value: 0.9736842105263158.\n",
      "[I 2025-07-22 20:05:55,408] A new study created in memory with name: no-name-3ed05445-f919-40a4-ac1f-8ed0551fe0a4\n",
      "[I 2025-07-22 20:05:55,729] Trial 0 finished with value: 0.9689608636977058 and parameters: {'n_estimators': 91, 'max_depth': 7}. Best is trial 0 with value: 0.9689608636977058.\n",
      "[I 2025-07-22 20:05:55,916] Trial 1 finished with value: 0.9692982456140351 and parameters: {'n_estimators': 78, 'max_depth': 4}. Best is trial 1 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 20:05:56,002] Trial 2 finished with value: 0.9686234817813765 and parameters: {'n_estimators': 64, 'max_depth': 1}. Best is trial 1 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 20:05:56,238] Trial 3 finished with value: 0.9692982456140351 and parameters: {'n_estimators': 75, 'max_depth': 6}. Best is trial 1 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 20:05:56,438] Trial 4 finished with value: 0.9692982456140351 and parameters: {'n_estimators': 71, 'max_depth': 5}. Best is trial 1 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 20:05:56,673] Trial 5 finished with value: 0.9692982456140351 and parameters: {'n_estimators': 86, 'max_depth': 5}. Best is trial 1 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 20:05:56,764] Trial 6 finished with value: 0.967948717948718 and parameters: {'n_estimators': 67, 'max_depth': 1}. Best is trial 1 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 20:05:57,171] Trial 7 finished with value: 0.9689608636977058 and parameters: {'n_estimators': 89, 'max_depth': 10}. Best is trial 1 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 20:05:57,452] Trial 8 finished with value: 0.9692982456140351 and parameters: {'n_estimators': 91, 'max_depth': 6}. Best is trial 1 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 20:05:57,571] Trial 9 finished with value: 0.9696356275303644 and parameters: {'n_estimators': 88, 'max_depth': 1}. Best is trial 9 with value: 0.9696356275303644.\n",
      "[I 2025-07-22 20:05:57,739] A new study created in memory with name: no-name-6d552137-cf66-49a7-b2bc-269541483170\n",
      "[I 2025-07-22 20:05:58,084] Trial 0 finished with value: 0.9662618083670715 and parameters: {'n_estimators': 99, 'max_depth': 7}. Best is trial 0 with value: 0.9662618083670715.\n",
      "[I 2025-07-22 20:05:58,193] Trial 1 finished with value: 0.9665991902834008 and parameters: {'n_estimators': 80, 'max_depth': 1}. Best is trial 1 with value: 0.9665991902834008.\n",
      "[I 2025-07-22 20:05:58,424] Trial 2 finished with value: 0.9662618083670715 and parameters: {'n_estimators': 63, 'max_depth': 7}. Best is trial 1 with value: 0.9665991902834008.\n",
      "[I 2025-07-22 20:05:58,756] Trial 3 finished with value: 0.9662618083670715 and parameters: {'n_estimators': 79, 'max_depth': 9}. Best is trial 1 with value: 0.9665991902834008.\n",
      "[I 2025-07-22 20:05:59,080] Trial 4 finished with value: 0.9659244264507423 and parameters: {'n_estimators': 94, 'max_depth': 7}. Best is trial 1 with value: 0.9665991902834008.\n",
      "[I 2025-07-22 20:05:59,171] Trial 5 finished with value: 0.9645748987854251 and parameters: {'n_estimators': 67, 'max_depth': 1}. Best is trial 1 with value: 0.9665991902834008.\n",
      "[I 2025-07-22 20:05:59,425] Trial 6 finished with value: 0.9669365721997301 and parameters: {'n_estimators': 92, 'max_depth': 5}. Best is trial 6 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 20:05:59,717] Trial 7 finished with value: 0.9665991902834008 and parameters: {'n_estimators': 93, 'max_depth': 6}. Best is trial 6 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 20:06:00,082] Trial 8 finished with value: 0.9662618083670715 and parameters: {'n_estimators': 96, 'max_depth': 8}. Best is trial 6 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 20:06:00,413] Trial 9 finished with value: 0.9662618083670715 and parameters: {'n_estimators': 86, 'max_depth': 8}. Best is trial 6 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 20:06:00,707] A new study created in memory with name: no-name-bf541102-c437-4df0-9f80-a9a6de455553\n",
      "[I 2025-07-22 20:06:01,091] Trial 0 finished with value: 0.9672629092136348 and parameters: {'n_estimators': 89, 'max_depth': 9}. Best is trial 0 with value: 0.9672629092136348.\n",
      "[I 2025-07-22 20:06:01,341] Trial 1 finished with value: 0.9672629092136348 and parameters: {'n_estimators': 71, 'max_depth': 7}. Best is trial 0 with value: 0.9672629092136348.\n",
      "[I 2025-07-22 20:06:01,496] Trial 2 finished with value: 0.9665879176510294 and parameters: {'n_estimators': 75, 'max_depth': 3}. Best is trial 0 with value: 0.9672629092136348.\n",
      "[I 2025-07-22 20:06:01,694] Trial 3 finished with value: 0.9672629092136348 and parameters: {'n_estimators': 62, 'max_depth': 6}. Best is trial 0 with value: 0.9672629092136348.\n",
      "[I 2025-07-22 20:06:01,842] Trial 4 finished with value: 0.9669254134323321 and parameters: {'n_estimators': 88, 'max_depth': 2}. Best is trial 0 with value: 0.9672629092136348.\n",
      "[I 2025-07-22 20:06:01,954] Trial 5 finished with value: 0.9662504218697267 and parameters: {'n_estimators': 66, 'max_depth': 2}. Best is trial 0 with value: 0.9672629092136348.\n",
      "[I 2025-07-22 20:06:02,288] Trial 6 finished with value: 0.9669254134323321 and parameters: {'n_estimators': 93, 'max_depth': 7}. Best is trial 0 with value: 0.9672629092136348.\n",
      "[I 2025-07-22 20:06:02,563] Trial 7 finished with value: 0.9672629092136348 and parameters: {'n_estimators': 77, 'max_depth': 7}. Best is trial 0 with value: 0.9672629092136348.\n",
      "[I 2025-07-22 20:06:02,861] Trial 8 finished with value: 0.9665879176510294 and parameters: {'n_estimators': 64, 'max_depth': 10}. Best is trial 0 with value: 0.9672629092136348.\n",
      "[I 2025-07-22 20:06:02,986] Trial 9 finished with value: 0.9635504556193047 and parameters: {'n_estimators': 96, 'max_depth': 1}. Best is trial 0 with value: 0.9672629092136348.\n",
      "[I 2025-07-22 20:06:03,421] A new study created in memory with name: no-name-0016ea9c-8d6c-48c3-a456-9fd60b8b73f4\n",
      "[I 2025-07-22 20:06:03,516] Trial 0 finished with value: 0.9392712550607287 and parameters: {'C': 0.01840795252710855, 'max_iter': 123}. Best is trial 0 with value: 0.9392712550607287.\n",
      "[I 2025-07-22 20:06:03,584] Trial 1 finished with value: 0.9089068825910931 and parameters: {'C': 0.01123150036393312, 'max_iter': 192}. Best is trial 0 with value: 0.9392712550607287.\n",
      "[I 2025-07-22 20:06:03,775] Trial 2 finished with value: 0.7510121457489879 and parameters: {'C': 0.41347724823872223, 'max_iter': 196}. Best is trial 0 with value: 0.9392712550607287.\n",
      "[I 2025-07-22 20:06:03,834] Trial 3 finished with value: 0.9493927125506073 and parameters: {'C': 0.0407955764423989, 'max_iter': 164}. Best is trial 3 with value: 0.9493927125506073.\n",
      "[I 2025-07-22 20:06:03,854] Trial 4 finished with value: 0.9605263157894737 and parameters: {'C': 0.0005663119480724903, 'max_iter': 168}. Best is trial 4 with value: 0.9605263157894737.\n",
      "[I 2025-07-22 20:06:03,889] Trial 5 finished with value: 0.917004048582996 and parameters: {'C': 0.01762969892974643, 'max_iter': 148}. Best is trial 4 with value: 0.9605263157894737.\n",
      "[I 2025-07-22 20:06:03,921] Trial 6 finished with value: 0.9608636977058029 and parameters: {'C': 0.00021904949757333507, 'max_iter': 174}. Best is trial 6 with value: 0.9608636977058029.\n",
      "[I 2025-07-22 20:06:04,349] Trial 7 finished with value: 0.7479757085020243 and parameters: {'C': 6.642118554087679, 'max_iter': 112}. Best is trial 6 with value: 0.9608636977058029.\n",
      "[I 2025-07-22 20:06:04,471] Trial 8 finished with value: 0.7516869095816464 and parameters: {'C': 0.40230057510476086, 'max_iter': 102}. Best is trial 6 with value: 0.9608636977058029.\n",
      "[I 2025-07-22 20:06:04,584] Trial 9 finished with value: 0.9510796221322537 and parameters: {'C': 0.027389027353859893, 'max_iter': 106}. Best is trial 6 with value: 0.9608636977058029.\n",
      "[I 2025-07-22 20:06:04,711] A new study created in memory with name: no-name-7667756f-7258-4eac-bf7c-1a6b0b678c2a\n",
      "[I 2025-07-22 20:06:04,863] Trial 0 finished with value: 0.9699730094466936 and parameters: {'C': 6.638705107342075, 'max_iter': 172}. Best is trial 0 with value: 0.9699730094466936.\n",
      "[I 2025-07-22 20:06:04,907] Trial 1 finished with value: 0.9659244264507423 and parameters: {'C': 0.07267685439893724, 'max_iter': 149}. Best is trial 0 with value: 0.9699730094466936.\n",
      "[I 2025-07-22 20:06:05,098] Trial 2 finished with value: 0.9696356275303644 and parameters: {'C': 1.4807831092880803, 'max_iter': 174}. Best is trial 0 with value: 0.9699730094466936.\n",
      "[I 2025-07-22 20:06:05,143] Trial 3 finished with value: 0.97165991902834 and parameters: {'C': 0.00045735545693230596, 'max_iter': 160}. Best is trial 3 with value: 0.97165991902834.\n",
      "[I 2025-07-22 20:06:05,347] Trial 4 finished with value: 0.9699730094466936 and parameters: {'C': 2.8271261423054357, 'max_iter': 176}. Best is trial 3 with value: 0.97165991902834.\n",
      "[I 2025-07-22 20:06:05,377] Trial 5 finished with value: 0.97165991902834 and parameters: {'C': 0.0017386911512834605, 'max_iter': 102}. Best is trial 3 with value: 0.97165991902834.\n",
      "[I 2025-07-22 20:06:05,435] Trial 6 finished with value: 0.9682860998650472 and parameters: {'C': 0.3979172146989064, 'max_iter': 166}. Best is trial 3 with value: 0.97165991902834.\n",
      "[I 2025-07-22 20:06:05,522] Trial 7 finished with value: 0.9662618083670715 and parameters: {'C': 0.08453864937680482, 'max_iter': 117}. Best is trial 3 with value: 0.97165991902834.\n",
      "[I 2025-07-22 20:06:05,571] Trial 8 finished with value: 0.970310391363023 and parameters: {'C': 0.0022693228112782857, 'max_iter': 113}. Best is trial 3 with value: 0.97165991902834.\n",
      "[I 2025-07-22 20:06:05,698] Trial 9 finished with value: 0.9662618083670715 and parameters: {'C': 0.09962559555802036, 'max_iter': 141}. Best is trial 3 with value: 0.97165991902834.\n",
      "[I 2025-07-22 20:06:05,859] A new study created in memory with name: no-name-99719d77-7e45-4c46-bd09-384567fcdcb4\n",
      "[I 2025-07-22 20:06:05,907] Trial 0 finished with value: 0.9649122807017544 and parameters: {'C': 0.006218125173417576, 'max_iter': 184}. Best is trial 0 with value: 0.9649122807017544.\n",
      "[I 2025-07-22 20:06:06,278] Trial 1 finished with value: 0.9672739541160594 and parameters: {'C': 5.630885802961544, 'max_iter': 200}. Best is trial 1 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:06:06,350] Trial 2 finished with value: 0.9615384615384616 and parameters: {'C': 0.0008577135873354815, 'max_iter': 125}. Best is trial 1 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:06:06,412] Trial 3 finished with value: 0.9608636977058029 and parameters: {'C': 0.0012868063788196188, 'max_iter': 118}. Best is trial 1 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:06:06,469] Trial 4 finished with value: 0.9622132253711201 and parameters: {'C': 0.000544258409985132, 'max_iter': 157}. Best is trial 1 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:06:06,539] Trial 5 finished with value: 0.9608636977058029 and parameters: {'C': 0.00184832521248643, 'max_iter': 137}. Best is trial 1 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:06:06,689] Trial 6 finished with value: 0.9649122807017544 and parameters: {'C': 0.044953349226044936, 'max_iter': 193}. Best is trial 1 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:06:06,793] Trial 7 finished with value: 0.9655870445344129 and parameters: {'C': 0.05506503782504426, 'max_iter': 136}. Best is trial 1 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:06:06,861] Trial 8 finished with value: 0.9669365721997301 and parameters: {'C': 0.2906736558446659, 'max_iter': 169}. Best is trial 1 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:06:06,955] Trial 9 finished with value: 0.9665991902834008 and parameters: {'C': 0.9704692116480819, 'max_iter': 138}. Best is trial 1 with value: 0.9672739541160594.\n",
      "[I 2025-07-22 20:06:07,259] A new study created in memory with name: no-name-b97d6667-bbd7-4ed3-a726-6f3dcff9eff6\n",
      "[I 2025-07-22 20:06:07,336] Trial 0 finished with value: 0.9598515519568152 and parameters: {'C': 0.3148563001103047, 'max_iter': 120}. Best is trial 0 with value: 0.9598515519568152.\n",
      "[I 2025-07-22 20:06:07,375] Trial 1 finished with value: 0.9605263157894737 and parameters: {'C': 0.0004374131442651382, 'max_iter': 107}. Best is trial 1 with value: 0.9605263157894737.\n",
      "[I 2025-07-22 20:06:07,474] Trial 2 finished with value: 0.9612010796221323 and parameters: {'C': 0.05122579410503641, 'max_iter': 139}. Best is trial 2 with value: 0.9612010796221323.\n",
      "[I 2025-07-22 20:06:07,536] Trial 3 finished with value: 0.9608636977058029 and parameters: {'C': 0.011099732758623724, 'max_iter': 145}. Best is trial 2 with value: 0.9612010796221323.\n",
      "[I 2025-07-22 20:06:07,591] Trial 4 finished with value: 0.9598515519568152 and parameters: {'C': 0.0025985383825162483, 'max_iter': 141}. Best is trial 2 with value: 0.9612010796221323.\n",
      "[I 2025-07-22 20:06:07,685] Trial 5 finished with value: 0.9608636977058029 and parameters: {'C': 0.03222663987265281, 'max_iter': 172}. Best is trial 2 with value: 0.9612010796221323.\n",
      "[I 2025-07-22 20:06:07,758] Trial 6 finished with value: 0.9612010796221323 and parameters: {'C': 0.042459527492203664, 'max_iter': 160}. Best is trial 2 with value: 0.9612010796221323.\n",
      "[I 2025-07-22 20:06:07,788] Trial 7 finished with value: 0.9598515519568152 and parameters: {'C': 0.0026603210626050967, 'max_iter': 193}. Best is trial 2 with value: 0.9612010796221323.\n",
      "[I 2025-07-22 20:06:07,816] Trial 8 finished with value: 0.9598515519568152 and parameters: {'C': 0.002621756432621507, 'max_iter': 164}. Best is trial 2 with value: 0.9612010796221323.\n",
      "[I 2025-07-22 20:06:07,847] Trial 9 finished with value: 0.9595141700404858 and parameters: {'C': 0.0021766297194655577, 'max_iter': 125}. Best is trial 2 with value: 0.9612010796221323.\n",
      "[I 2025-07-22 20:06:07,974] A new study created in memory with name: no-name-4fe2d3d2-1eb0-45c0-9cb8-5962f94992e6\n",
      "[I 2025-07-22 20:06:08,186] Trial 0 finished with value: 0.9628754640566993 and parameters: {'C': 3.2612806427840466, 'max_iter': 140}. Best is trial 0 with value: 0.9628754640566993.\n",
      "[I 2025-07-22 20:06:08,259] Trial 1 finished with value: 0.9618629767127911 and parameters: {'C': 0.028990505258492557, 'max_iter': 194}. Best is trial 0 with value: 0.9628754640566993.\n",
      "[I 2025-07-22 20:06:08,326] Trial 2 finished with value: 0.9622004724940938 and parameters: {'C': 0.10632995281693983, 'max_iter': 115}. Best is trial 0 with value: 0.9628754640566993.\n",
      "[I 2025-07-22 20:06:08,366] Trial 3 finished with value: 0.9571380357745528 and parameters: {'C': 0.0003992651178990214, 'max_iter': 168}. Best is trial 0 with value: 0.9628754640566993.\n",
      "[I 2025-07-22 20:06:08,599] Trial 4 finished with value: 0.9628754640566993 and parameters: {'C': 1.7376924849994324, 'max_iter': 155}. Best is trial 0 with value: 0.9628754640566993.\n",
      "[I 2025-07-22 20:06:08,827] Trial 5 finished with value: 0.9628754640566993 and parameters: {'C': 4.489543622570863, 'max_iter': 186}. Best is trial 0 with value: 0.9628754640566993.\n",
      "[I 2025-07-22 20:06:08,868] Trial 6 finished with value: 0.9557880526493419 and parameters: {'C': 0.003920443005486682, 'max_iter': 115}. Best is trial 0 with value: 0.9628754640566993.\n",
      "[I 2025-07-22 20:06:09,092] Trial 7 finished with value: 0.9628754640566993 and parameters: {'C': 1.8081650645373868, 'max_iter': 134}. Best is trial 0 with value: 0.9628754640566993.\n",
      "[I 2025-07-22 20:06:09,282] Trial 8 finished with value: 0.9625379682753965 and parameters: {'C': 7.946522333145416, 'max_iter': 137}. Best is trial 0 with value: 0.9628754640566993.\n",
      "[I 2025-07-22 20:06:09,338] Trial 9 finished with value: 0.9628754640566993 and parameters: {'C': 0.7086279024902357, 'max_iter': 129}. Best is trial 0 with value: 0.9628754640566993.\n",
      "[I 2025-07-22 20:06:09,495] A new study created in memory with name: no-name-c8314249-1098-441d-8a98-0843e02d3d4a\n",
      "[I 2025-07-22 20:06:09,822] Trial 0 finished with value: 0.9659244264507423 and parameters: {'hidden_layer_width': 21, 'hidden_layer_depth': 1, 'activation': 'tanh', 'alpha': 0.002829735897807192, 'learning_rate_init': 0.0007604774049794175}. Best is trial 0 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 20:06:23,750] Trial 1 finished with value: 0.9655870445344129 and parameters: {'hidden_layer_width': 84, 'hidden_layer_depth': 3, 'activation': 'tanh', 'alpha': 3.7175536345353156e-05, 'learning_rate_init': 0.0014207886606824706}. Best is trial 0 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 20:06:30,189] Trial 2 finished with value: 0.9655870445344129 and parameters: {'hidden_layer_width': 33, 'hidden_layer_depth': 11, 'activation': 'relu', 'alpha': 0.00014862771530486232, 'learning_rate_init': 0.0001514100481809993}. Best is trial 0 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 20:08:40,972] Trial 3 finished with value: 0.9520917678812416 and parameters: {'hidden_layer_width': 80, 'hidden_layer_depth': 15, 'activation': 'relu', 'alpha': 0.00011229623752047637, 'learning_rate_init': 0.00042374195565632436}. Best is trial 0 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 20:09:07,392] Trial 4 finished with value: 0.963225371120108 and parameters: {'hidden_layer_width': 60, 'hidden_layer_depth': 15, 'activation': 'tanh', 'alpha': 0.0008348825823580277, 'learning_rate_init': 0.005418284351950808}. Best is trial 0 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 20:09:21,540] Trial 5 finished with value: 0.9662618083670715 and parameters: {'hidden_layer_width': 49, 'hidden_layer_depth': 11, 'activation': 'relu', 'alpha': 0.006419103629528906, 'learning_rate_init': 0.008196523049033665}. Best is trial 5 with value: 0.9662618083670715.\n",
      "[I 2025-07-22 20:09:57,273] Trial 6 finished with value: 0.9662618083670715 and parameters: {'hidden_layer_width': 98, 'hidden_layer_depth': 14, 'activation': 'relu', 'alpha': 0.008276528696386594, 'learning_rate_init': 0.006602238887062009}. Best is trial 5 with value: 0.9662618083670715.\n",
      "[I 2025-07-22 20:10:01,258] Trial 7 finished with value: 0.9652496626180836 and parameters: {'hidden_layer_width': 38, 'hidden_layer_depth': 3, 'activation': 'relu', 'alpha': 0.0001295498192290403, 'learning_rate_init': 0.004952953236952987}. Best is trial 5 with value: 0.9662618083670715.\n",
      "[I 2025-07-22 20:10:08,909] Trial 8 finished with value: 0.9645748987854251 and parameters: {'hidden_layer_width': 73, 'hidden_layer_depth': 3, 'activation': 'tanh', 'alpha': 1.0682733350560039e-05, 'learning_rate_init': 0.0011955547614148645}. Best is trial 5 with value: 0.9662618083670715.\n",
      "[I 2025-07-22 20:10:20,343] Trial 9 finished with value: 0.9608636977058029 and parameters: {'hidden_layer_width': 72, 'hidden_layer_depth': 5, 'activation': 'tanh', 'alpha': 0.0050389990024404694, 'learning_rate_init': 0.0006379505987934395}. Best is trial 5 with value: 0.9662618083670715.\n",
      "[I 2025-07-22 20:37:38,585] A new study created in memory with name: no-name-b62f0217-b661-4bf5-9843-587116de1dd8\n",
      "[I 2025-07-22 20:44:23,543] Trial 0 finished with value: 0.9612010796221323 and parameters: {'hidden_layer_width': 65, 'hidden_layer_depth': 9, 'activation': 'tanh', 'alpha': 0.00794459814723425, 'learning_rate_init': 0.0002741238726159996}. Best is trial 0 with value: 0.9612010796221323.\n",
      "[I 2025-07-22 20:49:18,045] Trial 1 finished with value: 0.9669365721997301 and parameters: {'hidden_layer_width': 76, 'hidden_layer_depth': 8, 'activation': 'relu', 'alpha': 0.001622146384940059, 'learning_rate_init': 0.0023915563885966657}. Best is trial 1 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 20:53:24,125] Trial 2 finished with value: 0.9706477732793523 and parameters: {'hidden_layer_width': 117, 'hidden_layer_depth': 9, 'activation': 'tanh', 'alpha': 7.796837256652502e-05, 'learning_rate_init': 0.0012951165364846974}. Best is trial 2 with value: 0.9706477732793523.\n",
      "[I 2025-07-22 21:04:27,741] Trial 3 finished with value: 0.9692982456140351 and parameters: {'hidden_layer_width': 73, 'hidden_layer_depth': 15, 'activation': 'tanh', 'alpha': 0.00015868593213429067, 'learning_rate_init': 0.0002466815480826988}. Best is trial 2 with value: 0.9706477732793523.\n",
      "[I 2025-07-22 21:10:34,310] Trial 4 finished with value: 0.9665991902834008 and parameters: {'hidden_layer_width': 44, 'hidden_layer_depth': 11, 'activation': 'tanh', 'alpha': 0.00020780991982372735, 'learning_rate_init': 0.00010538055637807069}. Best is trial 2 with value: 0.9706477732793523.\n",
      "[I 2025-07-22 21:20:05,398] Trial 5 finished with value: 0.9615384615384616 and parameters: {'hidden_layer_width': 74, 'hidden_layer_depth': 6, 'activation': 'tanh', 'alpha': 0.000941981826312317, 'learning_rate_init': 0.0003422499719776392}. Best is trial 2 with value: 0.9706477732793523.\n",
      "[I 2025-07-22 21:32:56,131] Trial 6 finished with value: 0.9642375168690959 and parameters: {'hidden_layer_width': 115, 'hidden_layer_depth': 7, 'activation': 'tanh', 'alpha': 0.006282253669047042, 'learning_rate_init': 0.00027676031551282965}. Best is trial 2 with value: 0.9706477732793523.\n",
      "[I 2025-07-22 21:34:39,300] Trial 7 finished with value: 0.9689608636977058 and parameters: {'hidden_layer_width': 60, 'hidden_layer_depth': 6, 'activation': 'relu', 'alpha': 1.4548121122607116e-05, 'learning_rate_init': 0.009862568863192405}. Best is trial 2 with value: 0.9706477732793523.\n",
      "[I 2025-07-22 21:36:05,591] Trial 8 finished with value: 0.9605263157894737 and parameters: {'hidden_layer_width': 111, 'hidden_layer_depth': 3, 'activation': 'relu', 'alpha': 1.531969044628795e-05, 'learning_rate_init': 0.00028423130982855606}. Best is trial 2 with value: 0.9706477732793523.\n",
      "[I 2025-07-22 21:36:06,721] Trial 9 finished with value: 0.9692982456140351 and parameters: {'hidden_layer_width': 21, 'hidden_layer_depth': 2, 'activation': 'relu', 'alpha': 0.00018536132146914727, 'learning_rate_init': 0.00046704393085972236}. Best is trial 2 with value: 0.9706477732793523.\n",
      "[I 2025-07-22 21:48:18,277] A new study created in memory with name: no-name-1a1391dc-dcd6-411c-ad49-67f81cac29a5\n",
      "[I 2025-07-22 21:49:19,150] Trial 0 finished with value: 0.9571524966261808 and parameters: {'hidden_layer_width': 70, 'hidden_layer_depth': 4, 'activation': 'tanh', 'alpha': 1.3489446090986494e-05, 'learning_rate_init': 0.009104697873220727}. Best is trial 0 with value: 0.9571524966261808.\n",
      "[I 2025-07-22 21:50:26,895] Trial 1 finished with value: 0.9591767881241565 and parameters: {'hidden_layer_width': 43, 'hidden_layer_depth': 15, 'activation': 'relu', 'alpha': 0.0012562811694944653, 'learning_rate_init': 0.0012591089776017546}. Best is trial 1 with value: 0.9591767881241565.\n",
      "[I 2025-07-22 21:51:39,312] Trial 2 finished with value: 0.9605263157894737 and parameters: {'hidden_layer_width': 101, 'hidden_layer_depth': 13, 'activation': 'relu', 'alpha': 6.50043774990913e-05, 'learning_rate_init': 0.003890342345359619}. Best is trial 2 with value: 0.9605263157894737.\n",
      "[I 2025-07-22 21:51:58,737] Trial 3 finished with value: 0.9517543859649122 and parameters: {'hidden_layer_width': 29, 'hidden_layer_depth': 15, 'activation': 'tanh', 'alpha': 3.267237285340278e-05, 'learning_rate_init': 0.0001625210528649991}. Best is trial 2 with value: 0.9605263157894737.\n",
      "[I 2025-07-22 21:53:33,377] Trial 4 finished with value: 0.9659244264507423 and parameters: {'hidden_layer_width': 44, 'hidden_layer_depth': 13, 'activation': 'relu', 'alpha': 0.0004123252172381186, 'learning_rate_init': 0.0001870277552406497}. Best is trial 4 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 21:55:50,385] Trial 5 finished with value: 0.9510796221322537 and parameters: {'hidden_layer_width': 83, 'hidden_layer_depth': 12, 'activation': 'tanh', 'alpha': 3.4175485285628194e-05, 'learning_rate_init': 0.000879615939798708}. Best is trial 4 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 21:56:45,985] Trial 6 finished with value: 0.9520917678812416 and parameters: {'hidden_layer_width': 71, 'hidden_layer_depth': 10, 'activation': 'tanh', 'alpha': 0.00046353675120645177, 'learning_rate_init': 0.00010806013834786007}. Best is trial 4 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 21:58:08,132] Trial 7 finished with value: 0.953778677462888 and parameters: {'hidden_layer_width': 101, 'hidden_layer_depth': 6, 'activation': 'tanh', 'alpha': 2.0236977582229645e-05, 'learning_rate_init': 0.0006169818740505119}. Best is trial 4 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 21:58:20,523] Trial 8 finished with value: 0.9595141700404858 and parameters: {'hidden_layer_width': 106, 'hidden_layer_depth': 1, 'activation': 'relu', 'alpha': 1.6200962387233023e-05, 'learning_rate_init': 0.002547001779471287}. Best is trial 4 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 21:58:31,904] Trial 9 finished with value: 0.9618758434547908 and parameters: {'hidden_layer_width': 42, 'hidden_layer_depth': 3, 'activation': 'tanh', 'alpha': 0.0031083611089235113, 'learning_rate_init': 0.004607791220135896}. Best is trial 4 with value: 0.9659244264507423.\n",
      "[I 2025-07-22 22:07:43,375] A new study created in memory with name: no-name-ff90cff0-d3bd-4274-a34c-c09f04a924f4\n",
      "[I 2025-07-22 22:10:02,378] Trial 0 finished with value: 0.9615384615384616 and parameters: {'hidden_layer_width': 74, 'hidden_layer_depth': 12, 'activation': 'relu', 'alpha': 0.001729184109434598, 'learning_rate_init': 0.00030964784785073035}. Best is trial 0 with value: 0.9615384615384616.\n",
      "[I 2025-07-22 22:11:19,563] Trial 1 finished with value: 0.9618758434547908 and parameters: {'hidden_layer_width': 55, 'hidden_layer_depth': 11, 'activation': 'tanh', 'alpha': 0.0009556224240396172, 'learning_rate_init': 0.00012083631678997256}. Best is trial 1 with value: 0.9618758434547908.\n",
      "[I 2025-07-22 22:12:11,858] Trial 2 finished with value: 0.9608636977058029 and parameters: {'hidden_layer_width': 78, 'hidden_layer_depth': 9, 'activation': 'tanh', 'alpha': 4.34601845595124e-05, 'learning_rate_init': 0.0020576607897740576}. Best is trial 1 with value: 0.9618758434547908.\n",
      "[I 2025-07-22 22:12:23,459] Trial 3 finished with value: 0.9578272604588394 and parameters: {'hidden_layer_width': 33, 'hidden_layer_depth': 15, 'activation': 'tanh', 'alpha': 0.002367961895659452, 'learning_rate_init': 0.00047066114515401396}. Best is trial 1 with value: 0.9618758434547908.\n",
      "[I 2025-07-22 22:12:46,364] Trial 4 finished with value: 0.9665991902834008 and parameters: {'hidden_layer_width': 53, 'hidden_layer_depth': 7, 'activation': 'relu', 'alpha': 0.001237701238858889, 'learning_rate_init': 0.00018708410506361356}. Best is trial 4 with value: 0.9665991902834008.\n",
      "[I 2025-07-22 22:13:56,688] Trial 5 finished with value: 0.9473684210526315 and parameters: {'hidden_layer_width': 109, 'hidden_layer_depth': 8, 'activation': 'relu', 'alpha': 0.0006856196900547405, 'learning_rate_init': 0.0005567277061223317}. Best is trial 4 with value: 0.9665991902834008.\n",
      "[I 2025-07-22 22:16:07,077] Trial 6 finished with value: 0.9544534412955465 and parameters: {'hidden_layer_width': 68, 'hidden_layer_depth': 7, 'activation': 'tanh', 'alpha': 4.758064873706017e-05, 'learning_rate_init': 0.0014542675015597358}. Best is trial 4 with value: 0.9665991902834008.\n",
      "[I 2025-07-22 22:17:11,995] Trial 7 finished with value: 0.9622132253711201 and parameters: {'hidden_layer_width': 64, 'hidden_layer_depth': 10, 'activation': 'relu', 'alpha': 9.099568918248498e-05, 'learning_rate_init': 0.0010489992755623736}. Best is trial 4 with value: 0.9665991902834008.\n",
      "[I 2025-07-22 22:17:55,375] Trial 8 finished with value: 0.9612010796221323 and parameters: {'hidden_layer_width': 79, 'hidden_layer_depth': 8, 'activation': 'tanh', 'alpha': 4.9133114273473874e-05, 'learning_rate_init': 0.005203914193075796}. Best is trial 4 with value: 0.9665991902834008.\n",
      "[I 2025-07-22 22:18:05,791] Trial 9 finished with value: 0.9601889338731444 and parameters: {'hidden_layer_width': 35, 'hidden_layer_depth': 5, 'activation': 'relu', 'alpha': 1.960129743621703e-05, 'learning_rate_init': 0.00010949122839055128}. Best is trial 4 with value: 0.9665991902834008.\n",
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[I 2025-07-22 22:26:00,709] A new study created in memory with name: no-name-f4b3fb83-8458-402a-b803-244a00f8b08c\n",
      "[I 2025-07-22 22:26:12,506] Trial 0 finished with value: 0.9561255484306446 and parameters: {'hidden_layer_width': 27, 'hidden_layer_depth': 13, 'activation': 'relu', 'alpha': 0.0006469732339174324, 'learning_rate_init': 0.0008375491791913497}. Best is trial 0 with value: 0.9561255484306446.\n",
      "[I 2025-07-22 22:28:17,302] Trial 1 finished with value: 0.9473506581167735 and parameters: {'hidden_layer_width': 101, 'hidden_layer_depth': 11, 'activation': 'relu', 'alpha': 1.5782619215188418e-05, 'learning_rate_init': 0.0009582975717719888}. Best is trial 0 with value: 0.9561255484306446.\n",
      "[I 2025-07-22 22:29:09,283] Trial 2 finished with value: 0.9551130610867364 and parameters: {'hidden_layer_width': 98, 'hidden_layer_depth': 5, 'activation': 'relu', 'alpha': 0.0007265329771078757, 'learning_rate_init': 0.0002994079278472501}. Best is trial 0 with value: 0.9561255484306446.\n",
      "[I 2025-07-22 22:29:48,114] Trial 3 finished with value: 0.9615254809314884 and parameters: {'hidden_layer_width': 86, 'hidden_layer_depth': 10, 'activation': 'tanh', 'alpha': 0.0002012993819679602, 'learning_rate_init': 0.006232104637231824}. Best is trial 3 with value: 0.9615254809314884.\n",
      "[I 2025-07-22 22:30:00,424] Trial 4 finished with value: 0.9598380020249747 and parameters: {'hidden_layer_width': 103, 'hidden_layer_depth': 2, 'activation': 'relu', 'alpha': 0.00016022495106991235, 'learning_rate_init': 0.001434232099578903}. Best is trial 3 with value: 0.9615254809314884.\n",
      "[I 2025-07-22 22:30:38,048] Trial 5 finished with value: 0.9544380695241309 and parameters: {'hidden_layer_width': 98, 'hidden_layer_depth': 7, 'activation': 'tanh', 'alpha': 0.005108155415771959, 'learning_rate_init': 0.009766646684809388}. Best is trial 3 with value: 0.9615254809314884.\n",
      "[I 2025-07-22 22:31:37,318] Trial 6 finished with value: 0.9517381032737091 and parameters: {'hidden_layer_width': 59, 'hidden_layer_depth': 4, 'activation': 'tanh', 'alpha': 0.0007252091040372448, 'learning_rate_init': 0.002965815900420362}. Best is trial 3 with value: 0.9615254809314884.\n",
      "[I 2025-07-22 22:32:46,524] Trial 7 finished with value: 0.9584880188997638 and parameters: {'hidden_layer_width': 95, 'hidden_layer_depth': 15, 'activation': 'tanh', 'alpha': 0.006483473241171692, 'learning_rate_init': 0.00036193779556937037}. Best is trial 3 with value: 0.9615254809314884.\n",
      "[I 2025-07-22 22:34:33,052] Trial 8 finished with value: 0.9571380357745528 and parameters: {'hidden_layer_width': 110, 'hidden_layer_depth': 12, 'activation': 'tanh', 'alpha': 0.0028097679271075625, 'learning_rate_init': 0.0001416465674278518}. Best is trial 3 with value: 0.9615254809314884.\n",
      "[I 2025-07-22 22:34:50,227] Trial 9 finished with value: 0.9611879851501857 and parameters: {'hidden_layer_width': 107, 'hidden_layer_depth': 4, 'activation': 'tanh', 'alpha': 0.0024934099719709675, 'learning_rate_init': 0.00014558381000433974}. Best is trial 3 with value: 0.9615254809314884.\n",
      "[I 2025-07-22 22:36:16,932] A new study created in memory with name: no-name-fd320ebb-b570-4188-80da-16bfabf829ab\n",
      "[I 2025-07-22 22:36:43,981] Trial 0 finished with value: 0.9591767881241565 and parameters: {'n_estimators': 101, 'learning_rate': 0.11355159188106123, 'max_depth': 10}. Best is trial 0 with value: 0.9591767881241565.\n",
      "[I 2025-07-22 22:36:56,384] Trial 1 finished with value: 0.9669365721997301 and parameters: {'n_estimators': 92, 'learning_rate': 0.023533151355785896, 'max_depth': 5}. Best is trial 1 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 22:37:21,075] Trial 2 finished with value: 0.9578272604588394 and parameters: {'n_estimators': 92, 'learning_rate': 0.189288572699207, 'max_depth': 10}. Best is trial 1 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 22:37:31,792] Trial 3 finished with value: 0.9645748987854251 and parameters: {'n_estimators': 97, 'learning_rate': 0.13561114378644587, 'max_depth': 4}. Best is trial 1 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 22:37:54,827] Trial 4 finished with value: 0.9618758434547908 and parameters: {'n_estimators': 96, 'learning_rate': 0.10256004948831804, 'max_depth': 9}. Best is trial 1 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 22:38:17,612] Trial 5 finished with value: 0.9652496626180836 and parameters: {'n_estimators': 96, 'learning_rate': 0.06336464302839648, 'max_depth': 9}. Best is trial 1 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 22:38:31,295] Trial 6 finished with value: 0.9639001349527665 and parameters: {'n_estimators': 100, 'learning_rate': 0.097218404579813, 'max_depth': 5}. Best is trial 1 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 22:38:59,907] Trial 7 finished with value: 0.9574898785425101 and parameters: {'n_estimators': 118, 'learning_rate': 0.1412597878233193, 'max_depth': 9}. Best is trial 1 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 22:39:10,317] Trial 8 finished with value: 0.9612010796221323 and parameters: {'n_estimators': 94, 'learning_rate': 0.18693414574324443, 'max_depth': 4}. Best is trial 1 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 22:39:21,763] Trial 9 finished with value: 0.9662618083670715 and parameters: {'n_estimators': 85, 'learning_rate': 0.05592232605113479, 'max_depth': 5}. Best is trial 1 with value: 0.9669365721997301.\n",
      "[I 2025-07-22 22:39:34,174] A new study created in memory with name: no-name-3511fedb-78d3-4dc2-8e5d-8ca83c10b605\n",
      "[I 2025-07-22 22:39:52,549] Trial 0 finished with value: 0.9692982456140351 and parameters: {'n_estimators': 84, 'learning_rate': 0.13131350178592857, 'max_depth': 8}. Best is trial 0 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 22:40:20,319] Trial 1 finished with value: 0.9696356275303644 and parameters: {'n_estimators': 113, 'learning_rate': 0.14115866273065358, 'max_depth': 9}. Best is trial 1 with value: 0.9696356275303644.\n",
      "[I 2025-07-22 22:40:47,558] Trial 2 finished with value: 0.967948717948718 and parameters: {'n_estimators': 111, 'learning_rate': 0.15613415594853777, 'max_depth': 9}. Best is trial 1 with value: 0.9696356275303644.\n",
      "[I 2025-07-22 22:40:56,694] Trial 3 finished with value: 0.9733468286099866 and parameters: {'n_estimators': 83, 'learning_rate': 0.038189879714737904, 'max_depth': 4}. Best is trial 3 with value: 0.9733468286099866.\n",
      "[I 2025-07-22 22:41:16,902] Trial 4 finished with value: 0.9706477732793523 and parameters: {'n_estimators': 83, 'learning_rate': 0.13216502117164125, 'max_depth': 9}. Best is trial 3 with value: 0.9733468286099866.\n",
      "[I 2025-07-22 22:41:27,804] Trial 5 finished with value: 0.9696356275303644 and parameters: {'n_estimators': 97, 'learning_rate': 0.19949753525943856, 'max_depth': 4}. Best is trial 3 with value: 0.9733468286099866.\n",
      "[I 2025-07-22 22:41:56,369] Trial 6 finished with value: 0.9723346828609987 and parameters: {'n_estimators': 107, 'learning_rate': 0.07286417255079482, 'max_depth': 10}. Best is trial 3 with value: 0.9733468286099866.\n",
      "[I 2025-07-22 22:42:09,693] Trial 7 finished with value: 0.9723346828609987 and parameters: {'n_estimators': 81, 'learning_rate': 0.09604777124305461, 'max_depth': 6}. Best is trial 3 with value: 0.9733468286099866.\n",
      "[I 2025-07-22 22:42:32,383] Trial 8 finished with value: 0.9692982456140351 and parameters: {'n_estimators': 83, 'learning_rate': 0.1262506608937344, 'max_depth': 10}. Best is trial 3 with value: 0.9733468286099866.\n",
      "[I 2025-07-22 22:43:01,833] Trial 9 finished with value: 0.9706477732793523 and parameters: {'n_estimators': 110, 'learning_rate': 0.07239280332083699, 'max_depth': 10}. Best is trial 3 with value: 0.9733468286099866.\n",
      "[I 2025-07-22 22:43:11,024] A new study created in memory with name: no-name-8ada825e-af05-4bdc-b806-84efc00a7c5b\n",
      "[I 2025-07-22 22:43:26,383] Trial 0 finished with value: 0.9588394062078273 and parameters: {'n_estimators': 84, 'learning_rate': 0.17403024735542413, 'max_depth': 7}. Best is trial 0 with value: 0.9588394062078273.\n",
      "[I 2025-07-22 22:43:41,782] Trial 1 finished with value: 0.9676113360323887 and parameters: {'n_estimators': 98, 'learning_rate': 0.08081846223158577, 'max_depth': 6}. Best is trial 1 with value: 0.9676113360323887.\n",
      "[I 2025-07-22 22:44:10,885] Trial 2 finished with value: 0.9665991902834008 and parameters: {'n_estimators': 113, 'learning_rate': 0.10248247047998037, 'max_depth': 10}. Best is trial 1 with value: 0.9676113360323887.\n",
      "[I 2025-07-22 22:44:40,539] Trial 3 finished with value: 0.9642375168690959 and parameters: {'n_estimators': 114, 'learning_rate': 0.1820502608100743, 'max_depth': 10}. Best is trial 1 with value: 0.9676113360323887.\n",
      "[I 2025-07-22 22:44:49,172] Trial 4 finished with value: 0.9689608636977058 and parameters: {'n_estimators': 107, 'learning_rate': 0.08205499621886483, 'max_depth': 3}. Best is trial 4 with value: 0.9689608636977058.\n",
      "[I 2025-07-22 22:45:10,867] Trial 5 finished with value: 0.9652496626180836 and parameters: {'n_estimators': 118, 'learning_rate': 0.11208373637343177, 'max_depth': 7}. Best is trial 4 with value: 0.9689608636977058.\n",
      "[I 2025-07-22 22:45:24,120] Trial 6 finished with value: 0.9615384615384616 and parameters: {'n_estimators': 100, 'learning_rate': 0.19677882962184234, 'max_depth': 5}. Best is trial 4 with value: 0.9689608636977058.\n",
      "[I 2025-07-22 22:45:40,753] Trial 7 finished with value: 0.9692982456140351 and parameters: {'n_estimators': 108, 'learning_rate': 0.04366279180478823, 'max_depth': 6}. Best is trial 7 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 22:46:07,473] Trial 8 finished with value: 0.9628879892037787 and parameters: {'n_estimators': 115, 'learning_rate': 0.1728197473567572, 'max_depth': 9}. Best is trial 7 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 22:46:15,603] Trial 9 finished with value: 0.9692982456140351 and parameters: {'n_estimators': 102, 'learning_rate': 0.05893671357053417, 'max_depth': 3}. Best is trial 7 with value: 0.9692982456140351.\n",
      "[I 2025-07-22 22:46:32,306] A new study created in memory with name: no-name-a44535ad-e658-4e54-b4f0-98c9015ab273\n",
      "[I 2025-07-22 22:46:55,508] Trial 0 finished with value: 0.9649122807017544 and parameters: {'n_estimators': 106, 'learning_rate': 0.04351387773145772, 'max_depth': 8}. Best is trial 0 with value: 0.9649122807017544.\n",
      "[I 2025-07-22 22:47:06,795] Trial 1 finished with value: 0.9639001349527665 and parameters: {'n_estimators': 80, 'learning_rate': 0.0932183128118295, 'max_depth': 5}. Best is trial 0 with value: 0.9649122807017544.\n",
      "[I 2025-07-22 22:47:15,658] Trial 2 finished with value: 0.9652496626180836 and parameters: {'n_estimators': 102, 'learning_rate': 0.1857028060022121, 'max_depth': 3}. Best is trial 2 with value: 0.9652496626180836.\n",
      "[I 2025-07-22 22:47:31,684] Trial 3 finished with value: 0.9628879892037787 and parameters: {'n_estimators': 113, 'learning_rate': 0.09340395709733486, 'max_depth': 5}. Best is trial 2 with value: 0.9652496626180836.\n"
     ]
    }
   ],
   "source": [
    "trials = 10\n",
    "folds = 5\n",
    "size = 60\n",
    "\n",
    "for i, name in enumerate(models):\n",
    "    for j, (train_idx, valid_idx) in enumerate(StratifiedKFold(n_splits=folds, shuffle=True, random_state=47).split(X_train, y_train)):\n",
    "        selected = np.random.default_rng(round(47*(i+1)/(j+1))).choice(feats, size=size, replace=False, p=probs)\n",
    "\n",
    "        X_tr, X_valid = X_train[selected].iloc[train_idx], X_train[selected].iloc[valid_idx]\n",
    "        X_tr = X_tr.clip(lower=X_tr.quantile(0.01), upper=X_tr.quantile(0.99), axis=1)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled_values = scaler.fit_transform(X_tr[selected])\n",
    "        X_tr[selected] = pd.DataFrame(df_scaled_values, columns=selected, index=X_tr.index)\n",
    "\n",
    "        df_scaled_values = scaler.transform(X_valid[selected])\n",
    "        X_valid[selected] = pd.DataFrame(df_scaled_values, columns=selected, index=X_valid.index)\n",
    "\n",
    "        y_tr, y_valid = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
    "\n",
    "        X_tr, y_tr = SMOTE(random_state=47).fit_resample(X_tr, y_tr)\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        obj = partial(objective, model_name=name, X_train=X_tr, y_train=y_tr, X_valid=X_valid, y_valid=y_valid)\n",
    "        study.optimize(obj, n_trials=trials, show_progress_bar=False)\n",
    "\n",
    "        best_params = study.best_params\n",
    "\n",
    "        model_cls = models[name]\n",
    "\n",
    "        if name == 'mlp':\n",
    "            best_params['hidden_layer_sizes'] = tuple([best_params['hidden_layer_width']] * best_params['hidden_layer_depth'])\n",
    "            del best_params['hidden_layer_width']\n",
    "            del best_params['hidden_layer_depth']\n",
    "\n",
    "        final_model = model_cls(**best_params)\n",
    "        final_model.fit(X_tr, y_tr)\n",
    "\n",
    "        models_trained[name].append((final_model, selected, scaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63cc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Going_outside_*_Media_Dependence': 45,\n",
       "         'Social_event_attendance_*_Media_Dependence': 45,\n",
       "         'Exp_Reclusion': 45,\n",
       "         'Friends_circle_size_*_Media_Dependence': 45,\n",
       "         'Reclusion_/_Post_frequency': 45,\n",
       "         'Media_Dependence_*_Social_Volume': 45,\n",
       "         'Social_event_attendance_*_Post_frequency': 45,\n",
       "         'Post_frequency_*_Social_Volume': 45,\n",
       "         'Time_spent_Alone_/_Post_frequency': 45,\n",
       "         'Stage_fear_+_Drained_after_socializing': 44,\n",
       "         'Log1p_Reclusion': 43,\n",
       "         'Social_Volume_/_Time_spent_Alone': 43,\n",
       "         'Going_outside_*_Social_Volume': 43,\n",
       "         'Going_outside_**_Social_event_attendance': 43,\n",
       "         'Sqrt_Reclusion': 42,\n",
       "         'Time_spent_Alone_/_Social_Volume': 42,\n",
       "         'Social_event_attendance_+_Media_Dependence': 42,\n",
       "         'Time_spent_Alone_/_Going_outside': 41,\n",
       "         'Social_Drainage_+_Actual_Fear_Exposure': 41,\n",
       "         'Time_spent_Alone_-_Social_event_attendance': 41,\n",
       "         'Going_outside_*_Post_frequency': 41,\n",
       "         'Reclusion': 40,\n",
       "         'Post_frequency_*_Reclusion': 36,\n",
       "         'Social_Drainage_**_Post_frequency': 35,\n",
       "         'Going_outside_/_Time_spent_Alone': 34,\n",
       "         'Media_Dependence_/_Post_frequency': 33,\n",
       "         'Actual_Fear_Exposure_**_Going_outside': 32,\n",
       "         'Social_Volume': 31,\n",
       "         'Friends_circle_size_+_Media_Dependence': 29,\n",
       "         'Going_outside_**_Post_frequency': 29,\n",
       "         'Media_Dependence': 29,\n",
       "         'Post_frequency_**_Going_outside': 28,\n",
       "         'Social_event_attendance_**_Going_outside': 27,\n",
       "         'Time_spent_Alone_/_Social_event_attendance': 27,\n",
       "         'Social_event_attendance_/_Time_spent_Alone': 27,\n",
       "         'Media_Dependence_**_Time_spent_Alone': 25,\n",
       "         'Social_Drainage_/_Friends_circle_size': 25,\n",
       "         'Social_event_attendance_+_Post_frequency': 25,\n",
       "         'Friends_circle_size_*_Post_frequency': 24,\n",
       "         'Time_spent_Alone_/_Friends_circle_size': 22,\n",
       "         'Drained_after_socializing_/_Going_outside': 21,\n",
       "         'Friends_circle_size_/_Media_Dependence': 12,\n",
       "         'Social_Volume_/_Going_outside': 12,\n",
       "         'Going_outside_/_Friends_circle_size': 12,\n",
       "         'Friends_circle_size_-_Media_Dependence': 12,\n",
       "         'Stage_fear_/_Friends_circle_size': 12,\n",
       "         'Stage_fear_/_Social_Volume': 12,\n",
       "         'Actual_Fear_Exposure_**_Social_event_attendance': 12,\n",
       "         'Drained_after_socializing_/_Stage_fear': 10,\n",
       "         'Actual_Fear_Exposure_/_Going_outside': 10,\n",
       "         'Post_frequency_/_Going_outside': 10,\n",
       "         'Going_outside_*_Friends_circle_size': 9,\n",
       "         'Actual_Fear_Exposure_**_Social_Volume': 9,\n",
       "         'Going_outside_**_Social_Drainage': 8,\n",
       "         'Social_Drainage_**_Reclusion': 8,\n",
       "         'Media_Dependence_**_Going_outside': 8,\n",
       "         'Social_Drainage_/_Time_spent_Alone': 8,\n",
       "         'Media_Dependence_/_Social_Volume': 8,\n",
       "         'Actual_Fear_Exposure_/_Social_Volume': 8,\n",
       "         'Social_event_attendance_/_Social_Volume': 8,\n",
       "         'Actual_Fear_Exposure_/_Social_event_attendance': 8,\n",
       "         'Social_Drainage_/_Going_outside': 8,\n",
       "         'Social_event_attendance_-_Media_Dependence': 7,\n",
       "         'Friends_circle_size_**_Social_Drainage': 7,\n",
       "         'Going_outside_-_Friends_circle_size': 7,\n",
       "         'Going_outside_/_Media_Dependence': 7,\n",
       "         'Reclusion_**_Post_frequency': 7,\n",
       "         'Social_event_attendance_-_Friends_circle_size': 7,\n",
       "         'Social_event_attendance_**_Friends_circle_size': 7,\n",
       "         'Reclusion_**_Going_outside': 7,\n",
       "         'Social_event_attendance_/_Reclusion': 7,\n",
       "         'Media_Dependence_/_Social_event_attendance': 7,\n",
       "         'Time_spent_Alone_-_Post_frequency': 7,\n",
       "         'Time_spent_Alone_-_Friends_circle_size': 7,\n",
       "         'Going_outside_+_Media_Dependence': 7,\n",
       "         'Media_Dependence_**_Reclusion': 7,\n",
       "         'Actual_Fear_Exposure': 7,\n",
       "         'Friends_circle_size_/_Going_outside': 7,\n",
       "         'Social_event_attendance_**_Drained_after_socializing': 7,\n",
       "         'Time_spent_Alone_*_Social_Volume': 7,\n",
       "         'Media_Dependence_**_Friends_circle_size': 6,\n",
       "         'Friends_circle_size_*_Actual_Fear_Exposure': 6,\n",
       "         'Going_outside_**_Stage_fear': 6,\n",
       "         'Social_event_attendance_**_Media_Dependence': 6,\n",
       "         'Social_Drainage_/_Post_frequency': 6,\n",
       "         'Reclusion_*_Social_Volume': 6,\n",
       "         'Friends_circle_size_/_Post_frequency': 6,\n",
       "         'Social_event_attendance_**_Social_Drainage': 6,\n",
       "         'Social_Drainage_**_Time_spent_Alone': 6,\n",
       "         'Post_frequency_/_Social_Volume': 6,\n",
       "         'Reclusion_**_Actual_Fear_Exposure': 6,\n",
       "         'Friends_circle_size_/_Social_event_attendance': 6,\n",
       "         'Social_event_attendance': 6,\n",
       "         'Social_Drainage_**_Social_event_attendance': 6,\n",
       "         'Actual_Fear_Exposure_*_Social_Volume': 5,\n",
       "         'Social_event_attendance_**_Post_frequency': 5,\n",
       "         'Media_Dependence_**_Social_event_attendance': 5,\n",
       "         'Going_outside_/_Post_frequency': 5,\n",
       "         'Friends_circle_size_*_Reclusion': 5,\n",
       "         'Social_event_attendance_/_Post_frequency': 5,\n",
       "         'Actual_Fear_Exposure_**_Friends_circle_size': 5,\n",
       "         'Time_spent_Alone_**_Media_Dependence': 5,\n",
       "         'Stage_fear_*_Social_Drainage': 5,\n",
       "         'Social_Volume_**_Going_outside': 5,\n",
       "         'Social_event_attendance_+_Going_outside': 5,\n",
       "         'Friends_circle_size_/_Reclusion': 5,\n",
       "         'Friends_circle_size': 5,\n",
       "         'Actual_Fear_Exposure_/_Time_spent_Alone': 5,\n",
       "         'Social_Drainage_/_Stage_fear': 5,\n",
       "         'Social_event_attendance_*_Reclusion': 5,\n",
       "         'Post_frequency_/_Reclusion': 5,\n",
       "         'Social_event_attendance_**_Actual_Fear_Exposure': 4,\n",
       "         'Social_Drainage_*_Media_Dependence': 4,\n",
       "         'Exp_Friends_circle_size': 4,\n",
       "         'Time_spent_Alone_*_Post_frequency': 4,\n",
       "         'Friends_circle_size_-_Post_frequency': 4,\n",
       "         'Social_event_attendance_*_Going_outside': 4,\n",
       "         'Time_spent_Alone_*_Social_Drainage': 4,\n",
       "         'Time_spent_Alone_**_Stage_fear': 4,\n",
       "         'Time_spent_Alone_+_Media_Dependence': 4,\n",
       "         'Social_Drainage_**_Actual_Fear_Exposure': 4,\n",
       "         'Post_frequency_**_Friends_circle_size': 4,\n",
       "         'Post_frequency_/_Social_event_attendance': 4,\n",
       "         'Time_spent_Alone_+_Post_frequency': 4,\n",
       "         'Friends_circle_size_**_Media_Dependence': 4,\n",
       "         'Time_spent_Alone_/_Stage_fear': 4,\n",
       "         'Friends_circle_size_**_Going_outside': 4,\n",
       "         'Time_spent_Alone_*_Friends_circle_size': 4,\n",
       "         'Time_spent_Alone_*_Media_Dependence': 4,\n",
       "         'Social_Volume_/_Media_Dependence': 4,\n",
       "         'Exp_Social_event_attendance': 4,\n",
       "         'Time_spent_Alone_**_Social_event_attendance': 4,\n",
       "         'Social_event_attendance_/_Media_Dependence': 4,\n",
       "         'Drained_after_socializing_/_Social_Volume': 4,\n",
       "         'Social_Volume_/_Post_frequency': 4,\n",
       "         'Stage_fear_/_Going_outside': 3,\n",
       "         'Social_Drainage': 3,\n",
       "         'Stage_fear_*_Post_frequency': 3,\n",
       "         'Reclusion_**_Social_Drainage': 3,\n",
       "         'Friends_circle_size_+_Reclusion': 3,\n",
       "         'Friends_circle_size_/_Social_Volume': 3,\n",
       "         'Time_spent_Alone_**_Drained_after_socializing': 3,\n",
       "         'Social_Volume_**_Reclusion': 3,\n",
       "         'Exp_Post_frequency': 3,\n",
       "         'Social_Drainage_/_Social_Volume': 3,\n",
       "         'Social_event_attendance_-_Going_outside': 3,\n",
       "         'Exp_Time_spent_Alone': 3,\n",
       "         'Time_spent_Alone_*_Social_event_attendance': 3,\n",
       "         'Social_event_attendance_+_Reclusion': 3,\n",
       "         'Exp_Media_Dependence': 3,\n",
       "         'Friends_circle_size_/_Time_spent_Alone': 3,\n",
       "         'Social_event_attendance_/_Going_outside': 3,\n",
       "         'Social_event_attendance_*_Actual_Fear_Exposure': 3,\n",
       "         'Reclusion_+_Media_Dependence': 3,\n",
       "         'Time_spent_Alone_+_Social_event_attendance': 3,\n",
       "         'Square_Reclusion': 3,\n",
       "         'Post_frequency_+_Reclusion': 3,\n",
       "         'Going_outside_-_Media_Dependence': 2,\n",
       "         'Social_event_attendance_*_Social_Drainage': 2,\n",
       "         'Actual_Fear_Exposure_/_Friends_circle_size': 2,\n",
       "         'Friends_circle_size_+_Post_frequency': 2,\n",
       "         'Time_spent_Alone_**_Going_outside': 2,\n",
       "         'Going_outside_*_Actual_Fear_Exposure': 2,\n",
       "         'Stage_fear_/_Social_event_attendance': 2,\n",
       "         'Actual_Fear_Exposure_/_Drained_after_socializing': 2,\n",
       "         'Social_event_attendance_/_Friends_circle_size': 2,\n",
       "         'Drained_after_socializing_*_Actual_Fear_Exposure': 2,\n",
       "         'Stage_fear_*_Media_Dependence': 2,\n",
       "         'Media_Dependence_/_Going_outside': 2,\n",
       "         'Going_outside_*_Reclusion': 2,\n",
       "         'Media_Dependence_**_Social_Drainage': 2,\n",
       "         'Going_outside_**_Actual_Fear_Exposure': 2,\n",
       "         'Stage_fear_/_Drained_after_socializing': 2,\n",
       "         'Friends_circle_size_**_Post_frequency': 2,\n",
       "         'Social_Volume_**_Actual_Fear_Exposure': 2,\n",
       "         'Social_Drainage_/_Reclusion': 2,\n",
       "         'Going_outside_/_Social_Volume': 2,\n",
       "         'Going_outside_/_Social_event_attendance': 2,\n",
       "         'Stage_fear_/_Reclusion': 2,\n",
       "         'Drained_after_socializing_*_Social_Volume': 2,\n",
       "         'Going_outside_**_Reclusion': 2,\n",
       "         'GT_Mean_Social_event_attendance': 2,\n",
       "         'Social_event_attendance_-_Post_frequency': 2,\n",
       "         'Log1p_Friends_circle_size': 2,\n",
       "         'Stage_fear_*_Friends_circle_size': 2,\n",
       "         'GT_Mean_Media_Dependence': 2,\n",
       "         'Media_Dependence_/_Reclusion': 2,\n",
       "         'Social_Drainage_*_Actual_Fear_Exposure': 2,\n",
       "         'Friends_circle_size_**_Social_event_attendance': 2,\n",
       "         'Reclusion_**_Friends_circle_size': 2,\n",
       "         'Post_frequency_**_Reclusion': 1,\n",
       "         'Going_outside_/_Reclusion': 1,\n",
       "         'Post_frequency_**_Media_Dependence': 1,\n",
       "         'Log1p_Time_spent_Alone': 1,\n",
       "         'Drained_after_socializing_/_Time_spent_Alone': 1,\n",
       "         'Time_spent_Alone_**_Actual_Fear_Exposure': 1,\n",
       "         'Friends_circle_size_**_Time_spent_Alone': 1,\n",
       "         'Going_outside_**_Time_spent_Alone': 1,\n",
       "         'Post_frequency_+_Media_Dependence': 1,\n",
       "         'Drained_after_socializing_/_Post_frequency': 1,\n",
       "         'Time_spent_Alone_/_Reclusion': 1,\n",
       "         'Friends_circle_size_**_Actual_Fear_Exposure': 1,\n",
       "         'Social_event_attendance_**_Time_spent_Alone': 1,\n",
       "         'Post_frequency_**_Social_event_attendance': 1,\n",
       "         'Going_outside_-_Post_frequency': 1,\n",
       "         'Drained_after_socializing_*_Friends_circle_size': 1,\n",
       "         'Going_outside_**_Media_Dependence': 1,\n",
       "         'Social_Drainage_/_Social_event_attendance': 1,\n",
       "         'Drained_after_socializing_**_Reclusion': 1,\n",
       "         'Time_spent_Alone_+_Going_outside': 1,\n",
       "         'Drained_after_socializing_/_Social_event_attendance': 1,\n",
       "         'Time_spent_Alone': 1,\n",
       "         'Post_frequency_*_Social_Drainage': 1,\n",
       "         'Post_frequency_**_Drained_after_socializing': 1,\n",
       "         'Social_Volume_**_Social_Drainage': 1,\n",
       "         'Time_spent_Alone_*_Going_outside': 1,\n",
       "         'Going_outside_**_Friends_circle_size': 1,\n",
       "         'GT_Median_Post_frequency': 1,\n",
       "         'Social_event_attendance_**_Social_Volume': 1,\n",
       "         'Social_Drainage_**_Social_Volume': 1,\n",
       "         'Reclusion_**_Social_event_attendance': 1,\n",
       "         'Actual_Fear_Exposure_**_Social_Drainage': 1,\n",
       "         'Social_Drainage_**_Going_outside': 1})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_counter = Counter()\n",
    "\n",
    "for model_type in models_trained:\n",
    "    for model in models_trained[model_type]:\n",
    "        feature_counter.update(list(model[1]))\n",
    "\n",
    "feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5740af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "XT = X_test.copy()\n",
    "\n",
    "preds = pd.DataFrame(index=X_test.index)\n",
    "for model_type in models_trained:\n",
    "    for i, model in enumerate(models_trained[model_type]):\n",
    "        model, columns, scaler = model\n",
    "        X_test_sc = scaler.transform(X_test[columns])\n",
    "        X_test[columns] = pd.DataFrame(X_test_sc, columns=columns, index=X_test.index)\n",
    "        preds[model_type + '_' + str(i)] = model.predict_proba(X_test[columns])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e126c85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgb_0</th>\n",
       "      <th>xgb_1</th>\n",
       "      <th>xgb_2</th>\n",
       "      <th>xgb_3</th>\n",
       "      <th>xgb_4</th>\n",
       "      <th>lgbm_0</th>\n",
       "      <th>lgbm_1</th>\n",
       "      <th>lgbm_2</th>\n",
       "      <th>lgbm_3</th>\n",
       "      <th>lgbm_4</th>\n",
       "      <th>...</th>\n",
       "      <th>gb_0</th>\n",
       "      <th>gb_1</th>\n",
       "      <th>gb_2</th>\n",
       "      <th>gb_3</th>\n",
       "      <th>gb_4</th>\n",
       "      <th>hgb_0</th>\n",
       "      <th>hgb_1</th>\n",
       "      <th>hgb_2</th>\n",
       "      <th>hgb_3</th>\n",
       "      <th>hgb_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>xgb_0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945442</td>\n",
       "      <td>0.894016</td>\n",
       "      <td>0.879393</td>\n",
       "      <td>0.872739</td>\n",
       "      <td>0.912050</td>\n",
       "      <td>0.821600</td>\n",
       "      <td>0.673942</td>\n",
       "      <td>0.862313</td>\n",
       "      <td>0.945085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802830</td>\n",
       "      <td>0.853112</td>\n",
       "      <td>0.752593</td>\n",
       "      <td>0.878930</td>\n",
       "      <td>0.462791</td>\n",
       "      <td>0.851145</td>\n",
       "      <td>0.805057</td>\n",
       "      <td>0.852323</td>\n",
       "      <td>0.787943</td>\n",
       "      <td>0.893668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_1</th>\n",
       "      <td>0.945442</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872454</td>\n",
       "      <td>0.857925</td>\n",
       "      <td>0.833139</td>\n",
       "      <td>0.884406</td>\n",
       "      <td>0.783958</td>\n",
       "      <td>0.662338</td>\n",
       "      <td>0.832704</td>\n",
       "      <td>0.913030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.765023</td>\n",
       "      <td>0.819628</td>\n",
       "      <td>0.722274</td>\n",
       "      <td>0.867435</td>\n",
       "      <td>0.436872</td>\n",
       "      <td>0.816616</td>\n",
       "      <td>0.766074</td>\n",
       "      <td>0.810533</td>\n",
       "      <td>0.756276</td>\n",
       "      <td>0.853234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_2</th>\n",
       "      <td>0.894016</td>\n",
       "      <td>0.872454</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931458</td>\n",
       "      <td>0.913449</td>\n",
       "      <td>0.936576</td>\n",
       "      <td>0.886047</td>\n",
       "      <td>0.753261</td>\n",
       "      <td>0.925072</td>\n",
       "      <td>0.931930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.861533</td>\n",
       "      <td>0.890231</td>\n",
       "      <td>0.839760</td>\n",
       "      <td>0.924739</td>\n",
       "      <td>0.485463</td>\n",
       "      <td>0.913144</td>\n",
       "      <td>0.865062</td>\n",
       "      <td>0.881350</td>\n",
       "      <td>0.867047</td>\n",
       "      <td>0.918426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_3</th>\n",
       "      <td>0.879393</td>\n",
       "      <td>0.857925</td>\n",
       "      <td>0.931458</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960123</td>\n",
       "      <td>0.947213</td>\n",
       "      <td>0.947302</td>\n",
       "      <td>0.825121</td>\n",
       "      <td>0.966729</td>\n",
       "      <td>0.923026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.929199</td>\n",
       "      <td>0.939555</td>\n",
       "      <td>0.909225</td>\n",
       "      <td>0.935697</td>\n",
       "      <td>0.478407</td>\n",
       "      <td>0.947044</td>\n",
       "      <td>0.930039</td>\n",
       "      <td>0.938299</td>\n",
       "      <td>0.935635</td>\n",
       "      <td>0.922741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb_4</th>\n",
       "      <td>0.872739</td>\n",
       "      <td>0.833139</td>\n",
       "      <td>0.913449</td>\n",
       "      <td>0.960123</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956123</td>\n",
       "      <td>0.983985</td>\n",
       "      <td>0.841841</td>\n",
       "      <td>0.980533</td>\n",
       "      <td>0.925161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974056</td>\n",
       "      <td>0.981825</td>\n",
       "      <td>0.938481</td>\n",
       "      <td>0.928812</td>\n",
       "      <td>0.476397</td>\n",
       "      <td>0.966788</td>\n",
       "      <td>0.971202</td>\n",
       "      <td>0.976772</td>\n",
       "      <td>0.951352</td>\n",
       "      <td>0.944996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm_0</th>\n",
       "      <td>0.912050</td>\n",
       "      <td>0.884406</td>\n",
       "      <td>0.936576</td>\n",
       "      <td>0.947213</td>\n",
       "      <td>0.956123</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.935759</td>\n",
       "      <td>0.799517</td>\n",
       "      <td>0.970400</td>\n",
       "      <td>0.985537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.908991</td>\n",
       "      <td>0.955486</td>\n",
       "      <td>0.879544</td>\n",
       "      <td>0.970206</td>\n",
       "      <td>0.506099</td>\n",
       "      <td>0.942732</td>\n",
       "      <td>0.896253</td>\n",
       "      <td>0.915962</td>\n",
       "      <td>0.901268</td>\n",
       "      <td>0.933256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm_1</th>\n",
       "      <td>0.821600</td>\n",
       "      <td>0.783958</td>\n",
       "      <td>0.886047</td>\n",
       "      <td>0.947302</td>\n",
       "      <td>0.983985</td>\n",
       "      <td>0.935759</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.863584</td>\n",
       "      <td>0.987021</td>\n",
       "      <td>0.888606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.988358</td>\n",
       "      <td>0.977388</td>\n",
       "      <td>0.969522</td>\n",
       "      <td>0.918591</td>\n",
       "      <td>0.464233</td>\n",
       "      <td>0.976915</td>\n",
       "      <td>0.983507</td>\n",
       "      <td>0.983281</td>\n",
       "      <td>0.973547</td>\n",
       "      <td>0.924041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm_2</th>\n",
       "      <td>0.673942</td>\n",
       "      <td>0.662338</td>\n",
       "      <td>0.753261</td>\n",
       "      <td>0.825121</td>\n",
       "      <td>0.841841</td>\n",
       "      <td>0.799517</td>\n",
       "      <td>0.863584</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846613</td>\n",
       "      <td>0.748727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857593</td>\n",
       "      <td>0.843086</td>\n",
       "      <td>0.842921</td>\n",
       "      <td>0.786686</td>\n",
       "      <td>0.520236</td>\n",
       "      <td>0.841979</td>\n",
       "      <td>0.847164</td>\n",
       "      <td>0.837141</td>\n",
       "      <td>0.844610</td>\n",
       "      <td>0.780086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm_3</th>\n",
       "      <td>0.862313</td>\n",
       "      <td>0.832704</td>\n",
       "      <td>0.925072</td>\n",
       "      <td>0.966729</td>\n",
       "      <td>0.980533</td>\n",
       "      <td>0.970400</td>\n",
       "      <td>0.987021</td>\n",
       "      <td>0.846613</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963505</td>\n",
       "      <td>0.968484</td>\n",
       "      <td>0.954820</td>\n",
       "      <td>0.957377</td>\n",
       "      <td>0.484618</td>\n",
       "      <td>0.982224</td>\n",
       "      <td>0.957004</td>\n",
       "      <td>0.964641</td>\n",
       "      <td>0.967501</td>\n",
       "      <td>0.933936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm_4</th>\n",
       "      <td>0.945085</td>\n",
       "      <td>0.913030</td>\n",
       "      <td>0.931930</td>\n",
       "      <td>0.923026</td>\n",
       "      <td>0.925161</td>\n",
       "      <td>0.985537</td>\n",
       "      <td>0.888606</td>\n",
       "      <td>0.748727</td>\n",
       "      <td>0.933752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860568</td>\n",
       "      <td>0.920792</td>\n",
       "      <td>0.823402</td>\n",
       "      <td>0.953685</td>\n",
       "      <td>0.509440</td>\n",
       "      <td>0.908300</td>\n",
       "      <td>0.851833</td>\n",
       "      <td>0.881922</td>\n",
       "      <td>0.850422</td>\n",
       "      <td>0.919886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catboost_0</th>\n",
       "      <td>0.836986</td>\n",
       "      <td>0.824527</td>\n",
       "      <td>0.863952</td>\n",
       "      <td>0.898925</td>\n",
       "      <td>0.918411</td>\n",
       "      <td>0.921155</td>\n",
       "      <td>0.902270</td>\n",
       "      <td>0.747756</td>\n",
       "      <td>0.915541</td>\n",
       "      <td>0.901583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.906025</td>\n",
       "      <td>0.928888</td>\n",
       "      <td>0.865274</td>\n",
       "      <td>0.925698</td>\n",
       "      <td>0.454228</td>\n",
       "      <td>0.897040</td>\n",
       "      <td>0.891582</td>\n",
       "      <td>0.909841</td>\n",
       "      <td>0.881773</td>\n",
       "      <td>0.882683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catboost_1</th>\n",
       "      <td>0.900758</td>\n",
       "      <td>0.864156</td>\n",
       "      <td>0.931379</td>\n",
       "      <td>0.944300</td>\n",
       "      <td>0.947981</td>\n",
       "      <td>0.945807</td>\n",
       "      <td>0.935115</td>\n",
       "      <td>0.793270</td>\n",
       "      <td>0.956639</td>\n",
       "      <td>0.936127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.915760</td>\n",
       "      <td>0.930156</td>\n",
       "      <td>0.897099</td>\n",
       "      <td>0.933650</td>\n",
       "      <td>0.502212</td>\n",
       "      <td>0.948252</td>\n",
       "      <td>0.916928</td>\n",
       "      <td>0.935063</td>\n",
       "      <td>0.917085</td>\n",
       "      <td>0.931259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catboost_2</th>\n",
       "      <td>0.908377</td>\n",
       "      <td>0.875342</td>\n",
       "      <td>0.942247</td>\n",
       "      <td>0.967838</td>\n",
       "      <td>0.977966</td>\n",
       "      <td>0.982394</td>\n",
       "      <td>0.970980</td>\n",
       "      <td>0.820229</td>\n",
       "      <td>0.992760</td>\n",
       "      <td>0.960540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946031</td>\n",
       "      <td>0.963685</td>\n",
       "      <td>0.929662</td>\n",
       "      <td>0.965435</td>\n",
       "      <td>0.492421</td>\n",
       "      <td>0.976206</td>\n",
       "      <td>0.942516</td>\n",
       "      <td>0.959261</td>\n",
       "      <td>0.948264</td>\n",
       "      <td>0.948964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catboost_3</th>\n",
       "      <td>0.908515</td>\n",
       "      <td>0.879286</td>\n",
       "      <td>0.936902</td>\n",
       "      <td>0.955662</td>\n",
       "      <td>0.967969</td>\n",
       "      <td>0.979560</td>\n",
       "      <td>0.954955</td>\n",
       "      <td>0.797430</td>\n",
       "      <td>0.979800</td>\n",
       "      <td>0.963386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.935344</td>\n",
       "      <td>0.958898</td>\n",
       "      <td>0.911295</td>\n",
       "      <td>0.966719</td>\n",
       "      <td>0.501109</td>\n",
       "      <td>0.963303</td>\n",
       "      <td>0.929032</td>\n",
       "      <td>0.947838</td>\n",
       "      <td>0.929748</td>\n",
       "      <td>0.940230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catboost_4</th>\n",
       "      <td>0.941963</td>\n",
       "      <td>0.923066</td>\n",
       "      <td>0.943108</td>\n",
       "      <td>0.941479</td>\n",
       "      <td>0.919643</td>\n",
       "      <td>0.967240</td>\n",
       "      <td>0.893899</td>\n",
       "      <td>0.734955</td>\n",
       "      <td>0.945147</td>\n",
       "      <td>0.967463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862022</td>\n",
       "      <td>0.899028</td>\n",
       "      <td>0.847052</td>\n",
       "      <td>0.953230</td>\n",
       "      <td>0.463377</td>\n",
       "      <td>0.916106</td>\n",
       "      <td>0.861405</td>\n",
       "      <td>0.890100</td>\n",
       "      <td>0.875030</td>\n",
       "      <td>0.915983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_0</th>\n",
       "      <td>0.826408</td>\n",
       "      <td>0.840839</td>\n",
       "      <td>0.813282</td>\n",
       "      <td>0.863873</td>\n",
       "      <td>0.864443</td>\n",
       "      <td>0.848019</td>\n",
       "      <td>0.842140</td>\n",
       "      <td>0.671141</td>\n",
       "      <td>0.856112</td>\n",
       "      <td>0.838322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.847852</td>\n",
       "      <td>0.866618</td>\n",
       "      <td>0.805294</td>\n",
       "      <td>0.851317</td>\n",
       "      <td>0.345039</td>\n",
       "      <td>0.826606</td>\n",
       "      <td>0.840878</td>\n",
       "      <td>0.864069</td>\n",
       "      <td>0.822672</td>\n",
       "      <td>0.847091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_1</th>\n",
       "      <td>0.828839</td>\n",
       "      <td>0.832275</td>\n",
       "      <td>0.901818</td>\n",
       "      <td>0.914491</td>\n",
       "      <td>0.870778</td>\n",
       "      <td>0.903031</td>\n",
       "      <td>0.865632</td>\n",
       "      <td>0.733851</td>\n",
       "      <td>0.919588</td>\n",
       "      <td>0.888858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.831248</td>\n",
       "      <td>0.840419</td>\n",
       "      <td>0.853888</td>\n",
       "      <td>0.935511</td>\n",
       "      <td>0.471779</td>\n",
       "      <td>0.907177</td>\n",
       "      <td>0.827879</td>\n",
       "      <td>0.850754</td>\n",
       "      <td>0.887014</td>\n",
       "      <td>0.844114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_2</th>\n",
       "      <td>0.909010</td>\n",
       "      <td>0.866993</td>\n",
       "      <td>0.912463</td>\n",
       "      <td>0.940644</td>\n",
       "      <td>0.970492</td>\n",
       "      <td>0.978675</td>\n",
       "      <td>0.954965</td>\n",
       "      <td>0.798343</td>\n",
       "      <td>0.966912</td>\n",
       "      <td>0.960004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.939768</td>\n",
       "      <td>0.974489</td>\n",
       "      <td>0.897544</td>\n",
       "      <td>0.936692</td>\n",
       "      <td>0.476753</td>\n",
       "      <td>0.945239</td>\n",
       "      <td>0.934218</td>\n",
       "      <td>0.949381</td>\n",
       "      <td>0.909457</td>\n",
       "      <td>0.940581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_3</th>\n",
       "      <td>0.864333</td>\n",
       "      <td>0.854043</td>\n",
       "      <td>0.932924</td>\n",
       "      <td>0.939560</td>\n",
       "      <td>0.913955</td>\n",
       "      <td>0.948120</td>\n",
       "      <td>0.909062</td>\n",
       "      <td>0.769423</td>\n",
       "      <td>0.957238</td>\n",
       "      <td>0.928554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871186</td>\n",
       "      <td>0.887840</td>\n",
       "      <td>0.877418</td>\n",
       "      <td>0.961909</td>\n",
       "      <td>0.481676</td>\n",
       "      <td>0.938393</td>\n",
       "      <td>0.866531</td>\n",
       "      <td>0.885362</td>\n",
       "      <td>0.918159</td>\n",
       "      <td>0.894514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_4</th>\n",
       "      <td>0.858378</td>\n",
       "      <td>0.841847</td>\n",
       "      <td>0.857093</td>\n",
       "      <td>0.874299</td>\n",
       "      <td>0.892946</td>\n",
       "      <td>0.919487</td>\n",
       "      <td>0.858373</td>\n",
       "      <td>0.771074</td>\n",
       "      <td>0.877538</td>\n",
       "      <td>0.917888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857922</td>\n",
       "      <td>0.906817</td>\n",
       "      <td>0.796946</td>\n",
       "      <td>0.888488</td>\n",
       "      <td>0.496387</td>\n",
       "      <td>0.856749</td>\n",
       "      <td>0.844894</td>\n",
       "      <td>0.867323</td>\n",
       "      <td>0.823920</td>\n",
       "      <td>0.881160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et_0</th>\n",
       "      <td>0.947498</td>\n",
       "      <td>0.915432</td>\n",
       "      <td>0.948518</td>\n",
       "      <td>0.956648</td>\n",
       "      <td>0.949624</td>\n",
       "      <td>0.973961</td>\n",
       "      <td>0.925389</td>\n",
       "      <td>0.776439</td>\n",
       "      <td>0.961244</td>\n",
       "      <td>0.972853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.898852</td>\n",
       "      <td>0.930777</td>\n",
       "      <td>0.878139</td>\n",
       "      <td>0.953738</td>\n",
       "      <td>0.495698</td>\n",
       "      <td>0.947459</td>\n",
       "      <td>0.901271</td>\n",
       "      <td>0.928289</td>\n",
       "      <td>0.902786</td>\n",
       "      <td>0.939676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et_1</th>\n",
       "      <td>0.947501</td>\n",
       "      <td>0.909627</td>\n",
       "      <td>0.933179</td>\n",
       "      <td>0.947998</td>\n",
       "      <td>0.966336</td>\n",
       "      <td>0.976589</td>\n",
       "      <td>0.937336</td>\n",
       "      <td>0.781604</td>\n",
       "      <td>0.956780</td>\n",
       "      <td>0.972488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925579</td>\n",
       "      <td>0.960786</td>\n",
       "      <td>0.881912</td>\n",
       "      <td>0.944123</td>\n",
       "      <td>0.476606</td>\n",
       "      <td>0.940076</td>\n",
       "      <td>0.924292</td>\n",
       "      <td>0.950229</td>\n",
       "      <td>0.899206</td>\n",
       "      <td>0.955968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et_2</th>\n",
       "      <td>0.917596</td>\n",
       "      <td>0.885340</td>\n",
       "      <td>0.941310</td>\n",
       "      <td>0.970478</td>\n",
       "      <td>0.978587</td>\n",
       "      <td>0.974651</td>\n",
       "      <td>0.969045</td>\n",
       "      <td>0.821123</td>\n",
       "      <td>0.987735</td>\n",
       "      <td>0.955946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.949216</td>\n",
       "      <td>0.963396</td>\n",
       "      <td>0.926601</td>\n",
       "      <td>0.955565</td>\n",
       "      <td>0.485344</td>\n",
       "      <td>0.974214</td>\n",
       "      <td>0.947776</td>\n",
       "      <td>0.966035</td>\n",
       "      <td>0.947838</td>\n",
       "      <td>0.951114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et_3</th>\n",
       "      <td>0.947435</td>\n",
       "      <td>0.906410</td>\n",
       "      <td>0.936967</td>\n",
       "      <td>0.951771</td>\n",
       "      <td>0.968705</td>\n",
       "      <td>0.971536</td>\n",
       "      <td>0.946257</td>\n",
       "      <td>0.779357</td>\n",
       "      <td>0.964314</td>\n",
       "      <td>0.965395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927595</td>\n",
       "      <td>0.957244</td>\n",
       "      <td>0.893024</td>\n",
       "      <td>0.944286</td>\n",
       "      <td>0.465753</td>\n",
       "      <td>0.953473</td>\n",
       "      <td>0.928921</td>\n",
       "      <td>0.955322</td>\n",
       "      <td>0.912497</td>\n",
       "      <td>0.954003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et_4</th>\n",
       "      <td>0.932752</td>\n",
       "      <td>0.896717</td>\n",
       "      <td>0.943394</td>\n",
       "      <td>0.962569</td>\n",
       "      <td>0.974720</td>\n",
       "      <td>0.985645</td>\n",
       "      <td>0.958898</td>\n",
       "      <td>0.806523</td>\n",
       "      <td>0.981602</td>\n",
       "      <td>0.972311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.936512</td>\n",
       "      <td>0.963522</td>\n",
       "      <td>0.908495</td>\n",
       "      <td>0.962924</td>\n",
       "      <td>0.495569</td>\n",
       "      <td>0.968285</td>\n",
       "      <td>0.932963</td>\n",
       "      <td>0.955291</td>\n",
       "      <td>0.930842</td>\n",
       "      <td>0.953550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr_0</th>\n",
       "      <td>0.413856</td>\n",
       "      <td>0.490622</td>\n",
       "      <td>0.480532</td>\n",
       "      <td>0.472408</td>\n",
       "      <td>0.372528</td>\n",
       "      <td>0.436376</td>\n",
       "      <td>0.382074</td>\n",
       "      <td>0.305405</td>\n",
       "      <td>0.462509</td>\n",
       "      <td>0.429139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318969</td>\n",
       "      <td>0.316962</td>\n",
       "      <td>0.414273</td>\n",
       "      <td>0.496960</td>\n",
       "      <td>0.127409</td>\n",
       "      <td>0.453200</td>\n",
       "      <td>0.328997</td>\n",
       "      <td>0.348557</td>\n",
       "      <td>0.427903</td>\n",
       "      <td>0.372920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr_1</th>\n",
       "      <td>0.571538</td>\n",
       "      <td>0.573299</td>\n",
       "      <td>0.484535</td>\n",
       "      <td>0.463833</td>\n",
       "      <td>0.440162</td>\n",
       "      <td>0.444096</td>\n",
       "      <td>0.405185</td>\n",
       "      <td>0.304750</td>\n",
       "      <td>0.425329</td>\n",
       "      <td>0.480518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414095</td>\n",
       "      <td>0.427660</td>\n",
       "      <td>0.368411</td>\n",
       "      <td>0.493628</td>\n",
       "      <td>0.232052</td>\n",
       "      <td>0.465362</td>\n",
       "      <td>0.436736</td>\n",
       "      <td>0.492495</td>\n",
       "      <td>0.442815</td>\n",
       "      <td>0.448916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr_2</th>\n",
       "      <td>0.413818</td>\n",
       "      <td>0.490502</td>\n",
       "      <td>0.480390</td>\n",
       "      <td>0.472388</td>\n",
       "      <td>0.372337</td>\n",
       "      <td>0.436196</td>\n",
       "      <td>0.381898</td>\n",
       "      <td>0.304884</td>\n",
       "      <td>0.462380</td>\n",
       "      <td>0.428850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318725</td>\n",
       "      <td>0.316917</td>\n",
       "      <td>0.414158</td>\n",
       "      <td>0.497019</td>\n",
       "      <td>0.127402</td>\n",
       "      <td>0.453096</td>\n",
       "      <td>0.328711</td>\n",
       "      <td>0.348251</td>\n",
       "      <td>0.428114</td>\n",
       "      <td>0.372747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr_3</th>\n",
       "      <td>0.413115</td>\n",
       "      <td>0.489462</td>\n",
       "      <td>0.480207</td>\n",
       "      <td>0.471953</td>\n",
       "      <td>0.371672</td>\n",
       "      <td>0.435656</td>\n",
       "      <td>0.381373</td>\n",
       "      <td>0.303246</td>\n",
       "      <td>0.461994</td>\n",
       "      <td>0.428243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318131</td>\n",
       "      <td>0.316173</td>\n",
       "      <td>0.413270</td>\n",
       "      <td>0.496723</td>\n",
       "      <td>0.127299</td>\n",
       "      <td>0.452749</td>\n",
       "      <td>0.328103</td>\n",
       "      <td>0.347590</td>\n",
       "      <td>0.427729</td>\n",
       "      <td>0.372194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr_4</th>\n",
       "      <td>0.658057</td>\n",
       "      <td>0.635450</td>\n",
       "      <td>0.623989</td>\n",
       "      <td>0.609896</td>\n",
       "      <td>0.609789</td>\n",
       "      <td>0.616241</td>\n",
       "      <td>0.564987</td>\n",
       "      <td>0.449907</td>\n",
       "      <td>0.591803</td>\n",
       "      <td>0.654577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561537</td>\n",
       "      <td>0.584911</td>\n",
       "      <td>0.517931</td>\n",
       "      <td>0.597114</td>\n",
       "      <td>0.326630</td>\n",
       "      <td>0.607180</td>\n",
       "      <td>0.583173</td>\n",
       "      <td>0.609933</td>\n",
       "      <td>0.561866</td>\n",
       "      <td>0.615585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_0</th>\n",
       "      <td>-0.400781</td>\n",
       "      <td>-0.349980</td>\n",
       "      <td>-0.329133</td>\n",
       "      <td>-0.312444</td>\n",
       "      <td>-0.326096</td>\n",
       "      <td>-0.342545</td>\n",
       "      <td>-0.311491</td>\n",
       "      <td>-0.245817</td>\n",
       "      <td>-0.329492</td>\n",
       "      <td>-0.363618</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.302415</td>\n",
       "      <td>-0.314518</td>\n",
       "      <td>-0.295053</td>\n",
       "      <td>-0.328145</td>\n",
       "      <td>-0.199190</td>\n",
       "      <td>-0.315207</td>\n",
       "      <td>-0.284135</td>\n",
       "      <td>-0.306960</td>\n",
       "      <td>-0.295762</td>\n",
       "      <td>-0.342593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_1</th>\n",
       "      <td>0.915348</td>\n",
       "      <td>0.885515</td>\n",
       "      <td>0.911289</td>\n",
       "      <td>0.906511</td>\n",
       "      <td>0.920728</td>\n",
       "      <td>0.988274</td>\n",
       "      <td>0.884254</td>\n",
       "      <td>0.753485</td>\n",
       "      <td>0.924662</td>\n",
       "      <td>0.986966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859906</td>\n",
       "      <td>0.930038</td>\n",
       "      <td>0.811763</td>\n",
       "      <td>0.944867</td>\n",
       "      <td>0.502902</td>\n",
       "      <td>0.892232</td>\n",
       "      <td>0.844256</td>\n",
       "      <td>0.869277</td>\n",
       "      <td>0.838192</td>\n",
       "      <td>0.912171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_2</th>\n",
       "      <td>0.283735</td>\n",
       "      <td>0.238980</td>\n",
       "      <td>0.322441</td>\n",
       "      <td>0.275320</td>\n",
       "      <td>0.288089</td>\n",
       "      <td>0.289980</td>\n",
       "      <td>0.264611</td>\n",
       "      <td>0.220871</td>\n",
       "      <td>0.274858</td>\n",
       "      <td>0.312346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273243</td>\n",
       "      <td>0.279191</td>\n",
       "      <td>0.231775</td>\n",
       "      <td>0.282027</td>\n",
       "      <td>0.193137</td>\n",
       "      <td>0.278212</td>\n",
       "      <td>0.276430</td>\n",
       "      <td>0.285722</td>\n",
       "      <td>0.279586</td>\n",
       "      <td>0.288074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_3</th>\n",
       "      <td>0.011185</td>\n",
       "      <td>-0.039122</td>\n",
       "      <td>-0.113398</td>\n",
       "      <td>-0.020818</td>\n",
       "      <td>0.083815</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.060851</td>\n",
       "      <td>0.071670</td>\n",
       "      <td>-0.017471</td>\n",
       "      <td>0.005461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134533</td>\n",
       "      <td>0.128749</td>\n",
       "      <td>0.015191</td>\n",
       "      <td>-0.038164</td>\n",
       "      <td>0.069585</td>\n",
       "      <td>-0.014507</td>\n",
       "      <td>0.116505</td>\n",
       "      <td>0.110362</td>\n",
       "      <td>0.021693</td>\n",
       "      <td>0.067300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_4</th>\n",
       "      <td>0.037347</td>\n",
       "      <td>0.031059</td>\n",
       "      <td>0.051460</td>\n",
       "      <td>0.046276</td>\n",
       "      <td>0.058976</td>\n",
       "      <td>0.055307</td>\n",
       "      <td>0.060258</td>\n",
       "      <td>0.054617</td>\n",
       "      <td>0.059411</td>\n",
       "      <td>0.048029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056088</td>\n",
       "      <td>0.048451</td>\n",
       "      <td>0.065808</td>\n",
       "      <td>0.055270</td>\n",
       "      <td>0.027872</td>\n",
       "      <td>0.058169</td>\n",
       "      <td>0.059188</td>\n",
       "      <td>0.061455</td>\n",
       "      <td>0.060662</td>\n",
       "      <td>0.055202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb_0</th>\n",
       "      <td>0.802830</td>\n",
       "      <td>0.765023</td>\n",
       "      <td>0.861533</td>\n",
       "      <td>0.929199</td>\n",
       "      <td>0.974056</td>\n",
       "      <td>0.908991</td>\n",
       "      <td>0.988358</td>\n",
       "      <td>0.857593</td>\n",
       "      <td>0.963505</td>\n",
       "      <td>0.860568</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974223</td>\n",
       "      <td>0.955084</td>\n",
       "      <td>0.889247</td>\n",
       "      <td>0.453415</td>\n",
       "      <td>0.953656</td>\n",
       "      <td>0.986870</td>\n",
       "      <td>0.981623</td>\n",
       "      <td>0.957137</td>\n",
       "      <td>0.913952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb_1</th>\n",
       "      <td>0.853112</td>\n",
       "      <td>0.819628</td>\n",
       "      <td>0.890231</td>\n",
       "      <td>0.939555</td>\n",
       "      <td>0.981825</td>\n",
       "      <td>0.955486</td>\n",
       "      <td>0.977388</td>\n",
       "      <td>0.843086</td>\n",
       "      <td>0.968484</td>\n",
       "      <td>0.920792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974223</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.924636</td>\n",
       "      <td>0.921165</td>\n",
       "      <td>0.477463</td>\n",
       "      <td>0.952516</td>\n",
       "      <td>0.965074</td>\n",
       "      <td>0.970235</td>\n",
       "      <td>0.934260</td>\n",
       "      <td>0.934712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb_2</th>\n",
       "      <td>0.752593</td>\n",
       "      <td>0.722274</td>\n",
       "      <td>0.839760</td>\n",
       "      <td>0.909225</td>\n",
       "      <td>0.938481</td>\n",
       "      <td>0.879544</td>\n",
       "      <td>0.969522</td>\n",
       "      <td>0.842921</td>\n",
       "      <td>0.954820</td>\n",
       "      <td>0.823402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955084</td>\n",
       "      <td>0.924636</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.878297</td>\n",
       "      <td>0.433604</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.956353</td>\n",
       "      <td>0.953948</td>\n",
       "      <td>0.955874</td>\n",
       "      <td>0.872971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb_3</th>\n",
       "      <td>0.878930</td>\n",
       "      <td>0.867435</td>\n",
       "      <td>0.924739</td>\n",
       "      <td>0.935697</td>\n",
       "      <td>0.928812</td>\n",
       "      <td>0.970206</td>\n",
       "      <td>0.918591</td>\n",
       "      <td>0.786686</td>\n",
       "      <td>0.957377</td>\n",
       "      <td>0.953685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.889247</td>\n",
       "      <td>0.921165</td>\n",
       "      <td>0.878297</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.499275</td>\n",
       "      <td>0.933513</td>\n",
       "      <td>0.874442</td>\n",
       "      <td>0.896090</td>\n",
       "      <td>0.903891</td>\n",
       "      <td>0.907331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gb_4</th>\n",
       "      <td>0.462791</td>\n",
       "      <td>0.436872</td>\n",
       "      <td>0.485463</td>\n",
       "      <td>0.478407</td>\n",
       "      <td>0.476397</td>\n",
       "      <td>0.506099</td>\n",
       "      <td>0.464233</td>\n",
       "      <td>0.520236</td>\n",
       "      <td>0.484618</td>\n",
       "      <td>0.509440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.453415</td>\n",
       "      <td>0.477463</td>\n",
       "      <td>0.433604</td>\n",
       "      <td>0.499275</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.498480</td>\n",
       "      <td>0.442747</td>\n",
       "      <td>0.454052</td>\n",
       "      <td>0.452552</td>\n",
       "      <td>0.454608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hgb_0</th>\n",
       "      <td>0.851145</td>\n",
       "      <td>0.816616</td>\n",
       "      <td>0.913144</td>\n",
       "      <td>0.947044</td>\n",
       "      <td>0.966788</td>\n",
       "      <td>0.942732</td>\n",
       "      <td>0.976915</td>\n",
       "      <td>0.841979</td>\n",
       "      <td>0.982224</td>\n",
       "      <td>0.908300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953656</td>\n",
       "      <td>0.952516</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.933513</td>\n",
       "      <td>0.498480</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951826</td>\n",
       "      <td>0.965770</td>\n",
       "      <td>0.958869</td>\n",
       "      <td>0.915142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hgb_1</th>\n",
       "      <td>0.805057</td>\n",
       "      <td>0.766074</td>\n",
       "      <td>0.865062</td>\n",
       "      <td>0.930039</td>\n",
       "      <td>0.971202</td>\n",
       "      <td>0.896253</td>\n",
       "      <td>0.983507</td>\n",
       "      <td>0.847164</td>\n",
       "      <td>0.957004</td>\n",
       "      <td>0.851833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986870</td>\n",
       "      <td>0.965074</td>\n",
       "      <td>0.956353</td>\n",
       "      <td>0.874442</td>\n",
       "      <td>0.442747</td>\n",
       "      <td>0.951826</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989567</td>\n",
       "      <td>0.952894</td>\n",
       "      <td>0.913566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hgb_2</th>\n",
       "      <td>0.852323</td>\n",
       "      <td>0.810533</td>\n",
       "      <td>0.881350</td>\n",
       "      <td>0.938299</td>\n",
       "      <td>0.976772</td>\n",
       "      <td>0.915962</td>\n",
       "      <td>0.983281</td>\n",
       "      <td>0.837141</td>\n",
       "      <td>0.964641</td>\n",
       "      <td>0.881922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981623</td>\n",
       "      <td>0.970235</td>\n",
       "      <td>0.953948</td>\n",
       "      <td>0.896090</td>\n",
       "      <td>0.454052</td>\n",
       "      <td>0.965770</td>\n",
       "      <td>0.989567</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954391</td>\n",
       "      <td>0.926420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hgb_3</th>\n",
       "      <td>0.787943</td>\n",
       "      <td>0.756276</td>\n",
       "      <td>0.867047</td>\n",
       "      <td>0.935635</td>\n",
       "      <td>0.951352</td>\n",
       "      <td>0.901268</td>\n",
       "      <td>0.973547</td>\n",
       "      <td>0.844610</td>\n",
       "      <td>0.967501</td>\n",
       "      <td>0.850422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957137</td>\n",
       "      <td>0.934260</td>\n",
       "      <td>0.955874</td>\n",
       "      <td>0.903891</td>\n",
       "      <td>0.452552</td>\n",
       "      <td>0.958869</td>\n",
       "      <td>0.952894</td>\n",
       "      <td>0.954391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.887254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hgb_4</th>\n",
       "      <td>0.893668</td>\n",
       "      <td>0.853234</td>\n",
       "      <td>0.918426</td>\n",
       "      <td>0.922741</td>\n",
       "      <td>0.944996</td>\n",
       "      <td>0.933256</td>\n",
       "      <td>0.924041</td>\n",
       "      <td>0.780086</td>\n",
       "      <td>0.933936</td>\n",
       "      <td>0.919886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.913952</td>\n",
       "      <td>0.934712</td>\n",
       "      <td>0.872971</td>\n",
       "      <td>0.907331</td>\n",
       "      <td>0.454608</td>\n",
       "      <td>0.915142</td>\n",
       "      <td>0.913566</td>\n",
       "      <td>0.926420</td>\n",
       "      <td>0.887254</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               xgb_0     xgb_1     xgb_2     xgb_3     xgb_4    lgbm_0  \\\n",
       "xgb_0       1.000000  0.945442  0.894016  0.879393  0.872739  0.912050   \n",
       "xgb_1       0.945442  1.000000  0.872454  0.857925  0.833139  0.884406   \n",
       "xgb_2       0.894016  0.872454  1.000000  0.931458  0.913449  0.936576   \n",
       "xgb_3       0.879393  0.857925  0.931458  1.000000  0.960123  0.947213   \n",
       "xgb_4       0.872739  0.833139  0.913449  0.960123  1.000000  0.956123   \n",
       "lgbm_0      0.912050  0.884406  0.936576  0.947213  0.956123  1.000000   \n",
       "lgbm_1      0.821600  0.783958  0.886047  0.947302  0.983985  0.935759   \n",
       "lgbm_2      0.673942  0.662338  0.753261  0.825121  0.841841  0.799517   \n",
       "lgbm_3      0.862313  0.832704  0.925072  0.966729  0.980533  0.970400   \n",
       "lgbm_4      0.945085  0.913030  0.931930  0.923026  0.925161  0.985537   \n",
       "catboost_0  0.836986  0.824527  0.863952  0.898925  0.918411  0.921155   \n",
       "catboost_1  0.900758  0.864156  0.931379  0.944300  0.947981  0.945807   \n",
       "catboost_2  0.908377  0.875342  0.942247  0.967838  0.977966  0.982394   \n",
       "catboost_3  0.908515  0.879286  0.936902  0.955662  0.967969  0.979560   \n",
       "catboost_4  0.941963  0.923066  0.943108  0.941479  0.919643  0.967240   \n",
       "rf_0        0.826408  0.840839  0.813282  0.863873  0.864443  0.848019   \n",
       "rf_1        0.828839  0.832275  0.901818  0.914491  0.870778  0.903031   \n",
       "rf_2        0.909010  0.866993  0.912463  0.940644  0.970492  0.978675   \n",
       "rf_3        0.864333  0.854043  0.932924  0.939560  0.913955  0.948120   \n",
       "rf_4        0.858378  0.841847  0.857093  0.874299  0.892946  0.919487   \n",
       "et_0        0.947498  0.915432  0.948518  0.956648  0.949624  0.973961   \n",
       "et_1        0.947501  0.909627  0.933179  0.947998  0.966336  0.976589   \n",
       "et_2        0.917596  0.885340  0.941310  0.970478  0.978587  0.974651   \n",
       "et_3        0.947435  0.906410  0.936967  0.951771  0.968705  0.971536   \n",
       "et_4        0.932752  0.896717  0.943394  0.962569  0.974720  0.985645   \n",
       "lr_0        0.413856  0.490622  0.480532  0.472408  0.372528  0.436376   \n",
       "lr_1        0.571538  0.573299  0.484535  0.463833  0.440162  0.444096   \n",
       "lr_2        0.413818  0.490502  0.480390  0.472388  0.372337  0.436196   \n",
       "lr_3        0.413115  0.489462  0.480207  0.471953  0.371672  0.435656   \n",
       "lr_4        0.658057  0.635450  0.623989  0.609896  0.609789  0.616241   \n",
       "mlp_0      -0.400781 -0.349980 -0.329133 -0.312444 -0.326096 -0.342545   \n",
       "mlp_1       0.915348  0.885515  0.911289  0.906511  0.920728  0.988274   \n",
       "mlp_2       0.283735  0.238980  0.322441  0.275320  0.288089  0.289980   \n",
       "mlp_3       0.011185 -0.039122 -0.113398 -0.020818  0.083815  0.002244   \n",
       "mlp_4       0.037347  0.031059  0.051460  0.046276  0.058976  0.055307   \n",
       "gb_0        0.802830  0.765023  0.861533  0.929199  0.974056  0.908991   \n",
       "gb_1        0.853112  0.819628  0.890231  0.939555  0.981825  0.955486   \n",
       "gb_2        0.752593  0.722274  0.839760  0.909225  0.938481  0.879544   \n",
       "gb_3        0.878930  0.867435  0.924739  0.935697  0.928812  0.970206   \n",
       "gb_4        0.462791  0.436872  0.485463  0.478407  0.476397  0.506099   \n",
       "hgb_0       0.851145  0.816616  0.913144  0.947044  0.966788  0.942732   \n",
       "hgb_1       0.805057  0.766074  0.865062  0.930039  0.971202  0.896253   \n",
       "hgb_2       0.852323  0.810533  0.881350  0.938299  0.976772  0.915962   \n",
       "hgb_3       0.787943  0.756276  0.867047  0.935635  0.951352  0.901268   \n",
       "hgb_4       0.893668  0.853234  0.918426  0.922741  0.944996  0.933256   \n",
       "\n",
       "              lgbm_1    lgbm_2    lgbm_3    lgbm_4  ...      gb_0      gb_1  \\\n",
       "xgb_0       0.821600  0.673942  0.862313  0.945085  ...  0.802830  0.853112   \n",
       "xgb_1       0.783958  0.662338  0.832704  0.913030  ...  0.765023  0.819628   \n",
       "xgb_2       0.886047  0.753261  0.925072  0.931930  ...  0.861533  0.890231   \n",
       "xgb_3       0.947302  0.825121  0.966729  0.923026  ...  0.929199  0.939555   \n",
       "xgb_4       0.983985  0.841841  0.980533  0.925161  ...  0.974056  0.981825   \n",
       "lgbm_0      0.935759  0.799517  0.970400  0.985537  ...  0.908991  0.955486   \n",
       "lgbm_1      1.000000  0.863584  0.987021  0.888606  ...  0.988358  0.977388   \n",
       "lgbm_2      0.863584  1.000000  0.846613  0.748727  ...  0.857593  0.843086   \n",
       "lgbm_3      0.987021  0.846613  1.000000  0.933752  ...  0.963505  0.968484   \n",
       "lgbm_4      0.888606  0.748727  0.933752  1.000000  ...  0.860568  0.920792   \n",
       "catboost_0  0.902270  0.747756  0.915541  0.901583  ...  0.906025  0.928888   \n",
       "catboost_1  0.935115  0.793270  0.956639  0.936127  ...  0.915760  0.930156   \n",
       "catboost_2  0.970980  0.820229  0.992760  0.960540  ...  0.946031  0.963685   \n",
       "catboost_3  0.954955  0.797430  0.979800  0.963386  ...  0.935344  0.958898   \n",
       "catboost_4  0.893899  0.734955  0.945147  0.967463  ...  0.862022  0.899028   \n",
       "rf_0        0.842140  0.671141  0.856112  0.838322  ...  0.847852  0.866618   \n",
       "rf_1        0.865632  0.733851  0.919588  0.888858  ...  0.831248  0.840419   \n",
       "rf_2        0.954965  0.798343  0.966912  0.960004  ...  0.939768  0.974489   \n",
       "rf_3        0.909062  0.769423  0.957238  0.928554  ...  0.871186  0.887840   \n",
       "rf_4        0.858373  0.771074  0.877538  0.917888  ...  0.857922  0.906817   \n",
       "et_0        0.925389  0.776439  0.961244  0.972853  ...  0.898852  0.930777   \n",
       "et_1        0.937336  0.781604  0.956780  0.972488  ...  0.925579  0.960786   \n",
       "et_2        0.969045  0.821123  0.987735  0.955946  ...  0.949216  0.963396   \n",
       "et_3        0.946257  0.779357  0.964314  0.965395  ...  0.927595  0.957244   \n",
       "et_4        0.958898  0.806523  0.981602  0.972311  ...  0.936512  0.963522   \n",
       "lr_0        0.382074  0.305405  0.462509  0.429139  ...  0.318969  0.316962   \n",
       "lr_1        0.405185  0.304750  0.425329  0.480518  ...  0.414095  0.427660   \n",
       "lr_2        0.381898  0.304884  0.462380  0.428850  ...  0.318725  0.316917   \n",
       "lr_3        0.381373  0.303246  0.461994  0.428243  ...  0.318131  0.316173   \n",
       "lr_4        0.564987  0.449907  0.591803  0.654577  ...  0.561537  0.584911   \n",
       "mlp_0      -0.311491 -0.245817 -0.329492 -0.363618  ... -0.302415 -0.314518   \n",
       "mlp_1       0.884254  0.753485  0.924662  0.986966  ...  0.859906  0.930038   \n",
       "mlp_2       0.264611  0.220871  0.274858  0.312346  ...  0.273243  0.279191   \n",
       "mlp_3       0.060851  0.071670 -0.017471  0.005461  ...  0.134533  0.128749   \n",
       "mlp_4       0.060258  0.054617  0.059411  0.048029  ...  0.056088  0.048451   \n",
       "gb_0        0.988358  0.857593  0.963505  0.860568  ...  1.000000  0.974223   \n",
       "gb_1        0.977388  0.843086  0.968484  0.920792  ...  0.974223  1.000000   \n",
       "gb_2        0.969522  0.842921  0.954820  0.823402  ...  0.955084  0.924636   \n",
       "gb_3        0.918591  0.786686  0.957377  0.953685  ...  0.889247  0.921165   \n",
       "gb_4        0.464233  0.520236  0.484618  0.509440  ...  0.453415  0.477463   \n",
       "hgb_0       0.976915  0.841979  0.982224  0.908300  ...  0.953656  0.952516   \n",
       "hgb_1       0.983507  0.847164  0.957004  0.851833  ...  0.986870  0.965074   \n",
       "hgb_2       0.983281  0.837141  0.964641  0.881922  ...  0.981623  0.970235   \n",
       "hgb_3       0.973547  0.844610  0.967501  0.850422  ...  0.957137  0.934260   \n",
       "hgb_4       0.924041  0.780086  0.933936  0.919886  ...  0.913952  0.934712   \n",
       "\n",
       "                gb_2      gb_3      gb_4     hgb_0     hgb_1     hgb_2  \\\n",
       "xgb_0       0.752593  0.878930  0.462791  0.851145  0.805057  0.852323   \n",
       "xgb_1       0.722274  0.867435  0.436872  0.816616  0.766074  0.810533   \n",
       "xgb_2       0.839760  0.924739  0.485463  0.913144  0.865062  0.881350   \n",
       "xgb_3       0.909225  0.935697  0.478407  0.947044  0.930039  0.938299   \n",
       "xgb_4       0.938481  0.928812  0.476397  0.966788  0.971202  0.976772   \n",
       "lgbm_0      0.879544  0.970206  0.506099  0.942732  0.896253  0.915962   \n",
       "lgbm_1      0.969522  0.918591  0.464233  0.976915  0.983507  0.983281   \n",
       "lgbm_2      0.842921  0.786686  0.520236  0.841979  0.847164  0.837141   \n",
       "lgbm_3      0.954820  0.957377  0.484618  0.982224  0.957004  0.964641   \n",
       "lgbm_4      0.823402  0.953685  0.509440  0.908300  0.851833  0.881922   \n",
       "catboost_0  0.865274  0.925698  0.454228  0.897040  0.891582  0.909841   \n",
       "catboost_1  0.897099  0.933650  0.502212  0.948252  0.916928  0.935063   \n",
       "catboost_2  0.929662  0.965435  0.492421  0.976206  0.942516  0.959261   \n",
       "catboost_3  0.911295  0.966719  0.501109  0.963303  0.929032  0.947838   \n",
       "catboost_4  0.847052  0.953230  0.463377  0.916106  0.861405  0.890100   \n",
       "rf_0        0.805294  0.851317  0.345039  0.826606  0.840878  0.864069   \n",
       "rf_1        0.853888  0.935511  0.471779  0.907177  0.827879  0.850754   \n",
       "rf_2        0.897544  0.936692  0.476753  0.945239  0.934218  0.949381   \n",
       "rf_3        0.877418  0.961909  0.481676  0.938393  0.866531  0.885362   \n",
       "rf_4        0.796946  0.888488  0.496387  0.856749  0.844894  0.867323   \n",
       "et_0        0.878139  0.953738  0.495698  0.947459  0.901271  0.928289   \n",
       "et_1        0.881912  0.944123  0.476606  0.940076  0.924292  0.950229   \n",
       "et_2        0.926601  0.955565  0.485344  0.974214  0.947776  0.966035   \n",
       "et_3        0.893024  0.944286  0.465753  0.953473  0.928921  0.955322   \n",
       "et_4        0.908495  0.962924  0.495569  0.968285  0.932963  0.955291   \n",
       "lr_0        0.414273  0.496960  0.127409  0.453200  0.328997  0.348557   \n",
       "lr_1        0.368411  0.493628  0.232052  0.465362  0.436736  0.492495   \n",
       "lr_2        0.414158  0.497019  0.127402  0.453096  0.328711  0.348251   \n",
       "lr_3        0.413270  0.496723  0.127299  0.452749  0.328103  0.347590   \n",
       "lr_4        0.517931  0.597114  0.326630  0.607180  0.583173  0.609933   \n",
       "mlp_0      -0.295053 -0.328145 -0.199190 -0.315207 -0.284135 -0.306960   \n",
       "mlp_1       0.811763  0.944867  0.502902  0.892232  0.844256  0.869277   \n",
       "mlp_2       0.231775  0.282027  0.193137  0.278212  0.276430  0.285722   \n",
       "mlp_3       0.015191 -0.038164  0.069585 -0.014507  0.116505  0.110362   \n",
       "mlp_4       0.065808  0.055270  0.027872  0.058169  0.059188  0.061455   \n",
       "gb_0        0.955084  0.889247  0.453415  0.953656  0.986870  0.981623   \n",
       "gb_1        0.924636  0.921165  0.477463  0.952516  0.965074  0.970235   \n",
       "gb_2        1.000000  0.878297  0.433604  0.948718  0.956353  0.953948   \n",
       "gb_3        0.878297  1.000000  0.499275  0.933513  0.874442  0.896090   \n",
       "gb_4        0.433604  0.499275  1.000000  0.498480  0.442747  0.454052   \n",
       "hgb_0       0.948718  0.933513  0.498480  1.000000  0.951826  0.965770   \n",
       "hgb_1       0.956353  0.874442  0.442747  0.951826  1.000000  0.989567   \n",
       "hgb_2       0.953948  0.896090  0.454052  0.965770  0.989567  1.000000   \n",
       "hgb_3       0.955874  0.903891  0.452552  0.958869  0.952894  0.954391   \n",
       "hgb_4       0.872971  0.907331  0.454608  0.915142  0.913566  0.926420   \n",
       "\n",
       "               hgb_3     hgb_4  \n",
       "xgb_0       0.787943  0.893668  \n",
       "xgb_1       0.756276  0.853234  \n",
       "xgb_2       0.867047  0.918426  \n",
       "xgb_3       0.935635  0.922741  \n",
       "xgb_4       0.951352  0.944996  \n",
       "lgbm_0      0.901268  0.933256  \n",
       "lgbm_1      0.973547  0.924041  \n",
       "lgbm_2      0.844610  0.780086  \n",
       "lgbm_3      0.967501  0.933936  \n",
       "lgbm_4      0.850422  0.919886  \n",
       "catboost_0  0.881773  0.882683  \n",
       "catboost_1  0.917085  0.931259  \n",
       "catboost_2  0.948264  0.948964  \n",
       "catboost_3  0.929748  0.940230  \n",
       "catboost_4  0.875030  0.915983  \n",
       "rf_0        0.822672  0.847091  \n",
       "rf_1        0.887014  0.844114  \n",
       "rf_2        0.909457  0.940581  \n",
       "rf_3        0.918159  0.894514  \n",
       "rf_4        0.823920  0.881160  \n",
       "et_0        0.902786  0.939676  \n",
       "et_1        0.899206  0.955968  \n",
       "et_2        0.947838  0.951114  \n",
       "et_3        0.912497  0.954003  \n",
       "et_4        0.930842  0.953550  \n",
       "lr_0        0.427903  0.372920  \n",
       "lr_1        0.442815  0.448916  \n",
       "lr_2        0.428114  0.372747  \n",
       "lr_3        0.427729  0.372194  \n",
       "lr_4        0.561866  0.615585  \n",
       "mlp_0      -0.295762 -0.342593  \n",
       "mlp_1       0.838192  0.912171  \n",
       "mlp_2       0.279586  0.288074  \n",
       "mlp_3       0.021693  0.067300  \n",
       "mlp_4       0.060662  0.055202  \n",
       "gb_0        0.957137  0.913952  \n",
       "gb_1        0.934260  0.934712  \n",
       "gb_2        0.955874  0.872971  \n",
       "gb_3        0.903891  0.907331  \n",
       "gb_4        0.452552  0.454608  \n",
       "hgb_0       0.958869  0.915142  \n",
       "hgb_1       0.952894  0.913566  \n",
       "hgb_2       0.954391  0.926420  \n",
       "hgb_3       1.000000  0.887254  \n",
       "hgb_4       0.887254  1.000000  \n",
       "\n",
       "[45 rows x 45 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnscaler = StandardScaler()\n",
    "preds = nnscaler.fit_transform(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a07ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KillWhenLowAcc(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, minaccuracy, epoch):\n",
    "        super(KillWhenLowAcc, self).__init__()\n",
    "        self.minaccuracy = minaccuracy\n",
    "        self.epoch = epoch\n",
    "        self.maxaccuracyyet = 0.0\n",
    "        self.epochindex = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        accuracy = logs.get('val_accuracy')\n",
    "\n",
    "        if accuracy > self.maxaccuracyyet:\n",
    "            self.maxaccuracyyet = accuracy\n",
    "\n",
    "        if self.epochindex < len(self.epoch) and epoch == self.epoch[self.epochindex]:\n",
    "\n",
    "            if self.maxaccuracyyet < self.minaccuracy[self.epochindex]:\n",
    "                self.model.stop_training = True\n",
    "\n",
    "            self.epochindex += 1\n",
    "\n",
    "\n",
    "class HyperNet(kt.HyperModel):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.ninputs = kwargs.get('ninputs')\n",
    "        self.noutputs = kwargs.get('noutputs')\n",
    "        super().__init__()\n",
    "\n",
    "    def build(self, hp):\n",
    "        nlength = hp.Int('nlength', min_value=1, max_value=6, step=1)\n",
    "        nwidth = hp.Int('nwidth', min_value=50, max_value=150, step=1)\n",
    "\n",
    "        input_ = tf.keras.layers.Input(shape=(self.ninputs,))\n",
    "\n",
    "        for n in range(0, nlength):\n",
    "            bn = tf.keras.layers.BatchNormalization()\n",
    "            midlayer = tf.keras.layers.Dense(nwidth, activation='leaky_relu', kernel_initializer='he_normal', use_bias=False)\n",
    "            do = tf.keras.layers.Dropout(rate=hp.Float('dropout_rate', min_value=0.0, max_value=0.6, step=0.01))\n",
    "\n",
    "            if n == 0:\n",
    "                mid = do(midlayer(bn(input_)))\n",
    "            elif n != nlength-1:\n",
    "                mid = do(midlayer(bn(mid)))\n",
    "            else:\n",
    "                mid = midlayer(bn(mid))\n",
    "\n",
    "        bn = tf.keras.layers.BatchNormalization()\n",
    "        lastdo = tf.keras.layers.Dropout(rate=hp.Float('lastdropout_rate', min_value=0.0, max_value=0.6, step=0.01))\n",
    "        outputlayer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "        output = outputlayer(lastdo(bn(mid)))\n",
    "\n",
    "        model = tf.keras.Model(inputs=[input_], outputs=[output])\n",
    "        opt = tf.keras.optimizers.Nadam(learning_rate=hp.Float('learning_rate', min_value=0.0001, max_value=0.1, step=0.001))\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686f5f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753217152.411298   16274 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1753217152.549609   16274 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1753217152.549657   16274 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available.\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24701037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 00m 05s]\n",
      "val_accuracy: 0.9635627269744873\n",
      "\n",
      "Best val_accuracy So Far: 0.9635627269744873\n",
      "Total elapsed time: 00h 00m 24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'nadam', because it has 2 variables whereas the saved optimizer has 29 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "X_trainnn, X_tnn, y_trainnn, y_tnn = train_test_split(preds, y_test, test_size=0.2, stratify=y_test, random_state=47)\n",
    "\n",
    "class_weights = dict(enumerate(class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_trainnn), y=y_trainnn)))\n",
    "\n",
    "tuner = kt.BayesianOptimization(hypermodel=HyperNet(ninputs=X_trainnn.shape[1], noutputs=len(np.unique(y_trainnn))),\n",
    "                                objective='val_accuracy',\n",
    "                                max_trials=trials,\n",
    "                                max_retries_per_trial=0,\n",
    "                                max_consecutive_failed_trials=trials,\n",
    "                                overwrite=True,\n",
    "                                seed=47)\n",
    "\n",
    "rlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15, min_lr=0.00001)\n",
    "es = tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True)\n",
    "kwa = KillWhenLowAcc(minaccuracy=[0.5, 0.8, 0.9], epoch=[2, 14, 29])\n",
    "\n",
    "X_trainnn, X_tnn, y_trainnn, y_tnn = X_trainnn, X_tnn, y_trainnn.to_numpy(), y_tnn.to_numpy()\n",
    "tuner.search(X_trainnn, y_trainnn, batch_size=64, validation_data=(X_tnn, y_tnn), class_weight=class_weights, callbacks=[es, rlr, kwa], epochs=1000)\n",
    "bestmodel = tuner.get_best_models(num_models=1)[0]\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d020814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = RidgeClassifier()\n",
    "ridge.fit(preds, y_test)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3910dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['id'] = df_sub['id'].astype(int)\n",
    "df_sub = df_sub.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906505ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df_sub.replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550ccae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpreds = pd.DataFrame(index=df_sub.index)\n",
    "for model_type in models_trained:\n",
    "    for i, model in enumerate(models_trained[model_type]):\n",
    "        model, columns, scaler = model\n",
    "        df_sub_sc = scaler.transform(df_sub[columns])\n",
    "        df_sub[columns] = pd.DataFrame(df_sub_sc, columns=columns, index=df_sub.index)\n",
    "        fpreds[model_type + '_' + str(i)] = model.predict_proba(df_sub[columns])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f27b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpreds_id = pd.DataFrame(fpreds.reset_index()['id'])\n",
    "rpreds_id = pd.DataFrame(fpreds.reset_index()['id'])\n",
    "lpreds_id = pd.DataFrame(fpreds.reset_index()['id'])\n",
    "oldfpreds = fpreds.reset_index(drop=True)\n",
    "fpreds = pd.DataFrame(nnscaler.transform(fpreds), columns=fpreds.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1333378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "fpreds_id['pred'] = bestmodel.predict(fpreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f8b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive = fpreds_id.copy()\n",
    "naive['pred'] = oldfpreds.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e59b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpreds = pd.DataFrame(index=X_train.index)\n",
    "for model_type in models_trained:\n",
    "    for i, model in enumerate(models_trained[model_type]):\n",
    "        model, columns, scaler = model\n",
    "        X_train_sc = scaler.transform(X_train[columns])\n",
    "        X_train[columns] = pd.DataFrame(X_train_sc, columns=columns, index=X_train.index)\n",
    "        Xpreds[model_type + '_' + str(i)] = model.predict_proba(X_train[columns])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd3db8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m464/464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "0.8\n",
      "0.26047641541264593\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "po = bestmodel.predict(Xpreds)\n",
    "\n",
    "for t in np.arange(0.80, 0.10, -0.01):\n",
    "    p = (po >= t).astype(int)\n",
    "    score = accuracy_score(y_train, p)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_threshold_nn = t\n",
    "\n",
    "print(best_threshold_nn)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08128bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.359999999958745\n",
      "0.967811593224914\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "\n",
    "for t in np.arange(100.0, 1.0, -0.01):\n",
    "    score = accuracy_score(y_train, np.where(Xpreds.sum(axis=1) >= t, 1, 0))\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_threshold_naive = t\n",
    "\n",
    "print(best_threshold_naive)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164db724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.9661245698090289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RidgeClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "po = ridge.predict(Xpreds)\n",
    "\n",
    "for t in np.arange(0.80, 0.10, -0.05):\n",
    "    p = (po >= t).astype(int)\n",
    "    score = accuracy_score(y_train, p)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_threshold_ridge = t\n",
    "\n",
    "print(best_threshold_ridge)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc64c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44999999999999973\n",
      "0.8779944665631959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "po = lr.predict_proba(Xpreds)[:,1]\n",
    "\n",
    "for t in np.arange(0.80, 0.10, -0.05):\n",
    "    p = (po >= t).astype(int)\n",
    "    score = accuracy_score(y_train, p)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_threshold_lr = t\n",
    "\n",
    "print(best_threshold_lr)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d9b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive['pred'] = np.where(naive['pred'] > best_threshold_naive, 'Introvert', 'Extrovert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73198ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive.to_csv('submission_naive.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e47e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21760</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19687</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23291</td>\n",
       "      <td>0.000051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19646</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24142</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6170</th>\n",
       "      <td>20054</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6171</th>\n",
       "      <td>24319</td>\n",
       "      <td>0.000109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6172</th>\n",
       "      <td>22817</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6173</th>\n",
       "      <td>21678</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6174</th>\n",
       "      <td>19234</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6175 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id      pred\n",
       "0     21760  1.000000\n",
       "1     19687  0.000031\n",
       "2     23291  0.000051\n",
       "3     19646  1.000000\n",
       "4     24142  1.000000\n",
       "...     ...       ...\n",
       "6170  20054  1.000000\n",
       "6171  24319  0.000109\n",
       "6172  22817  0.000030\n",
       "6173  21678  1.000000\n",
       "6174  19234  0.000060\n",
       "\n",
       "[6175 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpreds_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a86c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpreds_id['pred'] = np.where(fpreds_id['pred'] >= best_threshold_nn, 'Introvert', 'Extrovert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6883ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpreds_id.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c4a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RidgeClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rpreds_id['pred'] = ridge.predict(fpreds)\n",
    "rpreds_id['pred'] = np.where(rpreds_id['pred'] >= best_threshold_ridge, 'Introvert', 'Extrovert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514365ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpreds_id.to_csv('submission_ridge.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562079e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicomore/miniconda3/envs/tf-wsl/lib/python3.9/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RidgeClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lpreds_id['pred'] = ridge.predict(fpreds)\n",
    "lpreds_id['pred'] = np.where(lpreds_id['pred'] >= best_threshold_ridge, 'Introvert', 'Extrovert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b3b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpreds_id.to_csv('submission_lr.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
